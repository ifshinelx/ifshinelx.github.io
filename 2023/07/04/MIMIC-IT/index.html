<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/little%20star.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/little%20star.jpg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"ifshinelx.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.17.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>



<link rel="canonical" href="https://ifshinelx.github.io/2023/07/04/MIMIC-IT/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://ifshinelx.github.io/2023/07/04/MIMIC-IT/","path":"2023/07/04/MIMIC-IT/","title":"MIMIC-IT: Multi-Modal In-Context Instruction Tuning"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>MIMIC-IT: Multi-Modal In-Context Instruction Tuning | XinLiu's Homepage, Welcome!</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">XinLiu's Homepage, Welcome!</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#related-workmulti-modal-instruction-tuning-dataset"><span class="nav-number">1.</span> <span class="nav-text">1 - Related
Work(Multi-modal Instruction Tuning Dataset)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#multi-modal-in-context-instruction-tuning-dataset"><span class="nav-number">2.</span> <span class="nav-text">2 -
Multi-modal In-Context Instruction Tuning Dataset</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#mimic-it-data-format"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 - MIMIC-IT Data Format</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sythus-automatic-instruction-response-generation-pipeline"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 -
Sythus: Automatic Instruction-Response Generation Pipeline</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#visual-data-exploration"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 - Visual Data Exploration</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#general-scene-understanding"><span class="nav-number">2.3.1.</span> <span class="nav-text">2.3.1 - General Scene
Understanding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#egocentric-view-understanding"><span class="nav-number">2.3.2.</span> <span class="nav-text">2.3.2 - Egocentric View
Understanding</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dataset-statistics"><span class="nav-number">2.4.</span> <span class="nav-text">2.4 - Dataset Statistics</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#empricial-evaluation"><span class="nav-number">3.</span> <span class="nav-text">3 - Empricial Evaluation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#otter-a-multi-modal-in-context-instruction-tuned-model"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 -
Otter: A Multi-Modal In-Context Instruction Tuned Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#usage-examples-and-demonstrations"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 - Usage Examples and
Demonstrations</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#chatgpt-evaluation"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 - ChatGPT Evaluation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#human-evaluation"><span class="nav-number">3.4.</span> <span class="nav-text">3.4 - Human Evaluation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#few-shot-in-context-learning-metric-evaluation"><span class="nav-number">3.5.</span> <span class="nav-text">3.5 - Few-shot
In-Context Learning Metric Evaluation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#conclusion"><span class="nav-number">4.</span> <span class="nav-text">4 - Conclusion</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Xin Liu"
      src="/images/little%20star.jpg">
  <p class="site-author-name" itemprop="name">Xin Liu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">11</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/ifshinelx" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ifshinelx" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:ifshine_lx@163.com" title="E-Mail → mailto:ifshine_lx@163.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



  <div class="links-of-blogroll motion-element links-of-blogroll-block">
    <div class="links-of-blogroll-title">
      <!-- modify icon to fire by szw -->
      <i class="fa fa-history fa-" aria-hidden="true"></i>
      Recent Posts
    </div>
    <ul class="links-of-blogroll-list" style="padding: 0px 12px;">
      
      
      
        
          <li class="recent_posts_li">
            <a href="/2023/07/09/Shikra/" title="Shikra: Unleashing Multimodal LLM’s Referential Dialogue Magic" target="_blank">07-09 Shikra: Unleashing Multimodal LLM’s Referential Dialogue Magic </a>
          </li>
        
      
        
          <li class="recent_posts_li">
            <a href="/2023/07/06/Lynx/" title="(Lynx)What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?" target="_blank">07-06 (Lynx)What Matters in Training a GPT4-Style Language Model with Multimodal Inputs? </a>
          </li>
        
      
        
          <li class="recent_posts_li">
            <a href="/2023/07/04/MIMIC-IT/" title="MIMIC-IT: Multi-Modal In-Context Instruction Tuning" target="_blank">07-04 MIMIC-IT: Multi-Modal In-Context Instruction Tuning </a>
          </li>
        
      
        
          <li class="recent_posts_li">
            <a href="/2023/07/03/LENS/" title="(LENS)Towards Language Models That Can See: Computer Vision Through the LENS🔍 of Natural Language" target="_blank">07-03 (LENS)Towards Language Models That Can See: Computer Vision Through the LENS🔍 of Natural Language </a>
          </li>
        
      
        
          <li class="recent_posts_li">
            <a href="/2023/07/01/LongMem/" title="(LongMem)Augmenting Language Models with Long-Term Memory" target="_blank">07-01 (LongMem)Augmenting Language Models with Long-Term Memory </a>
          </li>
        
      
    </ul>
  </div>

  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://ifshinelx.github.io/2023/07/04/MIMIC-IT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/little%20star.jpg">
      <meta itemprop="name" content="Xin Liu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="XinLiu's Homepage, Welcome!">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="MIMIC-IT: Multi-Modal In-Context Instruction Tuning | XinLiu's Homepage, Welcome!">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          MIMIC-IT: Multi-Modal In-Context Instruction Tuning
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-07-04 13:00:00" itemprop="dateCreated datePublished" datetime="2023-07-04T13:00:00+08:00">2023-07-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-07-12 21:55:11" itemprop="dateModified" datetime="2023-07-12T21:55:11+08:00">2023-07-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper/" itemprop="url" rel="index"><span itemprop="name">Paper</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper/2023/" itemprop="url" rel="index"><span itemprop="name">2023</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper/2023/06/" itemprop="url" rel="index"><span itemprop="name">06</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>1.9k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>7 mins.</span>
    </span>
</div>

        </div>
          <div class="post-tags" style="margin-top: 5px;">
              <a href="/tags/LLM/" rel="tag" style="border: 0px; border-radius: 10px; padding: 0px 10px;"><i class="fa fa-tag"></i> LLM</a>
              <a href="/tags/Multi-modal/" rel="tag" style="border: 0px; border-radius: 10px; padding: 0px 10px;"><i class="fa fa-tag"></i> Multi-modal</a>
              <a href="/tags/Dataset/" rel="tag" style="border: 0px; border-radius: 10px; padding: 0px 10px;"><i class="fa fa-tag"></i> Dataset</a>
              <a href="/tags/In-Context-Learning/" rel="tag" style="border: 0px; border-radius: 10px; padding: 0px 10px;"><i class="fa fa-tag"></i> In-Context Learning</a>
              <a href="/tags/Instruction-Following/" rel="tag" style="border: 0px; border-radius: 10px; padding: 0px 10px;"><i class="fa fa-tag"></i> Instruction-Following</a>
          </div>
          <script type="text/javascript">
              var tagsall=document.getElementsByClassName("post-tags")
              for (var i = tagsall.length - 1; i >= 0; i--){
                  var tags=tagsall[i].getElementsByTagName("a");
                  for (var j = tags.length - 1; j >= 0; j--) {
                      var r=Math.floor(Math.random()*75+200);
                      var g=Math.floor(Math.random()*75+200);
                      var b=Math.floor(Math.random()*75+200);
                      tags[j].style.background = "rgb("+r+","+g+","+b+")";
                  }
              }                        
            </script>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-0.png" /></p>
<p>MIMIC-IT Paper: <a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/2306.05425.pdf">https://arxiv.org/pdf/2306.05425.pdf</a></p>
<p>Otter Paper: <a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.03726.pdf">https://arxiv.org/pdf/2305.03726.pdf</a></p>
<p>Code: <a
target="_blank" rel="noopener" href="https://github.com/Luodian/Otter">https://github.com/Luodian/Otter</a></p>
<p>Project Page: <a
target="_blank" rel="noopener" href="https://otter-ntu.github.io/">https://otter-ntu.github.io/</a></p>
<aside>
<p>👇 Problems</p>
</aside>
<p>Current VL instruction-response pairs in terms of quantity, diversity
and creativity are limited, posing challenges to the generalization of
interactive VLMs.</p>
<aside>
<p>👇 Contributions</p>
</aside>
<ul>
<li><strong>MIMIC-IT</strong>: dataset with 2.8M instruction-response
pairs</li>
<li><strong>Syphus</strong>: automatic instruction-response generation
pipeline &gt; Its filtering does not consider whether the visual info
and textual info are paired or not. &gt;</li>
<li><strong>Otter</strong>: improve OpenFlamingo in perception,
reasoning, and planning via finetuning it on MIMIC-IT</li>
</ul>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT%20Overview.jpg"
alt="MIMIC-IT overview. Each instruction is accompanied by multi-modal conversational context." />
<figcaption aria-hidden="true">MIMIC-IT overview. Each instruction is
accompanied by multi-modal conversational context.</figcaption>
</figure>
<h1 id="related-workmulti-modal-instruction-tuning-dataset">1 - Related
Work(Multi-modal Instruction Tuning Dataset)</h1>
<blockquote>
<p>One potential reason for the zero-shot performance gain by
instruction tuning is that it internalizes the <a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/2209.15189.pdf">context</a>, which is
preferred in user interactions especially when user input skips
commonsense context.</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2212.10773.pdf">Multi-Instruct</a>:
Initially introduces the notion of instruction tuning in multi-modal
models</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2304.10592.pdf">Mini-GPT4</a>: creates
its instruction-based dataset by merging Conceptual Caption, SBU, and
LAION with handwritten instruction templates.</p>
<aside>
<p>👇 <a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/2304.08485.pdf">LLaVA-Instruct-150K</a>
elevates the quality of VL instruction-following datasets.</p>
</aside>
<p>Utilizing self-instruct and handwritten seed instructions,
<strong>LLaVA-Instruct-150K</strong> obtains instructions and responses
from GPT-4 based on COCO image captions and object bounding boxes.</p>
<p><strong><em>Its three limitations:</em></strong></p>
<ol type="1">
<li><strong>Limited visual diversity</strong>: only COCO images.</li>
<li><strong>Single image as visual data</strong>: cannot process
multiple images or videos.</li>
<li><strong>Language-only in-context information</strong>: no
multi-modal in-context info</li>
</ol>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-1.png"
alt="LLaVA-Instruct-150K: Language-only In-context." />
<figcaption aria-hidden="true">LLaVA-Instruct-150K: Language-only
In-context.</figcaption>
</figure>
<aside>
<p>👇 Three characteristics of MIMIC-IT</p>
</aside>
<blockquote>
<p>While these previous works focuse on general scene images, MIMIC-IT
categorizes our data sources into indoor scenes, outdoor scenes,
conversations, and egocentric videos.</p>
</blockquote>
<ol type="1">
<li><strong>Diverse visual scenes:</strong> incorporating images and
videos from general scenes, egocentric view scenes, and indoor RGB-D
images.</li>
<li><strong>Multiple images (or a video) as visual data</strong>:
supporting instruction-response pairs accompanied by any number of
images or videos.</li>
<li><strong>Multi-modal in-context information</strong>: including
multiple instruction-response pairs and multiple images or videos</li>
</ol>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-2.png"
alt="MIMIC-IT: Multi-modal In-context. Left part is in-context, right part is the query." />
<figcaption aria-hidden="true">MIMIC-IT: Multi-modal In-context. Left
part is in-context, right part is the query.</figcaption>
</figure>
<h1 id="multi-modal-in-context-instruction-tuning-dataset">2 -
Multi-modal In-Context Instruction Tuning Dataset</h1>
<h2 id="mimic-it-data-format">2.1 - MIMIC-IT Data Format</h2>
<p><span class="math inline">\(I_q\)</span> denotes the <span
class="math inline">\(q\)</span>-th instruction, <span
class="math inline">\(R_q\)</span> is the response, <span
class="math inline">\(X_q\)</span> refers to images or videos.</p>
<blockquote>
<p>Videos can be viewed as ordered sequences of images.</p>
</blockquote>
<p>A <strong>query example</strong> is a tuple <span
class="math inline">\((I_q, R_q, X_q)\)</span>, where <span
class="math inline">\(\{x_{j=1}^N\}\in X_q\)</span>, <span
class="math inline">\(N\)</span> is image num.</p>
<aside>
<p>👆 <span class="math inline">\(p_{\theta}(R_q|(I_q,X_q))\)</span> is
the <strong>standard instruction-tuning process</strong> with trainable
params <span class="math inline">\(\theta\)</span>.</p>
</aside>
<p><span
class="math inline">\(C_{\psi}:(I_q,X_q)\mapsto\{(I_k,X_k)\}_{k=1}^M\)</span>
are <span class="math inline">\(M\)</span> <strong>in-context
examples</strong> of current query example.(The function is
task-dependent.)</p>
<p><strong>MIMIC-IT Data Format</strong>: <span
class="math inline">\(d_q=(I_q,R_q,X_q,C_{\psi}(I_q,X_q)),d_q\sim
D_{MIMIC-IT}\)</span>.</p>
<aside>
<p>👆 <span
class="math inline">\(p_{\theta}(R_q|(I_q,X_q,C_{\psi}(I_q,X_q)))\)</span>
incorporates in-context examples.</p>
</aside>
<h2 id="sythus-automatic-instruction-response-generation-pipeline">2.2 -
Sythus: Automatic Instruction-Response Generation Pipeline</h2>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-3.png"
alt="Sythus overview. Step 4 expands pairs into 8 languages via GPT—— Chinese (zh), Japanese (ja), Spanish (es), German (de), French (fr), Korean (ko), and Arabic (ar)." />
<figcaption aria-hidden="true">Sythus overview. Step 4 expands pairs
into 8 languages via GPT—— Chinese (zh), Japanese (ja), Spanish (es),
German (de), French (fr), Korean (ko), and Arabic (ar).</figcaption>
</figure>
<p><strong>System messages</strong> define the desired tone and style of
the generated instruction-response(IR) pairs.</p>
<p><strong>Visual annotations</strong> provide essential image
information such as bounding boxes, captions, timestamps.</p>
<p><strong>In-context examples</strong> assist ChatGPT in learning
within the context.</p>
<p><strong>Cold-start stage</strong> is before the large-scale query.
This stage identifies the optimal system message and in-context example
for each specific task via prompting ChatGPT.</p>
<p><strong>Safety and Ethical Filtering</strong>: The GPT content policy
eliminates output that is suspicious for unfair opportunities,
stereotyping, overrepresentation/underrepresentation, explicit content,
disinformation, or unreliable information.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-4.png"
alt="System message for TV show Captions (TVC) query." />
<figcaption aria-hidden="true">System message for TV show Captions (TVC)
query.</figcaption>
</figure>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-5.png"
alt="In-context exemplars for TV show Captions (TVC) query." />
<figcaption aria-hidden="true">In-context exemplars for TV show Captions
(TVC) query.</figcaption>
</figure>
<h2 id="visual-data-exploration">2.3 - Visual Data Exploration</h2>
<p><strong>Why we leverage existing datasets?</strong></p>
<ol type="1">
<li>high-quality visual annotations (2) align with real-world
distribution</li>
</ol>
<blockquote>
<p>In each sub-task, we elaborate on the process of organizing various
data into an in-context instruction tuning format.</p>
</blockquote>
<h3 id="general-scene-understanding">2.3.1 - General Scene
Understanding</h3>
<aside>
<p>👇 <strong>LLaVA-Interleaved (LA-I)</strong>.</p>
</aside>
<p>Retrieve ten in-context examples for each instruction-response pair
in <a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/2304.08485.pdf">LLaVA-Instruct-150K</a>
based on instruction text2text similarity(<strong>TTS</strong>) or
image2image similarity(<strong>IIS</strong>).</p>
<p>Trained on the LA task, the model exhibits exceptional scene
comprehension, reasoning abilities, and multi-round conversation
capabilities.</p>
<aside>
<p>👇 <strong>Spot The Difference (SD)</strong>. Identify scene
differences between the paired images with varying complexity
levels.</p>
</aside>
<ul>
<li><strong>General Scene Difference</strong>: (1) create image pairs
from the <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1405.0312.pdf">COCO2017</a> via
<strong>IIS</strong> (2) use image captions and object detection
annotations</li>
<li><strong>Subtle Difference</strong>: (1) image pairs from <a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1808.10584.pdf">Spot-the-Diff</a>(extracted
from surveillance footage) (2) employ natural language difference
descriptions as annotations</li>
</ul>
<aside>
<p>👇 <strong>Visual Story Telling (VIST)</strong>. Generate coherent
and engaging narratives based on visual input.</p>
</aside>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1604.03968.pdf">Visual
Storytelling</a> includes event-based image sequences and corresponding
inquiry questions.</p>
<p>Given that image annotations often contain narratives and timelines
not directly observable, we instruct ChatGPT to act as a viewer
answering questions about the images.</p>
<p>The prompts also incorporate thought-provoking inquiries to promote
creativity.</p>
<aside>
<p>👇 <strong>Dense Captions (DC)</strong>.</p>
</aside>
<p>Expanding the scope of video understanding, DC features dense
captions from <a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1705.00754.pdf">DenseCaption/Activity
caption</a> corresponding to clips within longer videos. The
instructions pose a diverse set of questions, addressing the general
visual content of the video, human actions, and behaviors, the
chronological sequence of events, and causal relationships. This
approach encourages VLMs to delve deeper into the intricacies of video
content.</p>
<aside>
<p>👇 <strong>TV Show Captions (TVC)</strong>.</p>
</aside>
<p>The primary purpose of incorporating TV show clips with high-level
captions into the training process of VLMs is to enhance their social
reasoning abilities and deepen their understanding of complex character
dynamics. By organizing drama clips from <a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/2001.09099.pdf">TVR</a> to analyze character
relationships and motivations, we aim to challenge VLMs to move beyond
mere perception and demonstrate their reasoning capabilities within the
context of TV show narratives. This focused approach is crucial for
fostering advanced VLMs capable of effectively handling diverse
real-world situations and user queries.</p>
<h3 id="egocentric-view-understanding">2.3.2 - Egocentric View
Understanding</h3>
<aside>
<p>👇 <strong>Indoor Event Planning (IEP)</strong>. Planning
capabilities for diverse 2D room photos.</p>
</aside>
<p><strong>2D Photos Sampling</strong>: We gather indoor scene RGB-D
images from <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1702.04405.pdf">ScanNetv2</a>
and sample them into multiple 2D visual inputs, representing a room’s
layout from a first-person perspective.</p>
<p><strong>Instructions Generation</strong> via prompting ChatGPT:
direct humans to perform various activities in indoor spaces.</p>
<ul>
<li>Initially, we have ChatGPT create a personality for the room
owner.</li>
<li>Subsequently, the planning should be intimately related to the
room’s layout and the generated room owner, underlining the importance
of context awareness in VLMs.</li>
</ul>
<aside>
<p>👇 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2110.07058.pdf">Ego4D (E4D,
videos)</a>. For first-person augmented reality (AR) headset assistant
applications.</p>
</aside>
<p>By prompting ChatGPT to generate instructions based on visual
descriptions, our goal is to simulate practical interactions between
users and AR assistants.</p>
<p>An instance of assistant-related questions and context-aware
responses:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Instruction: What should I do now?</span><br><span class="line">Response: Based on my observation, you can now proceed to do....</span><br></pre></td></tr></table></figure>
<h2 id="dataset-statistics">2.4 - Dataset Statistics</h2>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-6.png"
alt="Comparison between MIMIC-IT and other multi-modal instruction datasets. MIMICIT features: (1) The largest. (2) Including video data. (3) In-context scenarios. (4) Multilingual." />
<figcaption aria-hidden="true">Comparison between MIMIC-IT and other
multi-modal instruction datasets. MIMICIT features: (1) The largest. (2)
Including video data. (3) In-context scenarios. (4)
Multilingual.</figcaption>
</figure>
<p><strong>2.8M instruction-response pairs</strong>(2.2M unique
instructions): each pair includes at least one multi-modal in-context
example and one language-only in-context example.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-7.png"
alt="The data statistics of MIMIC-IT. (c) retains 25% of Ego4D instructions for a more balanced distribution." />
<figcaption aria-hidden="true">The data statistics of MIMIC-IT. (c)
retains 25% of Ego4D instructions for a more balanced
distribution.</figcaption>
</figure>
<ol type="a">
<li><p>and (b) examine the characteristics and diversity using <a
target="_blank" rel="noopener" href="https://github.com/explosion/spacy-models/releases/tag/en_core_web_md-3.5.0">spaCy</a>
to get top 20 most frequent root verbs alongside their top 4 direct noun
objects.</p></li>
<li><p>demonstrates diversity in terms of instructions/responses length,
image num per instruction, and in-context examples num per
instruction.</p></li>
</ol>
<h1 id="empricial-evaluation">3 - Empricial Evaluation</h1>
<h2 id="otter-a-multi-modal-in-context-instruction-tuned-model">3.1 -
Otter: A Multi-Modal In-Context Instruction Tuned Model</h2>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Otter.png" /></p>
<p><strong>Language Encoder</strong>: LLaMA-7B(frozen)</p>
<p><strong>Vision Encoder</strong>: CLIP ViT-L/14(frozen)</p>
<p><strong>Perceiver Resampler Module</strong>: ~1.3B trainable
parameters</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># [] indicates special word, &lt;&gt; indicates data slot</span><br><span class="line"># role label: User/GPT</span><br><span class="line"># format the training data as follows(chatbot-like format):</span><br><span class="line">&lt;context&gt; [image] User:&lt;instruction&gt; GPT:[answers] &lt;answer&gt;.[endofchunk]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>To support user-assistant conversations, we adopt "GPT" as the role
label because it does not have any specific semantic meaning in
vocabulary.</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Hyperparameters</span><br><span class="line">optimizer: AdamW</span><br><span class="line">starting learning rate(lr): 10^&#123;-5&#125;</span><br><span class="line">batch size: 4</span><br><span class="line">epoch: 6</span><br><span class="line">lr scheduler: cosine annealing scheduler</span><br><span class="line">gradient clipping threshold: 1.0</span><br><span class="line">loss: cross-entropy</span><br></pre></td></tr></table></figure>
<aside>
<p>👇 HuggingFace</p>
</aside>
<ul>
<li><a
target="_blank" rel="noopener" href="https://huggingface.co/luodian/OTTER-Image-LLaMA7B-LA-InContext">OTTER-Image-LLaMA7B-LA-InContext</a>:
as described in paper</li>
<li><a
target="_blank" rel="noopener" href="https://huggingface.co/luodian/OTTER-Image-MPT7B">OTTER-Image-MPT7B</a>:
Tune OpenFlamingv2 to enable generation abilities for both long and
short answers.</li>
<li><a
target="_blank" rel="noopener" href="https://huggingface.co/luodian/OTTER-Video-LLaMA7B-DenseCaption">OTTER-Video-LLaMA7B-DenseCaption</a></li>
<li><a
target="_blank" rel="noopener" href="https://huggingface.co/luodian/OTTER-Video-LLaMA7B-FunQA">OTTER-Video-LLaMA7B-FunQA</a></li>
</ul>
<blockquote>
<p><strong>Otter-Image</strong> supports multiple images input as
in-context examples, which is the first multi-modal instruction tuned
model that supports to organize inputs this way.</p>
</blockquote>
<blockquote>
<p><strong>Otter-Video</strong> supports videos inputs (frames are
arranged as original Flamingo's implementation) and multiple images
inputs (they serve as in-context examples for each other).</p>
</blockquote>
<aside>
<p>👇 Engineering Work</p>
</aside>
<p>We have <a
target="_blank" rel="noopener" href="https://huggingface.co/luodian/OTTER-9B-INIT">integrated Otter
into Hugging Face Transformers</a> and trained it using the <a
target="_blank" rel="noopener" href="https://huggingface.co/docs/accelerate/index">Hugging Face
Accelerator</a>.</p>
<p>We use <strong>bf16 mixed precision</strong> and train Otter on
<strong>4×RTX-3090 GPUs(24GB)</strong>.</p>
<blockquote>
<p>reduced from 1× A100 GPU</p>
</blockquote>
<p>We provide the support of Fully Sharded Data Parallel (FSDP) and
DeepSpeed.</p>
<p>We provide a <strong>script for converting</strong> the original
OpenFlamingo-9B checkpoint into the Hugging Face Model format.(<a
target="_blank" rel="noopener" href="https://huggingface.co/luodian/openflamingo-9b-hf">luodian/openflamingo-9b-hf</a>)</p>
<h2 id="usage-examples-and-demonstrations">3.2 - Usage Examples and
Demonstrations</h2>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Otter-example.png"
alt="Otter’s response examples in different scenarios. Otter is able to serve for situation understanding and reasoning, learning with in-context examples, and egocentric visual assistant." />
<figcaption aria-hidden="true"><strong>Otter’s response examples in
different scenarios</strong>. Otter is able to serve for situation
understanding and reasoning, learning with in-context examples, and
egocentric visual assistant.</figcaption>
</figure>
<p><strong>Otter’s response examples in different scenarios</strong>.
Otter is able to serve for situation understanding and reasoning,
learning with in-context examples, and egocentric visual assistant.</p>
<p><strong>Scene Understanding and Reasoning</strong>.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;image&gt;Human:&#123;instruction&#125; Assistant:&lt;answer&gt;&#123;model-generated response&#125;&lt;endofchunk&gt;</span><br></pre></td></tr></table></figure>
<p><strong>Learning with In-context Examples</strong>.</p>
<p>The organized input data format(on the LA-T2T task) is as
follows:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># Multiple in-context example with similar instructions</span><br><span class="line">&lt;image&gt;Human:&#123;instruction&#125; Assistant:&lt;answer&gt;&#123;response&#125;&lt;|endofchunk|&gt;</span><br><span class="line"># ....</span><br><span class="line">&lt;image&gt;Human:&#123;instruction&#125; Assistant:&lt;answer&gt;&#123;response&#125;&lt;|endofchunk|&gt;</span><br><span class="line"># Query example</span><br><span class="line">&lt;image&gt;Human:&#123;instruction&#125; Assistant:&lt;answer&gt;</span><br></pre></td></tr></table></figure>
<p><strong>Egocentric Visual Assistant(Otter-E)</strong>.</p>
<p>Otter-E is specifically designed for AR headset applications.</p>
<p>In real-life scenarios, you are not encouraged to consult visual
assistants for such hazardous actions.</p>
<h2 id="chatgpt-evaluation">3.3 - ChatGPT Evaluation</h2>
<blockquote>
<p>Current evaluation metrics for VL models, like VQAv2 acc, exhibit
shortcomings in terms of robustness.</p>
</blockquote>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-8.png"
alt="MMAGIBench evaluation results judged by ChatGPT." />
<figcaption aria-hidden="true"><strong><a
target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmagibench">MMAGIBench</a>
evaluation results</strong> judged by ChatGPT.</figcaption>
</figure>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-9.png"
alt="(a) ChatGPT evaluation. (b) Human evaluation comparison. (c) Few-shot in-context learning evaluation on COCO caption (CIDEr)." />
<figcaption aria-hidden="true">(a) ChatGPT evaluation. (b) Human
evaluation comparison. (c) Few-shot in-context learning evaluation on <a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1405.0312.pdf">COCO caption
(CIDEr)</a>.</figcaption>
</figure>
<h2 id="human-evaluation">3.4 - Human Evaluation</h2>
<p><a
target="_blank" rel="noopener" href="https://github.com/OpenGVLab/Multi-Modality-Arena">Multi-Modality
Arena</a> uses an Elo rating(higher is better) system to evaluate
<strong>the usefulness and alignment</strong> of VLM responses.</p>
<p>The system calculates the relative skill levels of players.</p>
<p>This system works well for evaluating conversational AI models,
because multiple models can have pairwise "battles" responding to the
same inputs in a user-blind evaluation.</p>
<h2 id="few-shot-in-context-learning-metric-evaluation">3.5 - Few-shot
In-Context Learning Metric Evaluation</h2>
<p>As expected, the finetuning also brings marginal performance gain on
zero-shot evaluation.</p>
<h1 id="conclusion">4 - Conclusion</h1>
<aside>
<p>👇 <strong>Limitations</strong></p>
</aside>
<p>For MIMIC-IT:</p>
<p>ChatGPT is prone to language hallucinations, therefore it might
generate incorrect responses.</p>
<p>For Otter:</p>
<ul>
<li><strong>Object hallucinations</strong>(objects mentioned in text
inputs do not appeare in images or videos)</li>
<li><strong>Language hallucinations</strong> from OpenFlamingo is
inherited by Otter.</li>
</ul>
<aside>
<p>👇 <strong>Future Works</strong></p>
</aside>
<p>For MIMIC-IT:</p>
<ul>
<li>More trustworthy LMs or generation techniques for self-instruct data
generation.</li>
<li>More embodied AI datasets such as <a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/2210.06407.pdf">LanguageTable</a> and <a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.01691.pdf">SayCan</a>.</li>
</ul>
<p>For Otter:</p>
<ul>
<li>introducing negative examples in the training data to reduce
hallucination issue</li>
<li>more efficient training schemas(e.g., LoRA)</li>
<li>more modalities.</li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>Post author:  </strong>Xin Liu
  </li>
  <li class="post-copyright-link">
      <strong>Post link: </strong>
      <a href="https://ifshinelx.github.io/2023/07/04/MIMIC-IT/" title="MIMIC-IT: Multi-Modal In-Context Instruction Tuning">https://ifshinelx.github.io/2023/07/04/MIMIC-IT/</a>
  </li>
  <li class="post-copyright-license">
      <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>


        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/07/03/LENS/" rel="prev" title="(LENS)Towards Language Models That Can See: Computer Vision Through the LENS🔍 of Natural Language">
                  <i class="fa fa-chevron-left"></i> (LENS)Towards Language Models That Can See: Computer Vision Through the LENS🔍 of Natural Language
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/07/06/Lynx/" rel="next" title="(Lynx)What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?">
                  (Lynx)What Matters in Training a GPT4-Style Language Model with Multimodal Inputs? <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Xin Liu</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="far fa-file-word"></i>
    </span>
    <span title="Word count total">16k</span>
  </span>

</div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"ifshinelx","repo":"GitalkForBlog","client_id":"0cbb3af62c4f4096c907","client_secret":"3b535adf6818071ff1dc37a695e585cf044a2541","admin_user":"ifshinelx","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":"en","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"a69aea55fdebb7acd67a30d5eeef0ff3"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
