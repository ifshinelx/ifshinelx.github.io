<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/little%20star.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/little%20star.jpg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"ifshinelx.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.17.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>



<link rel="canonical" href="https://ifshinelx.github.io/2023/07/04/MIMIC-IT/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://ifshinelx.github.io/2023/07/04/MIMIC-IT/","path":"2023/07/04/MIMIC-IT/","title":"MIMIC-IT: Multi-Modal In-Context Instruction Tuning"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>MIMIC-IT: Multi-Modal In-Context Instruction Tuning | XinLiu's Homepage, Welcome!</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">XinLiu's Homepage, Welcome!</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-Related-Work-Multi-modal-Instruction-Tuning-Dataset"><span class="nav-number">1.</span> <span class="nav-text">1 - Related Work(Multi-modal Instruction Tuning Dataset)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-Multi-modal-In-Context-Instruction-Tuning-Dataset"><span class="nav-number">2.</span> <span class="nav-text">2 - Multi-modal In-Context Instruction Tuning Dataset</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-MIMIC-IT-Data-Format"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 - MIMIC-IT Data Format</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-Sythus-Automatic-Instruction-Response-Generation-Pipeline"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 - Sythus: Automatic Instruction-Response Generation Pipeline</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-Visual-Data-Exploration"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 - Visual Data Exploration</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-1-General-Scene-Understanding"><span class="nav-number">2.3.1.</span> <span class="nav-text">2.3.1 - General Scene Understanding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-2-Egocentric-View-Understanding"><span class="nav-number">2.3.2.</span> <span class="nav-text">2.3.2 - Egocentric View Understanding</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4-Dataset-Statistics"><span class="nav-number">2.4.</span> <span class="nav-text">2.4 - Dataset Statistics</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-Empricial-Evaluation"><span class="nav-number">3.</span> <span class="nav-text">3 - Empricial Evaluation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-Otter-A-Multi-Modal-In-Context-Instruction-Tuned-Model"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 - Otter: A Multi-Modal In-Context Instruction Tuned Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-Usage-Examples-and-Demonstrations"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 - Usage Examples and Demonstrations</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-ChatGPT-Evaluation"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 - ChatGPT Evaluation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-4-Human-Evaluation"><span class="nav-number">3.4.</span> <span class="nav-text">3.4 - Human Evaluation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-5-Few-shot-In-Context-Learning-Metric-Evaluation"><span class="nav-number">3.5.</span> <span class="nav-text">3.5 - Few-shot In-Context Learning Metric Evaluation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-Conclusion"><span class="nav-number">4.</span> <span class="nav-text">4 - Conclusion</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Xin Liu"
      src="/images/little%20star.jpg">
  <p class="site-author-name" itemprop="name">Xin Liu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">11</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/ifshinelx" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ifshinelx" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:ifshine_lx@163.com" title="E-Mail → mailto:ifshine_lx@163.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



  <div class="links-of-blogroll motion-element links-of-blogroll-block">
    <div class="links-of-blogroll-title">
      <!-- modify icon to fire by szw -->
      <i class="fa fa-history fa-" aria-hidden="true"></i>
      Recent Posts
    </div>
    <ul class="links-of-blogroll-list" style="padding: 0px 12px;">
      
      
      
        
          <li class="recent_posts_li">
            <a href="/2023/07/09/Shikra/" title="Shikra: Unleashing Multimodal LLM’s Referential Dialogue Magic" target="_blank">07-09 Shikra: Unleashing Multimodal LLM’s Referential Dialogue Magic </a>
          </li>
        
      
        
          <li class="recent_posts_li">
            <a href="/2023/07/06/Lynx/" title="(Lynx)What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?" target="_blank">07-06 (Lynx)What Matters in Training a GPT4-Style Language Model with Multimodal Inputs? </a>
          </li>
        
      
        
          <li class="recent_posts_li">
            <a href="/2023/07/04/MIMIC-IT/" title="MIMIC-IT: Multi-Modal In-Context Instruction Tuning" target="_blank">07-04 MIMIC-IT: Multi-Modal In-Context Instruction Tuning </a>
          </li>
        
      
        
          <li class="recent_posts_li">
            <a href="/2023/07/03/LENS/" title="(LENS)Towards Language Models That Can See: Computer Vision Through the LENS🔍 of Natural Language" target="_blank">07-03 (LENS)Towards Language Models That Can See: Computer Vision Through the LENS🔍 of Natural Language </a>
          </li>
        
      
        
          <li class="recent_posts_li">
            <a href="/2023/07/01/LongMem/" title="(LongMem)Augmenting Language Models with Long-Term Memory" target="_blank">07-01 (LongMem)Augmenting Language Models with Long-Term Memory </a>
          </li>
        
      
    </ul>
  </div>

  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://ifshinelx.github.io/2023/07/04/MIMIC-IT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/little%20star.jpg">
      <meta itemprop="name" content="Xin Liu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="XinLiu's Homepage, Welcome!">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="MIMIC-IT: Multi-Modal In-Context Instruction Tuning | XinLiu's Homepage, Welcome!">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          MIMIC-IT: Multi-Modal In-Context Instruction Tuning
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-07-04 13:00:00" itemprop="dateCreated datePublished" datetime="2023-07-04T13:00:00+08:00">2023-07-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-07-10 21:10:24" itemprop="dateModified" datetime="2023-07-10T21:10:24+08:00">2023-07-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper/" itemprop="url" rel="index"><span itemprop="name">Paper</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper/2023/" itemprop="url" rel="index"><span itemprop="name">2023</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper/2023/06/" itemprop="url" rel="index"><span itemprop="name">06</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>1.9k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>7 mins.</span>
    </span>
</div>

        </div>
          <div class="post-tags" style="margin-top: 5px;">
              <a href="/tags/LLM/" rel="tag" style="border: 0px; border-radius: 10px; padding: 0px 10px;"><i class="fa fa-tag"></i> LLM</a>
              <a href="/tags/Dataset/" rel="tag" style="border: 0px; border-radius: 10px; padding: 0px 10px;"><i class="fa fa-tag"></i> Dataset</a>
              <a href="/tags/In-Context-Learning/" rel="tag" style="border: 0px; border-radius: 10px; padding: 0px 10px;"><i class="fa fa-tag"></i> In-Context Learning</a>
              <a href="/tags/Instrcution-Following/" rel="tag" style="border: 0px; border-radius: 10px; padding: 0px 10px;"><i class="fa fa-tag"></i> Instrcution-Following</a>
              <a href="/tags/Multi-modal/" rel="tag" style="border: 0px; border-radius: 10px; padding: 0px 10px;"><i class="fa fa-tag"></i> Multi-modal</a>
          </div>
          <script type="text/javascript">
              var tagsall=document.getElementsByClassName("post-tags")
              for (var i = tagsall.length - 1; i >= 0; i--){
                  var tags=tagsall[i].getElementsByTagName("a");
                  for (var j = tags.length - 1; j >= 0; j--) {
                      var r=Math.floor(Math.random()*75+200);
                      var g=Math.floor(Math.random()*75+200);
                      var b=Math.floor(Math.random()*75+200);
                      tags[j].style.background = "rgb("+r+","+g+","+b+")";
                  }
              }                        
            </script>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-0.png"></p>
<p>MIMIC-IT Paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2306.05425.pdf">https://arxiv.org/pdf/2306.05425.pdf</a></p>
<p>Otter Paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.03726.pdf">https://arxiv.org/pdf/2305.03726.pdf</a></p>
<p>Code: <a target="_blank" rel="noopener" href="https://github.com/Luodian/Otter">https://github.com/Luodian/Otter</a></p>
<p>Project Page: <a target="_blank" rel="noopener" href="https://otter-ntu.github.io/">https://otter-ntu.github.io/</a></p>
<aside>
👇 Problems

</aside>

<p>Current VL instruction-response pairs in terms of quantity, diversity and creativity are limited, posing challenges to the generalization of interactive VLMs.</p>
<aside>
👇 Contributions

</aside>

<ul>
<li><strong>MIMIC-IT</strong>: dataset with 2.8M instruction-response pairs</li>
<li><strong>Syphus</strong>: automatic instruction-response generation pipeline<blockquote>
<p>Its filtering does not consider whether the visual info and textual info are paired or not.</p>
</blockquote>
</li>
<li><strong>Otter</strong>: improve OpenFlamingo in perception, reasoning, and planning via finetuning it on MIMIC-IT</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT%20Overview.jpg" alt="MIMIC-IT overview. Each instruction is accompanied by multi-modal conversational context."></p>
<h1 id="1-Related-Work-Multi-modal-Instruction-Tuning-Dataset"><a href="#1-Related-Work-Multi-modal-Instruction-Tuning-Dataset" class="headerlink" title="1 - Related Work(Multi-modal Instruction Tuning Dataset)"></a>1 - Related Work(Multi-modal Instruction Tuning Dataset)</h1><blockquote>
<p>One potential reason for the zero-shot performance gain by instruction tuning is that it internalizes the <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2209.15189.pdf">context</a>, which is preferred in user interactions especially when user input skips commonsense context.</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2212.10773.pdf">Multi-Instruct</a>: Initially introduces the notion of instruction tuning in multi-modal models</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2304.10592.pdf">Mini-GPT4</a>: creates its instruction-based dataset by merging Conceptual Caption, SBU, and LAION with handwritten instruction templates.</p>
<aside>
👇 [LLaVA-Instruct-150K](https://arxiv.org/pdf/2304.08485.pdf) elevates the quality of VL instruction-following datasets.

</aside>

<p>Utilizing self-instruct and handwritten seed instructions, <strong>LLaVA-Instruct-150K</strong> obtains instructions and responses from GPT-4 based on COCO image captions and object bounding boxes.</p>
<p><em><strong>Its three limitations:</strong></em></p>
<ol>
<li><strong>Limited visual diversity</strong>: only COCO images.</li>
<li><strong>Single image as visual data</strong>: cannot process multiple images or videos.</li>
<li><strong>Language-only in-context information</strong>: no multi-modal in-context info</li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-1.png" alt="LLaVA-Instruct-150K: Language-only In-context."></p>
<aside>
👇 Three characteristics of MIMIC-IT

</aside>

<blockquote>
<p>While these previous works focuse on general scene images, MIMIC-IT categorizes our data sources into indoor scenes, outdoor scenes, conversations, and egocentric videos.</p>
</blockquote>
<ol>
<li><strong>Diverse visual scenes:</strong> incorporating images and videos from general scenes, egocentric view scenes, and indoor RGB-D images.</li>
<li><strong>Multiple images (or a video) as visual data</strong>: supporting instruction-response pairs accompanied by any number of images or videos.</li>
<li><strong>Multi-modal in-context information</strong>: including multiple instruction-response pairs and multiple images or videos</li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-2.png" alt="MIMIC-IT: Multi-modal In-context. Left part is in-context, right part is the query."></p>
<h1 id="2-Multi-modal-In-Context-Instruction-Tuning-Dataset"><a href="#2-Multi-modal-In-Context-Instruction-Tuning-Dataset" class="headerlink" title="2 - Multi-modal In-Context Instruction Tuning Dataset"></a>2 - Multi-modal In-Context Instruction Tuning Dataset</h1><h2 id="2-1-MIMIC-IT-Data-Format"><a href="#2-1-MIMIC-IT-Data-Format" class="headerlink" title="2.1 - MIMIC-IT Data Format"></a>2.1 - MIMIC-IT Data Format</h2><p>$I_q$ denotes the $q$-th instruction, $R_q$ is the response, $X_q$ refers to images or videos.</p>
<blockquote>
<p>Videos can be viewed as ordered sequences of images.</p>
</blockquote>
<p>A <strong>query example</strong> is a tuple $(I_q, R_q, X_q)$, where ${x_{j&#x3D;1}^N}\in X_q$, $N$ is image num.</p>
<aside>
👆 $p_{\theta}(R_q|(I_q,X_q))$ is the **standard instruction-tuning process** with trainable params $\theta$.

</aside>

<p>$C_{\psi}:(I_q,X_q)\mapsto{(I_k,X_k)}_{k&#x3D;1}^M$ are $M$ <strong>in-context examples</strong> of current query example.(The function is task-dependent.)</p>
<p><strong>MIMIC-IT Data Format</strong>: $d_q&#x3D;(I_q,R_q,X_q,C_{\psi}(I_q,X_q)),d_q\sim D_{MIMIC-IT}$.</p>
<aside>
👆 $p_{\theta}(R_q|(I_q,X_q,C_{\psi}(I_q,X_q)))$ incorporates in-context examples.

</aside>

<h2 id="2-2-Sythus-Automatic-Instruction-Response-Generation-Pipeline"><a href="#2-2-Sythus-Automatic-Instruction-Response-Generation-Pipeline" class="headerlink" title="2.2 - Sythus: Automatic Instruction-Response Generation Pipeline"></a>2.2 - Sythus: Automatic Instruction-Response Generation Pipeline</h2><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-3.png" alt="Sythus overview. Step 4 expands pairs into 8 languages via GPT—— Chinese (zh), Japanese (ja), Spanish (es), German (de), French (fr), Korean (ko), and Arabic (ar)."></p>
<p><strong>System messages</strong> define the desired tone and style of the generated instruction-response(IR) pairs.</p>
<p><strong>Visual annotations</strong> provide essential image information such as bounding boxes, captions, timestamps.</p>
<p><strong>In-context examples</strong> assist ChatGPT in learning within the context.</p>
<p><strong>Cold-start stage</strong> is before the large-scale query. This stage identifies the optimal system message and in-context example for each specific task via prompting ChatGPT.</p>
<p><strong>Safety and Ethical Filtering</strong>: The GPT content policy eliminates output that is suspicious for unfair opportunities, stereotyping, overrepresentation&#x2F;underrepresentation, explicit content, disinformation, or unreliable information.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-4.png" alt="System message for TV show Captions (TVC) query."></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-5.png" alt="In-context exemplars for TV show Captions (TVC) query."></p>
<h2 id="2-3-Visual-Data-Exploration"><a href="#2-3-Visual-Data-Exploration" class="headerlink" title="2.3 - Visual Data Exploration"></a>2.3 - Visual Data Exploration</h2><p><strong>Why we leverage existing datasets?</strong></p>
<p>(1) high-quality visual annotations (2) align with real-world distribution</p>
<blockquote>
<p>In each sub-task, we elaborate on the process of organizing various data into an in-context instruction tuning format.</p>
</blockquote>
<h3 id="2-3-1-General-Scene-Understanding"><a href="#2-3-1-General-Scene-Understanding" class="headerlink" title="2.3.1 - General Scene Understanding"></a>2.3.1 - General Scene Understanding</h3><aside>
👇 **LLaVA-Interleaved (LA-I)**.

</aside>

<p>Retrieve ten in-context examples for each instruction-response pair in <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2304.08485.pdf">LLaVA-Instruct-150K</a> based on instruction text2text similarity(<strong>TTS</strong>) or image2image similarity(<strong>IIS</strong>).</p>
<p>Trained on the LA task, the model exhibits exceptional scene comprehension, reasoning abilities, and multi-round conversation capabilities.</p>
<aside>
👇 **Spot The Difference (SD)**. Identify scene differences between the paired images with varying complexity levels.

</aside>

<ul>
<li><strong>General Scene Difference</strong>: (1) create image pairs from the <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1405.0312.pdf">COCO2017</a> via <strong>IIS</strong> (2) use image captions and object detection annotations</li>
<li><strong>Subtle Difference</strong>: (1) image pairs from <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1808.10584.pdf">Spot-the-Diff</a>(extracted from surveillance footage) (2) employ natural language difference descriptions as annotations</li>
</ul>
<aside>
👇 **Visual Story Telling (VIST)**. Generate coherent and engaging narratives based on visual input.

</aside>

<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1604.03968.pdf">Visual Storytelling</a> includes event-based image sequences and corresponding inquiry questions. </p>
<p>Given that image annotations often contain narratives and timelines not directly observable, we instruct ChatGPT to act as a viewer answering questions about the images. </p>
<p>The prompts also incorporate thought-provoking inquiries to promote creativity. </p>
<aside>
👇 **Dense Captions (DC)**.

</aside>

<p>Expanding the scope of video understanding, DC features dense captions from <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1705.00754.pdf">DenseCaption&#x2F;Activity caption</a> corresponding to clips within longer videos. The instructions pose a diverse set of questions, addressing the general visual content of the video, human actions, and behaviors, the chronological sequence of events, and causal relationships. This approach encourages VLMs to delve deeper into the intricacies of video content. </p>
<aside>
👇 **TV Show Captions (TVC)**.

</aside>

<p>The primary purpose of incorporating TV show clips with high-level captions into the training process of VLMs is to enhance their social reasoning abilities and deepen their understanding of complex character dynamics. By organizing drama clips from <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2001.09099.pdf">TVR</a> to analyze character relationships and motivations, we aim to challenge VLMs to move beyond mere perception and demonstrate their reasoning capabilities within the context of TV show narratives. This focused approach is crucial for fostering advanced VLMs capable of effectively handling diverse real-world situations and user queries.</p>
<h3 id="2-3-2-Egocentric-View-Understanding"><a href="#2-3-2-Egocentric-View-Understanding" class="headerlink" title="2.3.2 - Egocentric View Understanding"></a>2.3.2 - Egocentric View Understanding</h3><aside>
👇 **Indoor Event Planning (IEP)**. Planning capabilities for diverse 2D room photos.

</aside>

<p><strong>2D Photos Sampling</strong>: We gather indoor scene RGB-D images from <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1702.04405.pdf">ScanNetv2</a> and sample them into multiple 2D visual inputs, representing a room’s layout from a first-person perspective.</p>
<p><strong>Instructions Generation</strong> via prompting ChatGPT: direct humans to perform various activities in indoor spaces.</p>
<ul>
<li>Initially, we have ChatGPT create a personality for the room owner.</li>
<li>Subsequently, the planning should be intimately related to the room’s layout and the generated room owner, underlining the importance of context awareness in VLMs.</li>
</ul>
<aside>
👇 [Ego4D (E4D, videos)](https://arxiv.org/pdf/2110.07058.pdf). For first-person augmented reality (AR) headset assistant applications.

</aside>

<p>By prompting ChatGPT to generate instructions based on visual descriptions, our goal is to simulate practical interactions between users and AR assistants.</p>
<p>An instance of assistant-related questions and  context-aware responses:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Instruction: What should I do now?</span><br><span class="line">Response: Based on my observation, you can now proceed to do....</span><br></pre></td></tr></table></figure>

<h2 id="2-4-Dataset-Statistics"><a href="#2-4-Dataset-Statistics" class="headerlink" title="2.4 - Dataset Statistics"></a>2.4 - Dataset Statistics</h2><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-6.png" alt="Comparison between MIMIC-IT and other multi-modal instruction datasets. MIMICIT features: (1) The largest. (2) Including video data. (3) In-context scenarios. (4) Multilingual. "></p>
<p><strong>2.8M instruction-response pairs</strong>(2.2M unique instructions): each pair includes at least one multi-modal in-context example and one language-only in-context example.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-7.png" alt="The data statistics of MIMIC-IT. (c) retains 25% of Ego4D instructions for a more balanced distribution."></p>
<p>(a) and (b) examine the characteristics and diversity using <a target="_blank" rel="noopener" href="https://github.com/explosion/spacy-models/releases/tag/en_core_web_md-3.5.0">spaCy</a> to get top 20 most frequent root verbs alongside their top 4 direct noun objects.</p>
<p>(c) demonstrates diversity in terms of instructions&#x2F;responses length, image num per instruction, and in-context examples num per instruction.</p>
<h1 id="3-Empricial-Evaluation"><a href="#3-Empricial-Evaluation" class="headerlink" title="3 - Empricial Evaluation"></a>3 - Empricial Evaluation</h1><h2 id="3-1-Otter-A-Multi-Modal-In-Context-Instruction-Tuned-Model"><a href="#3-1-Otter-A-Multi-Modal-In-Context-Instruction-Tuned-Model" class="headerlink" title="3.1 - Otter: A Multi-Modal In-Context Instruction Tuned Model"></a>3.1 - Otter: A Multi-Modal In-Context Instruction Tuned Model</h2><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Otter.png"></p>
<p><strong>Language Encoder</strong>: LLaMA-7B(frozen)</p>
<p><strong>Vision Encoder</strong>: CLIP ViT-L&#x2F;14(frozen)</p>
<p><strong>Perceiver Resampler Module</strong>: ~1.3B trainable parameters</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># [] indicates special word, &lt;&gt; indicates data slot</span><br><span class="line"># role label: User/GPT</span><br><span class="line"># format the training data as follows(chatbot-like format):</span><br><span class="line">&lt;context&gt; [image] User:&lt;instruction&gt; GPT:[answers] &lt;answer&gt;.[endofchunk]</span><br></pre></td></tr></table></figure>

<blockquote>
<p>To support user-assistant conversations, we adopt “GPT” as the role label because it does not have any specific semantic meaning in vocabulary.</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Hyperparameters</span><br><span class="line">optimizer: AdamW</span><br><span class="line">starting learning rate(lr): 10^&#123;-5&#125;</span><br><span class="line">batch size: 4</span><br><span class="line">epoch: 6</span><br><span class="line">lr scheduler: cosine annealing scheduler</span><br><span class="line">gradient clipping threshold: 1.0</span><br><span class="line">loss: cross-entropy</span><br></pre></td></tr></table></figure>

<aside>
👇 HuggingFace

</aside>

<ul>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/luodian/OTTER-Image-LLaMA7B-LA-InContext">OTTER-Image-LLaMA7B-LA-InContext</a>: as described in paper</li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/luodian/OTTER-Image-MPT7B">OTTER-Image-MPT7B</a>: Tune OpenFlamingv2 to enable generation abilities for both long and short answers.</li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/luodian/OTTER-Video-LLaMA7B-DenseCaption">OTTER-Video-LLaMA7B-DenseCaption</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/luodian/OTTER-Video-LLaMA7B-FunQA">OTTER-Video-LLaMA7B-FunQA</a></li>
</ul>
<blockquote>
<p><strong>Otter-Image</strong> supports multiple images input as in-context examples, which is the first multi-modal instruction tuned model that supports to organize inputs this way.</p>
</blockquote>
<blockquote>
<p><strong>Otter-Video</strong> supports videos inputs (frames are arranged as original Flamingo’s implementation) and multiple images inputs (they serve as in-context examples for each other).</p>
</blockquote>
<aside>
👇 Engineering Work

</aside>

<p>We have <a target="_blank" rel="noopener" href="https://huggingface.co/luodian/OTTER-9B-INIT">integrated Otter into Hugging Face Transformers</a> and trained it using the <a target="_blank" rel="noopener" href="https://huggingface.co/docs/accelerate/index">Hugging Face Accelerator</a>.</p>
<p>We use <strong>bf16 mixed precision</strong> and train Otter on <strong>4×RTX-3090 GPUs(24GB)</strong>.</p>
<blockquote>
<p>reduced from 1× A100 GPU</p>
</blockquote>
<p>We provide the support of Fully Sharded Data Parallel (FSDP) and DeepSpeed.</p>
<p>We provide a <strong>script for converting</strong> the original OpenFlamingo-9B checkpoint into the Hugging Face Model format.(<a target="_blank" rel="noopener" href="https://huggingface.co/luodian/openflamingo-9b-hf">luodian&#x2F;openflamingo-9b-hf</a>)</p>
<h2 id="3-2-Usage-Examples-and-Demonstrations"><a href="#3-2-Usage-Examples-and-Demonstrations" class="headerlink" title="3.2 - Usage Examples and Demonstrations"></a>3.2 - Usage Examples and Demonstrations</h2><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Otter-example.png" alt="**Otter’s response examples in different scenarios**. Otter is able to serve for situation understanding and reasoning, learning with in-context examples, and egocentric visual assistant."></p>
<p><strong>Otter’s response examples in different scenarios</strong>. Otter is able to serve for situation understanding and reasoning, learning with in-context examples, and egocentric visual assistant.</p>
<p><strong>Scene Understanding and Reasoning</strong>.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;image&gt;Human:&#123;instruction&#125; Assistant:&lt;answer&gt;&#123;model-generated response&#125;&lt;endofchunk&gt;</span><br></pre></td></tr></table></figure>

<p><strong>Learning with In-context Examples</strong>.</p>
<p>The organized input data format(on the LA-T2T task) is as follows:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># Multiple in-context example with similar instructions</span><br><span class="line">&lt;image&gt;Human:&#123;instruction&#125; Assistant:&lt;answer&gt;&#123;response&#125;&lt;|endofchunk|&gt;</span><br><span class="line"># ....</span><br><span class="line">&lt;image&gt;Human:&#123;instruction&#125; Assistant:&lt;answer&gt;&#123;response&#125;&lt;|endofchunk|&gt;</span><br><span class="line"># Query example</span><br><span class="line">&lt;image&gt;Human:&#123;instruction&#125; Assistant:&lt;answer&gt;</span><br></pre></td></tr></table></figure>

<p><strong>Egocentric Visual Assistant(Otter-E)</strong>.</p>
<p>Otter-E is specifically designed for AR headset applications.</p>
<p>In real-life scenarios, you are not encouraged to consult visual assistants for such hazardous actions.</p>
<h2 id="3-3-ChatGPT-Evaluation"><a href="#3-3-ChatGPT-Evaluation" class="headerlink" title="3.3 - ChatGPT Evaluation"></a>3.3 - ChatGPT Evaluation</h2><blockquote>
<p>Current evaluation metrics for VL models, like VQAv2 acc, exhibit shortcomings in terms of robustness.</p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-8.png" alt="**[MMAGIBench](https://github.com/open-mmlab/mmagibench) evaluation results** judged by ChatGPT."></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-9.png" alt="(a) ChatGPT evaluation. (b) Human evaluation comparison. (c) Few-shot in-context learning evaluation on [COCO caption (CIDEr)](https://arxiv.org/pdf/1405.0312.pdf)."></p>
<h2 id="3-4-Human-Evaluation"><a href="#3-4-Human-Evaluation" class="headerlink" title="3.4 - Human Evaluation"></a>3.4 - Human Evaluation</h2><p><a target="_blank" rel="noopener" href="https://github.com/OpenGVLab/Multi-Modality-Arena">Multi-Modality Arena</a> uses an Elo rating(higher is better) system to evaluate <strong>the usefulness and alignment</strong> of VLM responses.</p>
<p>The system calculates the relative skill levels of players.</p>
<p>This system works well for evaluating conversational AI models, because multiple models can have pairwise “battles” responding to the same inputs in a user-blind evaluation.</p>
<h2 id="3-5-Few-shot-In-Context-Learning-Metric-Evaluation"><a href="#3-5-Few-shot-In-Context-Learning-Metric-Evaluation" class="headerlink" title="3.5 - Few-shot In-Context Learning Metric Evaluation"></a>3.5 - Few-shot In-Context Learning Metric Evaluation</h2><p>As expected, the finetuning also brings marginal performance gain on zero-shot evaluation.</p>
<h1 id="4-Conclusion"><a href="#4-Conclusion" class="headerlink" title="4 - Conclusion"></a>4 - Conclusion</h1><aside>
👇 **Limitations**

</aside>

<p>For MIMIC-IT:</p>
<p>ChatGPT is prone to language hallucinations, therefore it might generate incorrect responses.</p>
<p>For Otter:</p>
<ul>
<li><strong>Object hallucinations</strong>(objects mentioned in text inputs do not appeare in images or videos)</li>
<li><strong>Language hallucinations</strong> from OpenFlamingo is inherited by Otter.</li>
</ul>
<aside>
👇 **Future Works**

</aside>

<p>For MIMIC-IT:</p>
<ul>
<li>More trustworthy LMs or generation techniques for self-instruct data generation.</li>
<li>More embodied AI datasets such as <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2210.06407.pdf">LanguageTable</a> and <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.01691.pdf">SayCan</a>.</li>
</ul>
<p>For Otter:</p>
<ul>
<li>introducing negative examples in the training data to reduce hallucination issue</li>
<li>more efficient training schemas(e.g., LoRA)</li>
<li>more modalities.</li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>Post author:  </strong>Xin Liu
  </li>
  <li class="post-copyright-link">
      <strong>Post link: </strong>
      <a href="https://ifshinelx.github.io/2023/07/04/MIMIC-IT/" title="MIMIC-IT: Multi-Modal In-Context Instruction Tuning">https://ifshinelx.github.io/2023/07/04/MIMIC-IT/</a>
  </li>
  <li class="post-copyright-license">
      <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>


        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/07/03/LENS/" rel="prev" title="(LENS)Towards Language Models That Can See: Computer Vision Through the LENS🔍 of Natural Language">
                  <i class="fa fa-chevron-left"></i> (LENS)Towards Language Models That Can See: Computer Vision Through the LENS🔍 of Natural Language
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/07/06/Lynx/" rel="next" title="(Lynx)What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?">
                  (Lynx)What Matters in Training a GPT4-Style Language Model with Multimodal Inputs? <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Xin Liu</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="far fa-file-word"></i>
    </span>
    <span title="Word count total">16k</span>
  </span>

</div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"ifshinelx","repo":"GitalkForBlog","client_id":"0cbb3af62c4f4096c907","client_secret":"3b535adf6818071ff1dc37a695e585cf044a2541","admin_user":"ifshinelx","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":"en","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"a69aea55fdebb7acd67a30d5eeef0ff3"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
