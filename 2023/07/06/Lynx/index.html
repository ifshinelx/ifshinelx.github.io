<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/little%20star.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/little%20star.jpg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"ifshinelx.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.17.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"algolia":{"appID":"QRVCI4B1JK","apiKey":"683b41d81744270ced12c4596c915a0e","indexName":"hexo_index","hits":{"per_page":10}}}</script><script src="/js/config.js"></script>



<link rel="canonical" href="https://ifshinelx.github.io/2023/07/06/Lynx/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://ifshinelx.github.io/2023/07/06/Lynx/","path":"2023/07/06/Lynx/","title":"(Lynx)What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>(Lynx)What Matters in Training a GPT4-Style Language Model with Multimodal Inputs? | XinLiu's Homepage, Welcome!</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">XinLiu's Homepage, Welcome!</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container">
  <div class="algolia-stats"><hr></div>
  <div class="algolia-hits"></div>
  <div class="algolia-pagination"></div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#lynx"><span class="nav-number">1.</span> <span class="nav-text">1 - Lynx</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#formulations"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 - Formulations</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#details-of-model-architecture"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 - Details of Model
Architecture</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#adapter"><span class="nav-number">1.2.1.</span> <span class="nav-text">1.2.1 - Adapter</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#visual-encoder"><span class="nav-number">1.2.2.</span> <span class="nav-text">1.2.2 - Visual Encoder</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pretraining"><span class="nav-number">1.3.</span> <span class="nav-text">1.3 - Pretraining</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#instruction-fintuning"><span class="nav-number">1.4.</span> <span class="nav-text">1.4 - Instruction Fintuning</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#experiment"><span class="nav-number">2.</span> <span class="nav-text">2 - Experiment</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#evaluation-protocols"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 - Evaluation Protocols</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#quantitative-experiments"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 - Quantitative Experiments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ablation-study"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 - Ablation Study</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#related-work"><span class="nav-number">3.</span> <span class="nav-text">3 - Related Work</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#conclusions"><span class="nav-number">4.</span> <span class="nav-text">4 - Conclusions</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#findings-and-takeaways"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 - Findings and Takeaways</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#limitations"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 - Limitations</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#future-work"><span class="nav-number">4.3.</span> <span class="nav-text">4.3 - Future Work</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Xin Liu"
      src="/images/little%20star.jpg">
  <p class="site-author-name" itemprop="name">Xin Liu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">11</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/ifshinelx" title="GitHub ‚Üí https:&#x2F;&#x2F;github.com&#x2F;ifshinelx" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:ifshine_lx@163.com" title="E-Mail ‚Üí mailto:ifshine_lx@163.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



  <div class="links-of-blogroll motion-element links-of-blogroll-block">
    <div class="links-of-blogroll-title">
      <!-- modify icon to fire by szw -->
      <i class="fa fa-history fa-" aria-hidden="true"></i>
      Recent Posts
    </div>
    <ul class="links-of-blogroll-list" style="padding: 0px 12px;">
      
      
      
        
          <li class="recent_posts_li">
            <a href="/2023/07/09/Shikra/" title="Shikra: Unleashing Multimodal LLM‚Äôs Referential Dialogue Magic" target="_blank">07-09 Shikra: Unleashing Multimodal LLM‚Äôs Referential Dialogue Magic </a>
          </li>
        
      
        
          <li class="recent_posts_li">
            <a href="/2023/07/06/Lynx/" title="(Lynx)What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?" target="_blank">07-06 (Lynx)What Matters in Training a GPT4-Style Language Model with Multimodal Inputs? </a>
          </li>
        
      
        
          <li class="recent_posts_li">
            <a href="/2023/07/04/MIMIC-IT/" title="MIMIC-IT: Multi-Modal In-Context Instruction Tuning" target="_blank">07-04 MIMIC-IT: Multi-Modal In-Context Instruction Tuning </a>
          </li>
        
      
        
          <li class="recent_posts_li">
            <a href="/2023/07/03/LENS/" title="(LENS)Towards Language Models That Can See: Computer Vision Through the LENSüîç of Natural Language" target="_blank">07-03 (LENS)Towards Language Models That Can See: Computer Vision Through the LENSüîç of Natural Language </a>
          </li>
        
      
        
          <li class="recent_posts_li">
            <a href="/2023/07/01/LongMem/" title="(LongMem)Augmenting Language Models with Long-Term Memory" target="_blank">07-01 (LongMem)Augmenting Language Models with Long-Term Memory </a>
          </li>
        
      
    </ul>
  </div>

  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://ifshinelx.github.io/2023/07/06/Lynx/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/little%20star.jpg">
      <meta itemprop="name" content="Xin Liu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="XinLiu's Homepage, Welcome!">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="(Lynx)What Matters in Training a GPT4-Style Language Model with Multimodal Inputs? | XinLiu's Homepage, Welcome!">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          (Lynx)What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-07-06 13:00:00" itemprop="dateCreated datePublished" datetime="2023-07-06T13:00:00+08:00">2023-07-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-07-09 09:26:56" itemprop="dateModified" datetime="2023-07-09T09:26:56+08:00">2023-07-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper/" itemprop="url" rel="index"><span itemprop="name">Paper</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper/2023/" itemprop="url" rel="index"><span itemprop="name">2023</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper/2023/07/" itemprop="url" rel="index"><span itemprop="name">07</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>1.5k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>5 mins.</span>
    </span>
</div>

        </div>
          <div class="post-tags" style="margin-top: 5px;">
              <a href="/tags/LLM/" rel="tag" style="border: 0px; border-radius: 10px; padding: 0px 10px;"><i class="fa fa-tag"></i> LLM</a>
              <a href="/tags/Multi-modal/" rel="tag" style="border: 0px; border-radius: 10px; padding: 0px 10px;"><i class="fa fa-tag"></i> Multi-modal</a>
              <a href="/tags/Prefix-Tuning/" rel="tag" style="border: 0px; border-radius: 10px; padding: 0px 10px;"><i class="fa fa-tag"></i> Prefix-Tuning</a>
              <a href="/tags/Adapter/" rel="tag" style="border: 0px; border-radius: 10px; padding: 0px 10px;"><i class="fa fa-tag"></i> Adapter</a>
              <a href="/tags/Dataset/" rel="tag" style="border: 0px; border-radius: 10px; padding: 0px 10px;"><i class="fa fa-tag"></i> Dataset</a>
              <a href="/tags/Instrcution-Following/" rel="tag" style="border: 0px; border-radius: 10px; padding: 0px 10px;"><i class="fa fa-tag"></i> Instrcution-Following</a>
              <a href="/tags/Evaluation/" rel="tag" style="border: 0px; border-radius: 10px; padding: 0px 10px;"><i class="fa fa-tag"></i> Evaluation</a>
          </div>
          <script type="text/javascript">
              var tagsall=document.getElementsByClassName("post-tags")
              for (var i = tagsall.length - 1; i >= 0; i--){
                  var tags=tagsall[i].getElementsByTagName("a");
                  for (var j = tags.length - 1; j >= 0; j--) {
                      var r=Math.floor(Math.random()*75+200);
                      var g=Math.floor(Math.random()*75+200);
                      var b=Math.floor(Math.random()*75+200);
                      tags[j].style.background = "rgb("+r+","+g+","+b+")";
                  }
              }                        
            </script>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-0.png" /></p>
<p>Paper: <a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/2307.02469.pdf">https://arxiv.org/pdf/2307.02469.pdf</a></p>
<p>Project Page: <a
target="_blank" rel="noopener" href="https://lynx-llm.github.io/">https://lynx-llm.github.io/</a></p>
<aside>
<p>üëá Problems</p>
</aside>
<p>The performance of multimodal LLMs heavily relies on network
structures, training data, instruction diversity, and evaluation
benchmarks, which have not been extensively discussed.</p>
<aside>
<p>üëá Contributions</p>
</aside>
<ol type="1">
<li>Present a systematic and comprehensive study, quantitatively and
qualitatively.
<ul>
<li><strong>Network Structures</strong>
<ul>
<li>LLaMA vs Vicuna</li>
<li>Prefix-tuning vs Cross-attention</li>
</ul></li>
<li><strong>Training Data</strong>(combinations and sampling strategies)
<ul>
<li>Quality vs Quantity</li>
<li>COYO700M, DataComp1B, BlipCapFilt</li>
</ul></li>
<li><strong>Instruction Diversity</strong>
<ul>
<li>500 instructions for over 50 tasks</li>
</ul></li>
<li><strong>Evaluation</strong>. Collect the first comprehensive
evaluation set including both image and video tasks through
crowd-sourcing.</li>
</ul></li>
<li>Present Lynx model.</li>
</ol>
<h1 id="lynx">1 - Lynx</h1>
<figure>
<img src="https://lynx-llm.github.io/static/images/lynx.png"
alt="Our model is based on prefix-tuning architecture. In contrast to the cross-attention-based models like Flamingo." />
<figcaption aria-hidden="true">Our model is based on prefix-tuning
architecture. In contrast to the cross-attention-based models like
Flamingo.</figcaption>
</figure>
<p>Our model is based on prefix-tuning architecture. In contrast to the
cross-attention-based models like Flamingo.</p>
<h2 id="formulations">1.1 - Formulations</h2>
<p><strong>Prefix-Tuning</strong>: Visual tokens <span
class="math inline">\(\mathbf{w}_v=\{w_i\}_{i=1}^V\)</span> are directly
concatenated with instruction tokens <span
class="math inline">\(\mathbf{w}_l=\{w_j\}_{j=V+1}^{V+L}\)</span>.</p>
<p>Sentence prediction equation: <span
class="math inline">\(p(w_{V+L+1:V+L+T}|w_{1:V+L})\sim\prod\limits_{t=V+L+1}^{V+L+T}P(w_t|w_{&lt;t})\)</span>,
ended by <EOS>.</p>
<h2 id="details-of-model-architecture">1.2 - Details of Model
Architecture</h2>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-1.png"
alt="Lynx: train trainable adapters based on frozen Vicuna. Token dim in hidden state is 4096, while 2048 in adapter." />
<figcaption aria-hidden="true">Lynx: train trainable adapters based on
frozen Vicuna. Token dim in hidden state is 4096, while 2048 in
adapter.</figcaption>
</figure>
<h3 id="adapter">1.2.1 - Adapter</h3>
<p>The trainable adapters are inserted into the LLMs after every <span
class="math inline">\(M\)</span> blocks.(<span
class="math inline">\(M=1\)</span>)</p>
<h3 id="visual-encoder">1.2.2 - Visual Encoder</h3>
<p>Apply <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2211.07636.pdf">EVA-1B</a> as
<strong>Visual Encoder</strong> <span
class="math inline">\(\phi_v(x)\)</span> to map an image <span
class="math inline">\(x\)</span> with resolution <span
class="math inline">\(H\times W\)</span> to a sequence of <span
class="math inline">\(\frac{H}{14}\times \frac{W}{14}\)</span> visual
tokens.</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.03206.pdf">Resampler</a> <span
class="math inline">\(\Phi\)</span> mechanism reduces the dimensions of
vision inputs.</p>
<p>By injecting the long vision token sequence into a short and
<strong>learnable query sequence</strong> <span
class="math inline">\(\mathbf{w}_v^q\)</span>, we adapt the <span
class="math inline">\(\Phi\)</span> to improve the efficiency of
training and inference: <span
class="math inline">\(\mathbf{w}_v=\Phi(\phi_v(x),\mathbf{w}_v^q)\)</span>.</p>
<p><span class="math inline">\(\mathbf{w}_v\)</span> is the
<strong>condensed sequence</strong> of 32 tokens.</p>
<h2 id="pretraining">1.3 - Pretraining</h2>
<p>Utilize <span class="math inline">\(&gt;120M\)</span> image-text
pairs to build inter-modality connections.</p>
<p>Next-word prediction training with the cross entropy loss.(same in
finetuing)</p>
<blockquote>
<p>Compared to the contrastive pretraining, pretraining with next-word
prediction requires data with fluent texts that can represent the
‚Äúnatural‚Äù causal dependency between the predicted word and the past
context very well.</p>
</blockquote>
<p><span class="math inline">\(0\sim100k\)</span> steps for <span
class="math inline">\(224\times224\)</span> images, <span
class="math inline">\(100k\sim110k\)</span> steps for <span
class="math inline">\(420\times420\)</span> images.</p>
<p>After 110k steps, we freeze the visual encoder and <strong>thus the
expense of increasing image resolution is affordable</strong>.</p>
<h2 id="instruction-fintuning">1.4 - Instruction Fintuning</h2>
<p>We collect an instruction finetuning multi-modal dataset based on the
public ones.</p>
<p>Different <strong>weight combinations</strong> of instruction data
have a crucial influence on the final performance.</p>
<aside>
<p>üëâ (1) 50+ text-only, image-text, video-text tasks belonging to
<strong>5 Categories</strong>:</p>
</aside>
<ol type="1">
<li>Text-only Instruction-Following</li>
<li>Image/Video Visual Question Answering</li>
<li>Image/Video Captioning</li>
<li>Classification</li>
<li>Image-conditioned Dialog for Complex Reasoning and Instruction
Following</li>
</ol>
<p><strong>Provide corresponding instructions for each
task:</strong></p>
<p>Manually label ‚â•3 instructions and prompt GPT4 to automatically
generate more.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Here are some instructions that define a visual-language task. Continue to write 15 instructions with the same meaning: 1) PROMPT1; 2) PROMPT2; 3) PROMPT3;</span><br></pre></td></tr></table></figure>
<aside>
<p>üëâ (2) available public instruction data: FlanT5, Alpaca, Mini-GPT4,
LLaVA, Baize</p>
</aside>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-2.png" /></p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-3.png" /></p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-4.png"
alt="Training Data. \sim14B tokens for the pretraining, \sim3B for the instruction-finetuning. Ratio indicates weight strategy." />
<figcaption aria-hidden="true"><strong>Training Data.</strong> <span
class="math inline">\(\sim14B\)</span> tokens for the pretraining, <span
class="math inline">\(\sim3B\)</span> for the instruction-finetuning.
Ratio indicates weight strategy.</figcaption>
</figure>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-5.png"
alt="Training hyperparameters. Some parameters not use learning rate decay schedule. Use the DeepSpeed to accelerate training, and set the BFloat16 as the default model precision." />
<figcaption aria-hidden="true"><strong>Training
hyperparameters</strong>. Some parameters not use learning rate decay
schedule. Use the DeepSpeed to accelerate training, and set the BFloat16
as the default model precision.</figcaption>
</figure>
<h1 id="experiment">2 - Experiment</h1>
<p><strong>Description-first strategy</strong>: Before sending the
request from the user, we feed a fixed prompt ‚ÄúDescribe the image in
detail‚Äù first in the ‚Äú0th‚Äù round of the conversation. After that, the
user‚Äôs instructions will be sequentially processed.</p>
<p>During the deployment, this strategy improves performance of most
models(but not for MiniGPT4).</p>
<p>For MiniGPT4, we generated the response with its default
settings.</p>
<p>For mPLUG-owl, we follow the default parameters presented at <a
target="_blank" rel="noopener" href="http://vlarena.opengvlab.com/">http://vlarena.opengvlab.com/</a>.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-6.png"
alt="Hyper-parameters for Generation during the deployment. We set hyper-parameters to encourage short response generation for Open-VQA." />
<figcaption aria-hidden="true">Hyper-parameters for Generation during
the deployment. We set hyper-parameters to encourage short response
generation for Open-VQA.</figcaption>
</figure>
<h2 id="evaluation-protocols">2.1 - Evaluation Protocols</h2>
<aside>
<p>üëâ (1) Open-VQA(ours); (2) <a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/2304.14178.pdf">OwlEval</a> from mPLUG-Owl;
(3) <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2306.13394.pdf">MME</a></p>
</aside>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-7.png"
alt="Manually collected Open-VQA supports open-ended answers, which contains diverse questions on objects, OCR, counting, reasoning, action recognition, chronological ordering, etc." />
<figcaption aria-hidden="true">Manually collected
<strong>Open-VQA</strong> supports <strong>open-ended answers</strong>,
which contains diverse questions on objects, OCR, counting, reasoning,
action recognition, chronological ordering, etc.</figcaption>
</figure>
<p><strong>Open-VQA</strong> consists of 450 samples, based on VQA 2.0,
OCRVQA, Place365, MSVD, MSRVTT, SthV2.</p>
<p>Though Place365 is a classification task and SthV2 is a video
captioning task, we write proper prompts to make them both VQA
tasks.</p>
<p><strong>Use GPT4 as judger via following prompt:</strong></p>
<blockquote>
<p>GPT4 achieves a consistency of more than 95% compared with humans.
(We evaluate the consistency on 100 samples from a randomly selected
subset with our model.)</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Given the question ‚ÄúQUESTION‚Äù, does the answer ‚ÄúPREDICTION‚Äù imply the answer ‚ÄúGROUND_TRUTH‚Äù? Answer with Yes or No.</span><br></pre></td></tr></table></figure>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-8.png"
alt="To make the comparison fair, we pad each image in the OwlEval with 8 pixels." />
<figcaption aria-hidden="true">To make the comparison fair, we pad each
image in the <strong>OwlEval</strong> with 8 pixels.</figcaption>
</figure>
<p>Recruit <strong>humans</strong> to evaluate the quality(first
<strong>correctness</strong>, then <strong>richness</strong>) of
language generation.</p>
<p><strong>Scores</strong> range from 1 to 5 with <strong>2
rules</strong>:</p>
<ol type="1">
<li>At most 2 models that gain equal scores</li>
<li>for each annotator, total tie num ‚â§ 10 for the whole set</li>
</ol>
<h2 id="quantitative-experiments">2.2 - Quantitative Experiments</h2>
<aside>
<p>üëâ The key to <strong>balance language generation and
correctness</strong> is a high-quality VL dataset that (1) includes high
quality and fluent texts (2) aligns the texts and images well</p>
</aside>
<ul>
<li>If a model has lower accuracy on Open-VQA, it tends to make factual
errors inconsistent with the given image during text generation.
(under-training on VL tasks)</li>
<li>Models with higher performance on Open-VQA usually tend to lose
language generation ability, e.g., generate short sentences.
(over-training on VL tasks)</li>
</ul>
<figure>
<img src="https://lynx-llm.github.io/static/images/result_1.png"
alt="Compare existing multi-modal LLMs on the Open-VQA image benchmark. InstructBLIP and Lynx achieve high performance." />
<figcaption aria-hidden="true">Compare existing multi-modal LLMs on the
Open-VQA image benchmark. InstructBLIP and Lynx achieve high
performance.</figcaption>
</figure>
<p>Compare existing multi-modal LLMs on the Open-VQA image benchmark.
InstructBLIP and Lynx achieve high performance.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-9.png"
alt="Different from InstructBLIP, Lynx is more user-friendly." />
<figcaption aria-hidden="true">Different from InstructBLIP, Lynx is more
user-friendly.</figcaption>
</figure>
<figure>
<img src="https://lynx-llm.github.io/static/images/result_all.png"
alt="(b) MME: All scores are normalized to the range from 0 to 100. (d) Ablation study on our Open-VQA videos." />
<figcaption aria-hidden="true">(b) MME: All scores are normalized to the
range from 0 to 100. (d) Ablation study on our Open-VQA
videos.</figcaption>
</figure>
<ol start="2" type="a">
<li>MME: All scores are normalized to the range from 0 to 100. (d)
Ablation study on our Open-VQA videos.</li>
</ol>
<p>Lynx is good at MME perception tasks including Color, Celebrity,
Scene, Landmark, Position, Count, and Existence.</p>
<h2 id="ablation-study">2.3 - Ablation Study</h2>
<aside>
<p>üëâ What matters to train a high-performance GPT4-style model?</p>
</aside>
<blockquote>
<p>A GPT4-style LLM is defined as a decoder-only transformer that takes
both visual and instructional tokens as inputs and generates responses
in text auto-regressively.</p>
</blockquote>
<ul>
<li><p><strong>LLaMA</strong>(language generation) <strong>vs.
Vicuna</strong>(correctness, instruction-following ability)</p></li>
<li><p><strong>Impact of Diversified Instructions</strong>(during
training)</p></li>
<li><p><strong>Impact of Training Data</strong>(noise: COYO700M,
DataComp1B)</p>
<blockquote>
<p>attribute the worse results to the difference between generative
pretraining and contrastive pretraining.</p>
</blockquote></li>
<li><p><strong>Prefix-Tuning vs. Cross-Attn</strong> (follow
Open-Flamingo)</p>
<blockquote>
<p>We only use multi-modal instruction data for pre-training. For the
finetuning stage, we experiment with two variants, with or without
trainable LLM, i.e., with or without the use of text instruction
data.</p>
</blockquote></li>
<li><p><strong>Impact of Larger Image Resolution</strong></p></li>
</ul>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-10.png"
alt="Ablation study on our Open-VQA images." />
<figcaption aria-hidden="true">Ablation study on our Open-VQA
images.</figcaption>
</figure>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-11.png"
alt="Human evaluation of different ablation models. (a) w/ LLaMA vs w/ Vicuda; (b) w/o diversified instructions vs w/ diversified prompts; (c) w/ large-scale noisy data vs w/o large-scale noisy data; (d) prefix-finetuning vs cross-attention." />
<figcaption aria-hidden="true">Human evaluation of different ablation
models. (a) w/ LLaMA vs w/ Vicuda; (b) w/o diversified instructions vs
w/ diversified prompts; (c) w/ large-scale noisy data vs w/o large-scale
noisy data; (d) prefix-finetuning vs cross-attention.</figcaption>
</figure>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-12.png" /></p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-13.png"
alt="w/ diversified prompts versus w/o diversified prompts." />
<figcaption aria-hidden="true">w/ diversified prompts versus w/o
diversified prompts.</figcaption>
</figure>
<h1 id="related-work">3 - Related Work</h1>
<p><strong>(1) Centralized Multi-modal Interactive System</strong></p>
<ul>
<li>„Äêexisting work„ÄëVisual ChatGPT, MM-REACT, HuggingGPT, InternGPT,
SayCan, InnerMonologue</li>
<li>„Äêadavantage„Äëaddress problems that are well-defined</li>
<li>„Äêlimit„Äëlack zero-shot ability to handle open-ended
instructions</li>
</ul>
<p><strong>(2) End-to-end Multi-modal LLMs</strong></p>
<blockquote>
<p>adding some additional trainable parameters</p>
<ul>
<li>„Äêtype 1„ÄëCross-Attention(Flamingo)</li>
<li>„Äêtype 2„Äëdirectly concatenate visual and textual tokens</li>
<li>BLIP2: Q-Former</li>
<li>PaLM-E: no fixed layers</li>
<li>Mini-GPT4: projection layer</li>
<li>LLaVA: tunes LLM during the instruction finetuning stage</li>
<li>mPLUG-Owl: first stage tunes vision encoder, second stage tunes
LLM</li>
<li>Kosmos-1: train a LLM from scratch</li>
</ul>
</blockquote>
<h1 id="conclusions">4 - Conclusions</h1>
<h2 id="findings-and-takeaways">4.1 - Findings and Takeaways</h2>
<ul>
<li>Prefix-tuning may be the currently best way to multi-modal
adaptation for LLMs.</li>
<li>Multi-modal LLMs are not as instruction-following as LLMs.</li>
<li>The quality of training data is critical to model performance.</li>
<li>Diverse tasks and prompts are crucial for zero-shot abilities.</li>
<li>Balancing the correctness and language generation ability is
important.</li>
</ul>
<h2 id="limitations">4.2 - Limitations</h2>
<ul>
<li><strong>Training Data</strong>.
<ul>
<li>Struggle to <strong>balance different
abilities</strong>(correctness&amp;language generation, long&amp;short
answer)</li>
<li>no available image-text datasets that contain <strong>long
texts</strong> which are ideal for pretraining</li>
<li>do not find the <strong>optimal data combination</strong> strategy
restricted by computational resource</li>
</ul></li>
<li><strong>Safety</strong>. We do not conduct safety checks and
restrictions(e.g., ethical, political, and racism issues).</li>
</ul>
<h2 id="future-work">4.3 - Future Work</h2>
<ul>
<li>Scale up model size</li>
<li>a high-quality VL dataset for training
<ul>
<li>Larger and more diverse instructional tasks</li>
<li>Find optimal data combination</li>
</ul></li>
<li>More comprehensive evaluation</li>
<li>Multi-Lingual</li>
<li>Safety</li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>Post author:  </strong>Xin Liu
  </li>
  <li class="post-copyright-link">
      <strong>Post link: </strong>
      <a href="https://ifshinelx.github.io/2023/07/06/Lynx/" title="(Lynx)What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?">https://ifshinelx.github.io/2023/07/06/Lynx/</a>
  </li>
  <li class="post-copyright-license">
      <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>


        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/07/04/MIMIC-IT/" rel="prev" title="MIMIC-IT: Multi-Modal In-Context Instruction Tuning">
                  <i class="fa fa-chevron-left"></i> MIMIC-IT: Multi-Modal In-Context Instruction Tuning
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/07/09/Shikra/" rel="next" title="Shikra: Unleashing Multimodal LLM‚Äôs Referential Dialogue Magic">
                  Shikra: Unleashing Multimodal LLM‚Äôs Referential Dialogue Magic <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Xin Liu</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="far fa-file-word"></i>
    </span>
    <span title="Word count total">16k</span>
  </span>

</div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/algoliasearch/4.17.1/algoliasearch-lite.umd.js" integrity="sha256-F7emIId74fYoGrHzsnu3iClRHIbBMhMCbxDoA1cfMAY=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/instantsearch.js/4.56.1/instantsearch.production.min.js" integrity="sha256-lz9C+x8+6w2rh56x5TrH5iYmE4Js2FiJS5h0tuMz7hQ=" crossorigin="anonymous"></script><script src="/js/third-party/search/algolia-search.js"></script>







  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"ifshinelx","repo":"GitalkForBlog","client_id":"0cbb3af62c4f4096c907","client_secret":"3b535adf6818071ff1dc37a695e585cf044a2541","admin_user":"ifshinelx","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":"en","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"c553f40341cd39a803825fe80f05d074"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
