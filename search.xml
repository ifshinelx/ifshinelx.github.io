<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>[Waiting...]2022 Machine Learning Specialization</title>
    <url>/2023/06/15/2022-machine-learning-specialization/</url>
    <content><![CDATA[<blockquote>
<p><em>Definition of Machine Learning(informal)</em>: Field of study
that gives computers the ability to learn without being explicitly
programmed. [1959, Arthur Samuel]</p>
</blockquote>
<ul>
<li>Main Course Content
<ul>
<li><strong>Supervised Learning</strong></li>
<li>Unsupervised Learning</li>
</ul></li>
<li>Others
<ul>
<li>Reinforcement Learning</li>
<li>Practical advice for applying learning algorithms</li>
</ul></li>
</ul>
<h1
id="supervised-machine-learning-regression-and-classification">Supervised
Machine Learning: Regression and Classification</h1>
<h2 id="linear-regression">Linear Regression</h2>
<h2 id="logistic-regression">Logistic Regression</h2>
<h2 id="gradient-descent">Gradient Descent</h2>
<!-- ## Machine Learning Overview
## Linear Regression with One Variable
## Training Linear Regression
## Linear Regression with Multiple Variables
## Practical Tips for Linear Regression
## Classification
## Cost Function
## Gradient Descent
## Regularization to Reduce Overfitting -->
<h1 id="advanced-learning-algorithm">Advanced Learning Algorithm</h1>
<h2 id="neural-networks">Neural Networks</h2>
<h2 id="decision-trees">Decision Trees</h2>
<h2 id="advice-for-ml">Advice for ML</h2>
<!-- ## Neural Networks Intuition
## Neural Network Model
## TensorFlow Implementation
## Neural Network Implementation in Python
## Speculations on Artificial General Intelligence(AGI)
## Vectorization(optional)
## Neural Network Training
## Activation Functions
## Multiclass Classification
## Additional Neural Network Concepts
## Advice for Applying Machine Learning
## Bias and Variance
## Machine Learning Development Process
## Skewed datasets(optional)
## Decision Trees
## Decision Tree Learning
## Tree Ensembles -->
<h1
id="unsupervised-learning-recommender-systems-and-reinforcement-learning">Unsupervised
Learning: Recommender Systems and Reinforcement Learning</h1>
<h2 id="clustering">Clustering</h2>
<h2 id="anomaly-detection">Anomaly Detection</h2>
<h2 id="collaborative-filtering">Collaborative Filtering</h2>
<h2 id="content-based-filtering">Content-based Filtering</h2>
<h2 id="reinforcement-learning">Reinforcement Learning</h2>
<!-- ## Clustering
## Anomaly Detection
## Recommender System
## Recommender Systems Implementation
## Content-based Filtering
## Reinforcement Learning
## State-action Value Function
## Continuous State Spaces -->
]]></content>
      <categories>
        <category>Online Course</category>
        <category>Andrew Ng</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>(LongMem)Augmenting Language Models with Long-Term Memory</title>
    <url>/2023/07/01/LongMem/</url>
    <content><![CDATA[<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/longmem-0.png" /></p>
<p>Paper: <a
href="https://arxiv.org/pdf/2306.07174.pdf">https://arxiv.org/pdf/2306.07174.pdf</a></p>
<p>Code: <a
href="https://github.com/Victorwz/LongMem">https://github.com/Victorwz/LongMem</a></p>
<p>Bilibili: <a
href="https://www.bilibili.com/video/BV17M4y177WG/?share_source=copy_web">Augmenting
Language Models with Long-Term Memory ÔºàUCSBÔºå Microsoft 2023Ôºâ</a></p>
<aside>
<p>üëá <strong>Problems</strong></p>
</aside>
<ul>
<li>LLMs can <strong>only afford fix-sized inputs</strong> due to the
input length limit, preventing them from utilizing rich long-context
information from past inputs.</li>
</ul>
<p><strong><em>Two existing ways addressing the length limit
issue:</em></strong></p>
<ul>
<li>Directly increasing the input length of LLM will cause <strong>huge
computation complexity</strong>.</li>
<li>Developing sparse attention reduces the computation cost, but still
requires <strong>training from scratch</strong>.</li>
</ul>
<aside>
<p>üëá <strong>Contributions</strong></p>
</aside>
<ul>
<li><strong>LongMem</strong> enables LLMs to memorize long history,
which benefits various downstream tasks.</li>
</ul>
<blockquote>
<p>The idea behind this is very similar to LoRA's, creating a small
network next to the backbone.</p>
</blockquote>
<p><strong><em>Two Benefits of LongMem:</em></strong></p>
<ul>
<li>Decouples the process of encoding memory and the process of memory
retrieval and fusion, via decoupling LLM and SideNet, which effectively
<strong>resolves the issue of memory staleness</strong>.</li>
<li>The SideNet with frozen LLM is <strong>parameter-efficient</strong>
can avoid catastrophic forgetting.</li>
</ul>
<blockquote>
<p><strong>Remaining Questions</strong>: What is the ablation study of
<span class="math inline">\(m\)</span> and <span
class="math inline">\(m_s\)</span>? What are limitation of LongMem and
its future work?</p>
</blockquote>
<hr />
<h2 id="methods">Methods</h2>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/longmem-1.png"
alt="Overview of the memory caching and retrieval flow of LongMem." />
<figcaption aria-hidden="true">Overview of the memory caching and
retrieval flow of LongMem.</figcaption>
</figure>
<p>Three key components of LongMem: <strong>frozen LLM, Cache Memory
Bank(CMB), Residual SideNet(RS).</strong></p>
<p><strong>CMB</strong> is a cached head-wise vector queue that stores
the memory.</p>
<p><strong>RS</strong> is efficiently trained to fuse the memory context
information.</p>
<h3 id="encoding-and-storing-memory">Encoding and Storing Memory</h3>
<p>Most LLMs process a fixed-sized input. Therefore, split a long
sequence into fix-length segments.</p>
<blockquote>
<p>Each segment has several sequences.</p>
</blockquote>
<ul>
<li><strong>Previous segments</strong>: key-value pairs of
self-attention at m-th layer(<span
class="math inline">\(\mathcal{Z}_k,\mathcal{Z}_v\in\mathbb{R}^{H\times
M\times d}\)</span>) are stored in <strong>CMB</strong>
<ul>
<li><span class="math inline">\(H\)</span> is the attention heads num,
<span class="math inline">\(d\)</span> is per-head dimension, <span
class="math inline">\(M\)</span> indicates the capacity of
<strong>CMB</strong></li>
</ul></li>
<li><strong>Current segment <span
class="math inline">\(\{x_i\}_{i=1}^{|x|}:\)</span></strong> hidden
states(of each layer) are retained and transfered to <strong>RS</strong>
<ul>
<li><span
class="math inline">\(\mathbf{H}_{LLM}^0\in\mathbb{R}^{|x|\times
E}\)</span> is the initial hidden states of <span
class="math inline">\(\{x_i\}_{i=1}^{|x|}\)</span>, <span
class="math inline">\(E\)</span> is the hidden dimension</li>
<li><span
class="math inline">\(\mathbf{H}_{LLM}^{l&#39;}=f_{\theta_{LLM}^{l&#39;}}(\mathbf{H}_{LLM}^{l&#39;-1}),\forall
l&#39;\in[1,L&#39;]\)</span> is the successive hidden states, <span
class="math inline">\(L&#39;\)</span> is the total layer num of LLM</li>
</ul></li>
<li><strong>Current token</strong>: top-K relevant key-value pairs are
retrieved and fused into language modeling</li>
</ul>
<p><strong>CMB Update Mechanism</strong>: after memory retrieval and
fusion, the oldest sequences are removed, the current sequences are
appended.</p>
<h3 id="memory-retrieval">Memory Retrieval</h3>
<p><strong>Text-chunk</strong>: an n-gram structure of
<strong>chunk-size</strong> <span class="math inline">\(csz\)</span>
number of contiguous tokens.</p>
<p>The hyperparameter <span class="math inline">\(csz\)</span> controls
<strong>the granularity of retrieved contexts</strong>, which can be
empirically adjusted based on downstream tasks.</p>
<blockquote>
<p>For instance, In-Context Learning requires more fine-grained label
tokens from demostration examples cached in memory, where a smaller
<span class="math inline">\(csz\)</span> is helpful.</p>
</blockquote>
<p><strong>Advantages of Token-to-Chunk Retrieval</strong>(compared with
token-to-token):</p>
<ul>
<li>reduce the size of the retrieval index and
<strong>accelerate</strong> the process</li>
<li>further improve the retrieval <strong>accuracy</strong> (not always,
when <span class="math inline">\(csz\)</span> is too large)</li>
</ul>
<p><strong>Three steps of retrieval:</strong></p>
<ol type="1">
<li>The <strong>CMB</strong> has <span
class="math inline">\(M/csz\)</span> chunks. We use the
<strong>mean-pooled vector</strong> on the chunk-size dimension to get
the key vector for retrieval.</li>
<li>Then we retrieve the top-(<span
class="math inline">\(K/csz\)</span>) chunks w.r.t(with respect to) the
dot product between the <strong>attention query</strong> of current
token <span class="math inline">\(\mathbf{x}_i\)</span> and the
<strong>mean-pooled attention key</strong> of a candidate chunk.</li>
<li>Finally, by squeezing and flatterning, we get <span
class="math inline">\(K\)</span> key-valus pairs <span
class="math inline">\(\{\widetilde{\mathbf{K}}_j,\widetilde{\mathbf{V}}_j\}_{j=1}^K\)</span>
at token-level.</li>
</ol>
<h3 id="memory-fusion">Memory Fusion</h3>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/longmem-2.png"
alt="Overview of LongMem architecture. ‚ÄúMemAug‚Äù represents Memory-Augmented Layer for memory retrieval and fusion." />
<figcaption aria-hidden="true">Overview of LongMem architecture.
‚Äú<strong>MemAug</strong>‚Äù represents <strong>Memory-Augmented
Layer</strong> for memory retrieval and fusion.</figcaption>
</figure>
<aside>
<p>üëá <strong>Residual</strong> <strong>SideNet</strong> Architecture
and Initialization</p>
</aside>
<p>SideNet consists of (L-1) normal Transformer decoder layers and one
special <strong>MemAug</strong>.</p>
<p><span class="math inline">\(L\)</span> is the total layer num of
<strong>SideNet</strong>, <span class="math inline">\(L&#39;=\alpha L,
\alpha\in\{2,3,4,...\}\)</span>, <span
class="math inline">\(L&lt;L&#39;\)</span> for efficiency.</p>
<p><strong>Weight Initialization</strong>: <span
class="math inline">\(\mathcal{\Theta}_{Side}^{\frac{l&#39;}{\alpha}}=\mathcal{\Theta}_{LLM}^{l&#39;}\)</span></p>
<p><strong>Cross-Network Residual Connections</strong>:</p>
<p><span
class="math inline">\(\mathbf{H}_{Side}^l=f_{\mathcal{\Theta}_{Side}^l}(\mathbf{H}_{Side}^{l-1})+(\mathbf{H}_{LLM}^{\alpha
l}-\mathbf{H}_{LLM}^{\alpha(l-1)}),\forall l\in[1,L]\)</span></p>
<blockquote>
<p>The residual connections after the SA and FFN of a decoder layer will
be performed as normal in <span
class="math inline">\(f_{\mathcal{\Theta}_{Side}^l}(\mathbf{H}_{Side}^{l-1})\)</span>
and parallel to the proposed cross-network residual connections. SA is
self-attention. FFN is feed-forward network.</p>
</blockquote>
<aside>
<p>üëá <strong>MemAug:</strong> Each token attends on both local contexts
and retrieved memory contexts.</p>
</aside>
<p><span class="math inline">\(\mathbf{Q}, \mathbf{K}, \mathbf{V},
\mathbf{A},\mathbf{M}\in\mathbb{R}^{|x|\times d}\)</span>, <span
class="math inline">\(g\)</span> is a <strong>trainable</strong>
<strong>head-wise</strong> gating factor, <span
class="math inline">\(m_s\)</span> is the layer index of
<strong>MemAug.</strong></p>
<blockquote>
<p>The initialization of gating factor g:
Parameter(torch.zeros(self.num_heads))</p>
</blockquote>
<p>The hidden state output from previous layer (<span
class="math inline">\(\mathbf{H}^{(m_s-1)}_{Side}\in\mathbb{R}^{|x|\times
d}\)</span>) is linearly projected into <span
class="math inline">\(\mathbf{Q}, \mathbf{K}, \mathbf{V}\)</span> via
three matrices <span
class="math inline">\(W^Q,W^K,W^V\in\mathbb{R}^{d\times d}\)</span>.</p>
<p><span
class="math inline">\(\{\widetilde{\mathbf{K}}_i,\widetilde{\mathbf{V}}_i\}_{i=1}^{|x|}\in\mathbb{R}^{|x|\times
K\times d}\)</span> is the retrieved contexts. Each token has distinct
contexts.</p>
<p><span
class="math inline">\(\mathbf{A}=softmax(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d}})\mathbf{V},\mathbf{M}=Concat\{softmax(\frac{\mathbf{Q}_i\widetilde{\mathbf{K}}_i^T}{\sqrt{d}})\widetilde{\mathbf{V}}_i\}_{i=1}^{|x|}\)</span></p>
<p><span
class="math inline">\(\mathbf{H}^{m_s}_{Side}=sigmoid(g)¬∑\mathbf{A}+(1-sigmoid(g))¬∑\mathbf{M}:\)</span>
the output of <strong>MemAug</strong></p>
<h3 id="training-objective">Training Objective</h3>
<ul>
<li><span
class="math inline">\(P(\mathbf{x}_i|\mathbf{x}_1,...,\mathbf{x}_{i-1})=softmax(W\mathbf{H}^{L}_{Side}):\)</span>
the token probability is computed using the last SideNet hidden states
<ul>
<li><span class="math inline">\(W:\)</span> the frozen output embedding
weight shared by LLM and SideNet</li>
</ul></li>
<li><span
class="math inline">\(\max\sum_{x\in\mathcal{D}}\sum_{i=1}^{|x|}\log
P(\mathbf{x}_i|\mathbf{x}_1, ..., \mathbf{x}_{i-1}):\)</span> the
training objective of <strong>LongMem</strong>
<ul>
<li><span class="math inline">\(x\)</span> is a randomly sampled
sentence from the pre-training text corpus <span
class="math inline">\(\mathcal{D}\)</span></li>
</ul></li>
</ul>
<h2 id="experiments">Experiments</h2>
<h3 id="training-setup">Training Setup</h3>
<aside>
<p>üëá Batchfying the training corpora (document-level shuffling within
each group)</p>
</aside>
<blockquote>
<p>The conventional process truncates the corpora without padding and
conduct a segment-level shuffling.</p>
</blockquote>
<p>Doc Group Num = Batch Size</p>
<p>Segment num is same for each group.</p>
<blockquote>
<p>Maybe need padding. Seg num is unlimited.</p>
</blockquote>
<p>In a mini-batch, we select a segment from each group and get total
batch-size num of segments.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/longmem-3.png"
alt="Batchfying the large text corpora into batches to ensure that each consecutive segments within each document is distributed in consecutive batches." />
<figcaption aria-hidden="true"><strong>Batchfying the large text
corpora</strong> into batches to ensure that each consecutive segments
within each document is distributed in consecutive batches.</figcaption>
</figure>
<aside>
<p>üëá Training Corpus and Hyperparameters</p>
</aside>
<p><strong>Corpus</strong>: sample a subset of the <a
href="https://arxiv.org/pdf/2101.00027.pdf">Pile</a>.</p>
<p>The pre-training of reproduced GPT-2* iterates on 117B tokens in
total, with 512 batch-size and 1024-token fixed segment-length.</p>
<blockquote>
<p>Original GPT-2 adopts absolute position embedding, which is found to
perform poorly to enable LLM to learn long-distance dependencies.</p>
</blockquote>
<p><strong>MemAug</strong> is the 9-th layer of SideNet.</p>
<p><span
class="math inline">\(L&#39;=24,L=12,\alpha=\frac{L&#39;}{L}=2,H=16,d=64,\)</span>
<span class="math inline">\(csz=4,K=64,m=18\)</span></p>
<blockquote>
<p>The attention keys and values from m=18-th layer of backbone LLM is
cached into CMB.</p>
</blockquote>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/longmem-4.png"
alt="Memory-Augmented Adaptation and Architectural Hyperparameters." />
<figcaption aria-hidden="true">Memory-Augmented Adaptation and
Architectural Hyperparameters.</figcaption>
</figure>
<p>The pre-training and adaptation are trained on <strong>16
32GB-Tesla-V100 GPUs</strong>.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/longmem-5.png"
alt="The superiority of our method over fully dense self-attention (GPT-2*) in terms of inference speed and GPU-memory utilization." />
<figcaption aria-hidden="true">The superiority of our method over fully
dense self-attention (GPT-2*) in terms of <strong>inference speed and
GPU-memory utilization</strong>.</figcaption>
</figure>
<aside>
<p>üëá Memory Retrieval Module</p>
</aside>
<p>The fixed memory-size of <strong>CMB</strong> in one GPU is 65536
key-value pairs of tokens.</p>
<blockquote>
<p>We enable each GPU to construct and update their own memory retrieval
module for efficiency.</p>
<p>The capacity of CMB (M) is not unlimited.</p>
</blockquote>
<p>The retrieval takes about 15ms per 1k tokens, which is 55% timecost
of LLM forwarding pass.</p>
<p>Use <a href="https://arxiv.org/pdf/1702.08734.pdf">faiss</a> to store
the mean-pooled attention keys of <span
class="math inline">\(M/csz\)</span> chunks and perform efficient
retrieval.</p>
<aside>
<p>üëá Baselines</p>
</aside>
<p>Adopt reproduced GPT-2* and <a
href="https://arxiv.org/pdf/2203.08913.pdf">MemTRM</a> as two baseline
under same pre-training setting.</p>
<p>We insert the knn-augmented layer proposed by <a
href="https://arxiv.org/pdf/2203.08913.pdf">MemTRM</a> as the same 18-th
layer in the LLM decoder.</p>
<h3 id="long-context-language-modeling">Long-Context Language
Modeling</h3>
<aside>
<p>üëá Zero-Shot Evaluation Setting(3 long-context modeling datasets)</p>
</aside>
<p>The majority of included books or papers in these datasets have the
length of at least 16k tokens.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/longmem-6.png"
alt="Dataset Statistics of five splits of PG-22 based on length range and ArXiv." />
<figcaption aria-hidden="true">Dataset Statistics of five splits of
PG-22 based on length range and ArXiv.</figcaption>
</figure>
<p><strong>PG-22(Project Gutenberg 2020-2022 Language
Modeling):</strong> We crawled and cleaned the books published between
2020 and 2022 under <a href="https://www.gutenberg.org/">Project
Gutenberg Library</a>.</p>
<blockquote>
<p>Our training subset PG-19 only contains books published before
1919.</p>
</blockquote>
<p><strong>ArXiv:</strong> We select a val split of ArXiv paper subset
in the <a href="https://arxiv.org/pdf/2101.00027.pdf">Pile</a> corpus.
The val split does not exist in our training corpus.</p>
<p><strong><a
href="https://arxiv.org/pdf/2204.10878.pdf">ChapterBreak</a>:</strong>
We select the Archive of Our Own (AO3) subset. Use its splits of 4k, 6k,
and 8k prefix.</p>
<p><strong><em>Details of <a
href="https://arxiv.org/pdf/2204.10878.pdf">ChapterBreak</a>:</em></strong></p>
<ul>
<li>For LLMs that cannot process over 4k tokens, we abandon the front
prefix to fulfill the maximum input length of LLMs.</li>
<li>For <a href="https://arxiv.org/pdf/2203.08913.pdf">MemTRM</a> and
LongMem model, we firstly load the given 4k/6k/8k prefix contexts into
the cached memory(while the input length to local context is still 1k
tokens) and then do the scoring.</li>
<li>We use the perplexity as the scorer for each candidate suffix
segment in zero-shot evaluation manner.</li>
<li>Then the suffix segment with lower perplexity is selected as the
label.</li>
<li>The suffix identification accuracy is used as the evaluation
metric.</li>
</ul>
<aside>
<p>üëá Results</p>
</aside>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/longmem-7.png"
alt="Evaluation results on long-context language modeling datasets. We report token-level perplexity (PPL) (lower the better) on all datasets." />
<figcaption aria-hidden="true">Evaluation results on long-context
language modeling datasets. We report token-level perplexity (PPL)
(lower the better) on all datasets.</figcaption>
</figure>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/longmem-8.png"
alt="Zero-shot Suffix Identification Accuracy on AO3 subset of ChapterBreak. Baselines marked with ‚Ä† are directly cited from ChapterBreak." />
<figcaption aria-hidden="true">Zero-shot Suffix Identification Accuracy
on AO3 subset of ChapterBreak. Baselines marked with ‚Ä† are directly
cited from <a
href="https://arxiv.org/pdf/2204.10878.pdf">ChapterBreak</a>.</figcaption>
</figure>
<h3 id="memory-augmented-in-context-learning">Memory-Augmented
In-Context Learning</h3>
<blockquote>
<p>Conventional ICL is heavily restricted by input context length.</p>
</blockquote>
<p>LongMem: <strong>Infinite-length ICL when loading large number of
demonstration examples into cached memory.</strong></p>
<aside>
<p>üëá Evaluation Setting</p>
</aside>
<ul>
<li>NLU datasets, 4/20-shot: <a
href="https://aclanthology.org/D13-1170.pdf">SST-2</a>, <a
href="https://www.cs.cornell.edu/home/cardie/papers/lre05withappendix.pdf">MPQA</a>,
<a
href="http://richard.cyganiak.de/2008/papers/dbpedia-iswc2007.pdf">MR</a>,
<a href="https://aclanthology.org/P04-1035.pdf">Subj</a>, <a
href="https://aclanthology.org/D13-1170.pdf">SST-5</a></li>
<li>QA dataset, open-ended generation, 3-shot(about 1k tokens): <a
href="https://arxiv.org/pdf/1606.05250.pdf">SQuAD</a></li>
</ul>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/longmem-9.png"
alt="The hand-crafted ICL prompt templates. We concatenate the demonstration examples with newlines to delimit them." />
<figcaption aria-hidden="true">The hand-crafted <strong>ICL prompt
templates</strong>. We concatenate the demonstration examples with
newlines to delimit them.</figcaption>
</figure>
<p>The prediction label is directly generated using greedy decoding.</p>
<p>Chunk size <span class="math inline">\(csz=2\)</span></p>
<aside>
<p>üëá Results</p>
</aside>
<p>We report the mean and standard deviation of 6 runs with different
random seeds to overcome the randomness in selecting k-shot
demonstration examples.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/longmem-10.png"
alt="Accuracy [%] on 5 NLU tasks. We sample 2000 extra demonstration examples and load them into cached memory." />
<figcaption aria-hidden="true">Accuracy [%] on 5 NLU tasks. We sample
2000 extra demonstration examples and load them into cached
memory.</figcaption>
</figure>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/longmem-11.png"
alt="Exact match (EM) and F1 scores on SQuAD. LongMem loads 200 extra demonstration examples into cached memory." />
<figcaption aria-hidden="true">Exact match (EM) and F1 scores on SQuAD.
LongMem loads 200 extra demonstration examples into cached
memory.</figcaption>
</figure>
<h3 id="ablation-studies">Ablation Studies</h3>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/longmem-12.png"
alt="(a) Effects of chunk size on ICL; (b) Effects of memory size, ‚àÜPerplexity on 4 splits of PG-22 (notice msz=16k)." />
<figcaption aria-hidden="true">(a) Effects of chunk size on ICL; (b)
Effects of memory size, ‚àÜPerplexity on 4 splits of PG-22 (notice
msz=16k).</figcaption>
</figure>
<p>In general, the memory size should be compatible with the average
length of documents or contexts, i.e., a set of books with average 16k
tokens should deploy the memory size of 16k tokens in cached memory.</p>
<h2 id="related-work">Related Work</h2>
<p><strong>(1) Large Language Models</strong></p>
<p><strong>(2) x-formers</strong></p>
<p>x-former enables transformers to attend on longer context. However,
existing x-formers is <strong>not efficient</strong> and their
<strong>upper-bound seq_len</strong> is only 16k tokens.</p>
<p><a href="https://arxiv.org/pdf/1901.02860.pdf">Transformer-XL</a>
proposes to cache attention keys and values of past segment and reuse
them in recurrent manner.</p>
<p><a href="https://arxiv.org/pdf/2006.04768.pdf">LinFormer</a>, <a
href="https://arxiv.org/pdf/2004.05150.pdf">LongFormer</a>, <a
href="https://arxiv.org/pdf/2003.05997.pdf">Routing Transformer</a>
propose various sparse attention mechanisms for decreasing <span
class="math inline">\(O(n^2)\)</span> complexity to <span
class="math inline">\(O(n log n)\)</span> or even <span
class="math inline">\(O(n)\)</span>.</p>
<p><a href="https://arxiv.org/pdf/2007.14062.pdf">BigBird</a> achieves a
4k sequence length via attending on a subset of context tokens.</p>
<p><strong>(3) Side-Tuning (<a
href="https://arxiv.org/pdf/1912.13503.pdf">paper1</a>, <a
href="https://arxiv.org/pdf/2206.06522.pdf">paper2</a>)</strong></p>
<p>Side-Tuning is a <strong>task-specific</strong> tuning method for
pre-trained models via training a lightweight side-network that is fused
with the fixed pre-trained network via summation.</p>
<blockquote>
<p>In constrast, LongMem has well zero-shot performance.</p>
</blockquote>
<p>Our method distinguishes the side-tuning method in terms of
<strong>learning objective</strong> and <strong>cross-network fusion
ways</strong>.</p>
<p>LongMem proposes to augment LLMs with decoupled memory for memorizing
long past inputs, which does not involve any task-specific tuning.</p>
<p>The cross-network residual connections proposed by LongMem is novel
and distincts from the vanilla summation of Side-Tuning.</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>2023</category>
        <category>06</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Side-Tuning</tag>
        <tag>X-Formers</tag>
        <tag>PEFT</tag>
      </tags>
  </entry>
  <entry>
    <title>(LENS)Towards Language Models That Can See: Computer Vision Through the LENSüîç of Natural Language</title>
    <url>/2023/07/03/LENS/</url>
    <content><![CDATA[<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-0.png" /></p>
<p>Paper: <a
href="https://arxiv.org/pdf/2306.16410.pdf">https://arxiv.org/pdf/2306.16410.pdf</a></p>
<p>Code: <a
href="https://github.com/ContextualAI/lens">https://github.com/ContextualAI/lens</a></p>
<p>BlogPost: <a
href="https://contextual.ai/introducing-lens/">https://contextual.ai/introducing-lens/</a></p>
<p>Demo: <a
href="https://lens.contextual.ai/">https://lens.contextual.ai/</a></p>
<p><strong>Problems</strong>: Existing alignment methods for visual and
language modalities need training, which requires heavy computation
burden and large corpora.</p>
<p><strong>Contributions</strong>: Training-free LENS enables LLM to
have visual capabilities.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-1.png"
alt="Comparison of approaches for aligning visual and language modalities. Old-Style trains visual encoder. Flamingo inserts new layer into LLM. BLIP-2 introduces new lightweight module between visual encoder and LLM." />
<figcaption aria-hidden="true">Comparison of approaches for aligning
visual and language modalities. Old-Style trains visual encoder.
Flamingo inserts new layer into LLM. BLIP-2 introduces new lightweight
module between visual encoder and LLM.</figcaption>
</figure>
<h2 id="related-work">Related Work</h2>
<ul>
<li><a href="https://arxiv.org/pdf/2212.10846.pdf">Img2Prompt</a> uses
LLMs for solving VQA tasks.(In contrast, LENS extends the capacity of
LLM to also solve object recognition tasks.)</li>
<li><a href="https://arxiv.org/pdf/2303.08128.pdf">ViperGPT</a> also
leverages LLMs to solve VQA tasks, but heavily relies on BLIP2 which
needs extra training rounds of multimodal pre-training.</li>
</ul>
<p><strong>[Img2Prompt, ViperGPT] ‚Äútop-down‚Äù</strong>
approach(question-aware): attention mechanisms are driven by nonvisual
or task-specific contexts.</p>
<p><strong>[LENS] <a
href="https://arxiv.org/pdf/1707.07998.pdf">‚Äúbottom-up‚Äù</a></strong>
approach: does not involve any question-guided information
extraction</p>
<h2 id="method">Method</h2>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-2.png"
alt="The training-free LENS framework. Via a set of ‚Äúvision modules‚Äù, a frozen LLM can perform object recognition or visual reasoning tasks." />
<figcaption aria-hidden="true">The training-free LENS framework. Via a
set of ‚Äúvision modules‚Äù, a frozen LLM can perform object recognition or
visual reasoning tasks.</figcaption>
</figure>
<h3 id="two-visual-vocabularies"><strong>Two</strong> <strong>Visual
Vocabularies</strong></h3>
<blockquote>
<p>They act as a bridge to convert an image into textual info.</p>
</blockquote>
<p><strong>Tag(Object) Vocabulary</strong>: We collect tags from various
sources, including image classification(<a
href="https://arxiv.org/pdf/1409.0575.pdf">ImageNet</a>, <a
href="https://data.caltech.edu/records/mzrjq-6wc02">Caltech 101</a>, <a
href="https://arxiv.org/pdf/1311.3618.pdf">DTD</a>, <a
href="https://ieeexplore.ieee.org/document/6248092">Cats and Dogs</a>,
<a
href="https://www.robots.ox.ac.uk/~vgg/publications/2008/Nilsback08/nilsback08.pdf">Flower</a>,
<a
href="https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/">Food
101</a>, <a
href="https://vision.princeton.edu/projects/2010/SUN/paper.pdf">SUN</a>,
<a href="http://vision.stanford.edu/pdf/3drr13.pdf">3D</a>), object
detection and semantic segmentation(<a
href="https://arxiv.org/pdf/1908.03195.pdf">LAVIS</a>, <a
href="https://arxiv.org/pdf/1405.0312.pdf">MS COCO</a>, <a
href="https://arxiv.org/pdf/1811.00982.pdf">Open Images V4</a>), <a
href="https://arxiv.org/pdf/1602.07332.pdf">Visual Genome</a>.</p>
<p><strong>Attribute Vocabulary</strong>: Following <a
href="https://arxiv.org/pdf/2210.07183.pdf">Menon &amp; Vondrick</a>, we
employ GPT-3 to generate descriptions of the visual characteristics that
differentiate each object category.</p>
<h3 id="four-lens-components">Four LENS<strong>üîç</strong>
Components</h3>
<p><strong>Tag Module</strong> identifies and assigns suitable tags to
the image via CLIP. We adopt a common prompt: "A photo of
{tagname}".</p>
<p><strong>Attributes Module</strong> identifies and assigns relevant
attributes to the objects present in the image via CLIP. We incorporates
the task-specific prompts outlined in <a
href="https://arxiv.org/pdf/2210.07183.pdf">Menon &amp;
Vondrick</a>.</p>
<p><strong>Intensive Captioner</strong> utilizes BLIP and applies <a
href="https://arxiv.org/pdf/1805.04833.pdf">stochastic top-k
sampling</a> to generate N captions per image.</p>
<p><strong>Reasoning Module</strong> generates answers based on the
textual descriptions from vision modules, along with the task-specific
instructions.</p>
<h3 id="prompt-design">Prompt Design</h3>
<blockquote>
<p>Concatenate generic prompts with task-specific prompts.</p>
<p>Can see in demo.</p>
</blockquote>
<p>OCR is optional(for hateful-memes task).</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># prompt template</span><br><span class="line">Tags: &#123;Top-k tags&#125;</span><br><span class="line">Attributes: &#123;Top-K attributes&#125;</span><br><span class="line">Captions: &#123;Top-N Captions&#125;</span><br><span class="line">OCR: this is an image with written &quot;&#123;meme text&#125;&quot; on it</span><br><span class="line">Question: &#123;task-specific prompt&#125; \n Short Answer:</span><br></pre></td></tr></table></figure>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-3.png"
alt="Object recognition prompt used in LENS." />
<figcaption aria-hidden="true">Object recognition prompt used in
LENS.</figcaption>
</figure>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-4.png"
alt="VQA prompt used in LENS." />
<figcaption aria-hidden="true">VQA prompt used in LENS.</figcaption>
</figure>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-5.png"
alt="Hateful-memes prompt used in LENS." />
<figcaption aria-hidden="true">Hateful-memes prompt used in
LENS.</figcaption>
</figure>
<h2 id="experiments">Experiments</h2>
<blockquote>
<p>The experiments is only about Flan-T5 and do not include other
LLMs.</p>
</blockquote>
<h3 id="datasets">Datasets</h3>
<p><strong>Object Recognition</strong>: 8 benchmarks in <a
href="https://arxiv.org/pdf/2103.00020.pdf">CLIP</a>.</p>
<p><strong>VL Reasoning</strong>: the test-dev split of <a
href="https://arxiv.org/pdf/1612.00837.pdf">VQA 2.0</a>, the test set of
<a href="https://arxiv.org/pdf/1906.00067.pdf">OK-VQA</a>.</p>
<p><strong>Others</strong>: the dev and test-seen sets of the <a
href="https://arxiv.org/pdf/2005.04790.pdf">Hateful Memes</a>, the test
set of <a href="https://arxiv.org/pdf/2103.00020.pdf">Rendered
SST2</a>.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-6.png"
alt="Datasets examined for evaluation of LENS. The closed-ended manner uses a fixed class(/tag/object) vocabulary. ." />
<figcaption aria-hidden="true">Datasets examined for evaluation of LENS.
The closed-ended manner uses a fixed class(/tag/object) vocabulary.
.</figcaption>
</figure>
<h3 id="implementation-details">Implementation Details</h3>
<ul>
<li><strong>Tags and attributes modules</strong>: <a
href="https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K">OpenCLIP-H/142</a>,
<a
href="https://huggingface.co/openai/clip-vit-large-patch14">CLIP-L/143</a></li>
<li><strong>Captioner</strong>: <a
href="https://huggingface.co/Salesforce/blip-image-captioning-large">BLIP-large4</a>
finetuned on COCO to generate 50 captions per image.</li>
<li><strong>Frozen LLMs</strong>: <a
href="https://arxiv.org/pdf/2301.13688.pdf">Flan-T5</a> models.</li>
</ul>
<p>We employ beam search with number of beams equal to 5.</p>
<blockquote>
<p>Beam search is an improved algorithm for greedy search.</p>
</blockquote>
<p>Additionally, we apply a length penalty equal to -1, encouraging the
generation of concise answers as in <a
href="https://arxiv.org/pdf/2301.12597.pdf">BLIP-2</a>.</p>
<p>These experiments were conducted on 8 NVIDIA A100 (40GB) GPUs.</p>
<aside>
<p>üëá Task-specific optimizations</p>
</aside>
<ul>
<li><strong>Object recognition</strong>: we utilize the tag module and
the attribute module, but skip the intensive captioning modules.</li>
<li><strong>VQA</strong>: we solely use the intensive captioning
module.</li>
<li><strong>Hateful Memes and Rendered-SST2</strong>: we incorporate the
tag, attributes, and captioning modules.</li>
</ul>
<p>We generate only one caption using beam search with a width of 5.</p>
<h3 id="results">Results</h3>
<aside>
<p>üëá Object recognition(0/1/3-shot)</p>
</aside>
<p>We compare LENS with SOTA models following <a
href="https://arxiv.org/pdf/2210.07183.pdf">Menon &amp;
Vondrick</a>.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-7.png"
alt="Zero-shot results for LENS in object recognition tasks." />
<figcaption aria-hidden="true"><strong>Zero-shot results</strong> for
LENS in object recognition tasks.</figcaption>
</figure>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-8.png"
alt="Average few-shot performance of LENS on vision tasks except ImageNet (due to its large size)." />
<figcaption aria-hidden="true">Average few-shot performance of LENS on
vision tasks except ImageNet (due to its large size).</figcaption>
</figure>
<ul>
<li>More shots help to increase performance.</li>
<li>The both results show direct relationship between ViT size and
performance(but not for LLM size).</li>
</ul>
<aside>
<p>üëá Vision and Language reasoning(0-shot)</p>
</aside>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-9.png"
alt="The highly competitive nature of LENS." />
<figcaption aria-hidden="true">The highly competitive nature of
LENS.</figcaption>
</figure>
<h3 id="ablations-on-lens-components">Ablations on LENS components</h3>
<aside>
<p>üëá Object recognition</p>
</aside>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-10.png"
alt="Ablations results using OpenCLIP-H/14 as vision encoder and Flan-T5_{XL} as the LLM." />
<figcaption aria-hidden="true">Ablations results using OpenCLIP-H/14 as
vision encoder and <span class="math inline">\(Flan-T5_{XL}\)</span> as
the LLM.</figcaption>
</figure>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-11.png"
alt="The detailed results." />
<figcaption aria-hidden="true">The detailed results.</figcaption>
</figure>
<p>The object information helps more than the attributes, but together
they are complimentary and lead to overall better performance.</p>
<aside>
<p>üëá Vision and Language reasoning</p>
</aside>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-12.png"
alt="Ablation results of LENS with Flan-T5XXL on VQA 2.0 minival split." />
<figcaption aria-hidden="true">Ablation results of LENS with Flan-T5XXL
on VQA 2.0 minival split.</figcaption>
</figure>
<p>Increasing the caption num improves the performance gradually but
starts saturating eventually.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-13.png"
alt="On the dev set of Hateful Memes. Adding more visual information improves the performance consistently." />
<figcaption aria-hidden="true">On the dev set of Hateful Memes. Adding
more visual information improves the performance
consistently.</figcaption>
</figure>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-14.png"
alt="Selected examples of LENS with all prompts, illustrating its reasoning capabilities by answering questions about complex scenes and scenarios." />
<figcaption aria-hidden="true"><strong>Selected examples of LENS with
all prompts</strong>, illustrating its reasoning capabilities by
answering questions about complex scenes and scenarios.</figcaption>
</figure>
<h2 id="conclusion">Conclusion</h2>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-15.png"
alt="Failure examples of LENS." />
<figcaption aria-hidden="true"><strong>Failure examples of
LENS.</strong></figcaption>
</figure>
<ol type="a">
<li><p><strong>Incorrect Visual Info</strong> of objects or attributes
from CLIP and BLIP</p></li>
<li><p><strong>Inconsistency</strong> between the responses</p></li>
<li><p><strong>Presuppositions</strong> and in-built biases</p></li>
<li><p><strong>Forgetting</strong> and limitations of context windows of
the LLMs.</p></li>
</ol>
<p><strong><em>Limitations of LENS</em></strong></p>
<ul>
<li>the vision capability of LENS heavily relies on CLIP and BLIP</li>
<li>LENS evaluation still requires substantial computational
resources.</li>
</ul>
<p><strong><em>Future work</em></strong></p>
<ul>
<li>more effective ways integrating visual modules</li>
<li>more efficient methods to reduce the computational burden</li>
<li>expand to more modalities(e.g., audio, video)</li>
</ul>
]]></content>
      <categories>
        <category>Paper</category>
        <category>2023</category>
        <category>06</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Multi-modal</tag>
        <tag>Expert Integration</tag>
      </tags>
  </entry>
  <entry>
    <title>(Lynx)What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?</title>
    <url>/2023/07/06/Lynx/</url>
    <content><![CDATA[<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-0.png" /></p>
<p>Paper: <a
href="https://arxiv.org/pdf/2307.02469.pdf">https://arxiv.org/pdf/2307.02469.pdf</a></p>
<p>Project Page: <a
href="https://lynx-llm.github.io/">https://lynx-llm.github.io/</a></p>
<aside>
<p>üëá Problems</p>
</aside>
<p>The performance of multimodal LLMs heavily relies on network
structures, training data, instruction diversity, and evaluation
benchmarks, which have not been extensively discussed.</p>
<aside>
<p>üëá Contributions</p>
</aside>
<ol type="1">
<li>Present a systematic and comprehensive study, quantitatively and
qualitatively.
<ul>
<li><strong>Network Structures</strong>
<ul>
<li>LLaMA vs Vicuna</li>
<li>Prefix-tuning vs Cross-attention</li>
</ul></li>
<li><strong>Training Data</strong>(combinations and sampling strategies)
<ul>
<li>Quality vs Quantity</li>
<li>COYO700M, DataComp1B, BlipCapFilt</li>
</ul></li>
<li><strong>Instruction Diversity</strong>
<ul>
<li>500 instructions for over 50 tasks</li>
</ul></li>
<li><strong>Evaluation</strong>. Collect the first comprehensive
evaluation set including both image and video tasks through
crowd-sourcing.</li>
</ul></li>
<li>Present Lynx model.</li>
</ol>
<h1 id="lynx">1 - Lynx</h1>
<figure>
<img src="https://lynx-llm.github.io/static/images/lynx.png"
alt="Our model is based on prefix-tuning architecture. In contrast to the cross-attention-based models like Flamingo." />
<figcaption aria-hidden="true">Our model is based on prefix-tuning
architecture. In contrast to the cross-attention-based models like
Flamingo.</figcaption>
</figure>
<p>Our model is based on prefix-tuning architecture. In contrast to the
cross-attention-based models like Flamingo.</p>
<h2 id="formulations">1.1 - Formulations</h2>
<p><strong>Prefix-Tuning</strong>: Visual tokens <span
class="math inline">\(\mathbf{w}_v=\{w_i\}_{i=1}^V\)</span> are directly
concatenated with instruction tokens <span
class="math inline">\(\mathbf{w}_l=\{w_j\}_{j=V+1}^{V+L}\)</span>.</p>
<p>Sentence prediction equation: <span
class="math inline">\(p(w_{V+L+1:V+L+T}|w_{1:V+L})\sim\prod\limits_{t=V+L+1}^{V+L+T}P(w_t|w_{&lt;t})\)</span>,
ended by <EOS>.</p>
<h2 id="details-of-model-architecture">1.2 - Details of Model
Architecture</h2>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-1.png"
alt="Lynx: train trainable adapters based on frozen Vicuna. Token dim in hidden state is 4096, while 2048 in adapter." />
<figcaption aria-hidden="true">Lynx: train trainable adapters based on
frozen Vicuna. Token dim in hidden state is 4096, while 2048 in
adapter.</figcaption>
</figure>
<h3 id="adapter">1.2.1 - Adapter</h3>
<p>The trainable adapters are inserted into the LLMs after every <span
class="math inline">\(M\)</span> blocks.(<span
class="math inline">\(M=1\)</span>)</p>
<h3 id="visual-encoder">1.2.2 - Visual Encoder</h3>
<p>Apply <a href="https://arxiv.org/pdf/2211.07636.pdf">EVA-1B</a> as
<strong>Visual Encoder</strong> <span
class="math inline">\(\phi_v(x)\)</span> to map an image <span
class="math inline">\(x\)</span> with resolution <span
class="math inline">\(H\times W\)</span> to a sequence of <span
class="math inline">\(\frac{H}{14}\times \frac{W}{14}\)</span> visual
tokens.</p>
<p><a href="https://arxiv.org/pdf/2103.03206.pdf">Resampler</a> <span
class="math inline">\(\Phi\)</span> mechanism reduces the dimensions of
vision inputs.</p>
<p>By injecting the long vision token sequence into a short and
<strong>learnable query sequence</strong> <span
class="math inline">\(\mathbf{w}_v^q\)</span>, we adapt the <span
class="math inline">\(\Phi\)</span> to improve the efficiency of
training and inference: <span
class="math inline">\(\mathbf{w}_v=\Phi(\phi_v(x),\mathbf{w}_v^q)\)</span>.</p>
<p><span class="math inline">\(\mathbf{w}_v\)</span> is the
<strong>condensed sequence</strong> of 32 tokens.</p>
<h2 id="pretraining">1.3 - Pretraining</h2>
<p>Utilize <span class="math inline">\(&gt;120M\)</span> image-text
pairs to build inter-modality connections.</p>
<p>Next-word prediction training with the cross entropy loss.(same in
finetuing)</p>
<blockquote>
<p>Compared to the contrastive pretraining, pretraining with next-word
prediction requires data with fluent texts that can represent the
‚Äúnatural‚Äù causal dependency between the predicted word and the past
context very well.</p>
</blockquote>
<p><span class="math inline">\(0\sim100k\)</span> steps for <span
class="math inline">\(224\times224\)</span> images, <span
class="math inline">\(100k\sim110k\)</span> steps for <span
class="math inline">\(420\times420\)</span> images.</p>
<p>After 110k steps, we freeze the visual encoder and <strong>thus the
expense of increasing image resolution is affordable</strong>.</p>
<h2 id="instruction-fintuning">1.4 - Instruction Fintuning</h2>
<p>We collect an instruction finetuning multi-modal dataset based on the
public ones.</p>
<p>Different <strong>weight combinations</strong> of instruction data
have a crucial influence on the final performance.</p>
<aside>
<p>üëâ (1) 50+ text-only, image-text, video-text tasks belonging to
<strong>5 Categories</strong>:</p>
</aside>
<ol type="1">
<li>Text-only Instruction-Following</li>
<li>Image/Video Visual Question Answering</li>
<li>Image/Video Captioning</li>
<li>Classification</li>
<li>Image-conditioned Dialog for Complex Reasoning and Instruction
Following</li>
</ol>
<p><strong>Provide corresponding instructions for each
task:</strong></p>
<p>Manually label ‚â•3 instructions and prompt GPT4 to automatically
generate more.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Here are some instructions that define a visual-language task. Continue to write 15 instructions with the same meaning: 1) PROMPT1; 2) PROMPT2; 3) PROMPT3;</span><br></pre></td></tr></table></figure>
<aside>
<p>üëâ (2) available public instruction data: FlanT5, Alpaca, Mini-GPT4,
LLaVA, Baize</p>
</aside>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-2.png" /></p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-3.png" /></p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-4.png"
alt="Training Data. \sim14B tokens for the pretraining, \sim3B for the instruction-finetuning. Ratio indicates weight strategy." />
<figcaption aria-hidden="true"><strong>Training Data.</strong> <span
class="math inline">\(\sim14B\)</span> tokens for the pretraining, <span
class="math inline">\(\sim3B\)</span> for the instruction-finetuning.
Ratio indicates weight strategy.</figcaption>
</figure>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-5.png"
alt="Training hyperparameters. Some parameters not use learning rate decay schedule. Use the DeepSpeed to accelerate training, and set the BFloat16 as the default model precision." />
<figcaption aria-hidden="true"><strong>Training
hyperparameters</strong>. Some parameters not use learning rate decay
schedule. Use the DeepSpeed to accelerate training, and set the BFloat16
as the default model precision.</figcaption>
</figure>
<h1 id="experiment">2 - Experiment</h1>
<p><strong>Description-first strategy</strong>: Before sending the
request from the user, we feed a fixed prompt ‚ÄúDescribe the image in
detail‚Äù first in the ‚Äú0th‚Äù round of the conversation. After that, the
user‚Äôs instructions will be sequentially processed.</p>
<p>During the deployment, this strategy improves performance of most
models(but not for MiniGPT4).</p>
<p>For MiniGPT4, we generated the response with its default
settings.</p>
<p>For mPLUG-owl, we follow the default parameters presented at <a
href="http://vlarena.opengvlab.com/">http://vlarena.opengvlab.com/</a>.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-6.png"
alt="Hyper-parameters for Generation during the deployment. We set hyper-parameters to encourage short response generation for Open-VQA." />
<figcaption aria-hidden="true">Hyper-parameters for Generation during
the deployment. We set hyper-parameters to encourage short response
generation for Open-VQA.</figcaption>
</figure>
<h2 id="evaluation-protocols">2.1 - Evaluation Protocols</h2>
<aside>
<p>üëâ (1) Open-VQA(ours); (2) <a
href="https://arxiv.org/pdf/2304.14178.pdf">OwlEval</a> from mPLUG-Owl;
(3) <a href="https://arxiv.org/pdf/2306.13394.pdf">MME</a></p>
</aside>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-7.png"
alt="Manually collected Open-VQA supports open-ended answers, which contains diverse questions on objects, OCR, counting, reasoning, action recognition, chronological ordering, etc." />
<figcaption aria-hidden="true">Manually collected
<strong>Open-VQA</strong> supports <strong>open-ended answers</strong>,
which contains diverse questions on objects, OCR, counting, reasoning,
action recognition, chronological ordering, etc.</figcaption>
</figure>
<p><strong>Open-VQA</strong> consists of 450 samples, based on VQA 2.0,
OCRVQA, Place365, MSVD, MSRVTT, SthV2.</p>
<p>Though Place365 is a classification task and SthV2 is a video
captioning task, we write proper prompts to make them both VQA
tasks.</p>
<p><strong>Use GPT4 as judger via following prompt:</strong></p>
<blockquote>
<p>GPT4 achieves a consistency of more than 95% compared with humans.
(We evaluate the consistency on 100 samples from a randomly selected
subset with our model.)</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Given the question ‚ÄúQUESTION‚Äù, does the answer ‚ÄúPREDICTION‚Äù imply the answer ‚ÄúGROUND_TRUTH‚Äù? Answer with Yes or No.</span><br></pre></td></tr></table></figure>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-8.png"
alt="To make the comparison fair, we pad each image in the OwlEval with 8 pixels." />
<figcaption aria-hidden="true">To make the comparison fair, we pad each
image in the <strong>OwlEval</strong> with 8 pixels.</figcaption>
</figure>
<p>Recruit <strong>humans</strong> to evaluate the quality(first
<strong>correctness</strong>, then <strong>richness</strong>) of
language generation.</p>
<p><strong>Scores</strong> range from 1 to 5 with <strong>2
rules</strong>:</p>
<ol type="1">
<li>At most 2 models that gain equal scores</li>
<li>for each annotator, total tie num ‚â§ 10 for the whole set</li>
</ol>
<h2 id="quantitative-experiments">2.2 - Quantitative Experiments</h2>
<aside>
<p>üëâ The key to <strong>balance language generation and
correctness</strong> is a high-quality VL dataset that (1) includes high
quality and fluent texts (2) aligns the texts and images well</p>
</aside>
<ul>
<li>If a model has lower accuracy on Open-VQA, it tends to make factual
errors inconsistent with the given image during text generation.
(under-training on VL tasks)</li>
<li>Models with higher performance on Open-VQA usually tend to lose
language generation ability, e.g., generate short sentences.
(over-training on VL tasks)</li>
</ul>
<figure>
<img src="https://lynx-llm.github.io/static/images/result_1.png"
alt="Compare existing multi-modal LLMs on the Open-VQA image benchmark. InstructBLIP and Lynx achieve high performance." />
<figcaption aria-hidden="true">Compare existing multi-modal LLMs on the
Open-VQA image benchmark. InstructBLIP and Lynx achieve high
performance.</figcaption>
</figure>
<p>Compare existing multi-modal LLMs on the Open-VQA image benchmark.
InstructBLIP and Lynx achieve high performance.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-9.png"
alt="Different from InstructBLIP, Lynx is more user-friendly." />
<figcaption aria-hidden="true">Different from InstructBLIP, Lynx is more
user-friendly.</figcaption>
</figure>
<figure>
<img src="https://lynx-llm.github.io/static/images/result_all.png"
alt="(b) MME: All scores are normalized to the range from 0 to 100. (d) Ablation study on our Open-VQA videos." />
<figcaption aria-hidden="true">(b) MME: All scores are normalized to the
range from 0 to 100. (d) Ablation study on our Open-VQA
videos.</figcaption>
</figure>
<ol start="2" type="a">
<li>MME: All scores are normalized to the range from 0 to 100. (d)
Ablation study on our Open-VQA videos.</li>
</ol>
<p>Lynx is good at MME perception tasks including Color, Celebrity,
Scene, Landmark, Position, Count, and Existence.</p>
<h2 id="ablation-study">2.3 - Ablation Study</h2>
<aside>
<p>üëâ What matters to train a high-performance GPT4-style model?</p>
</aside>
<blockquote>
<p>A GPT4-style LLM is defined as a decoder-only transformer that takes
both visual and instructional tokens as inputs and generates responses
in text auto-regressively.</p>
</blockquote>
<ul>
<li><p><strong>LLaMA</strong>(language generation) <strong>vs.
Vicuna</strong>(correctness, instruction-following ability)</p></li>
<li><p><strong>Impact of Diversified Instructions</strong>(during
training)</p></li>
<li><p><strong>Impact of Training Data</strong>(noise: COYO700M,
DataComp1B)</p>
<blockquote>
<p>attribute the worse results to the difference between generative
pretraining and contrastive pretraining.</p>
</blockquote></li>
<li><p><strong>Prefix-Tuning vs. Cross-Attn</strong> (follow
Open-Flamingo)</p>
<blockquote>
<p>We only use multi-modal instruction data for pre-training. For the
finetuning stage, we experiment with two variants, with or without
trainable LLM, i.e., with or without the use of text instruction
data.</p>
</blockquote></li>
<li><p><strong>Impact of Larger Image Resolution</strong></p></li>
</ul>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-10.png"
alt="Ablation study on our Open-VQA images." />
<figcaption aria-hidden="true">Ablation study on our Open-VQA
images.</figcaption>
</figure>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-11.png"
alt="Human evaluation of different ablation models. (a) w/ LLaMA vs w/ Vicuda; (b) w/o diversified instructions vs w/ diversified prompts; (c) w/ large-scale noisy data vs w/o large-scale noisy data; (d) prefix-finetuning vs cross-attention." />
<figcaption aria-hidden="true">Human evaluation of different ablation
models. (a) w/ LLaMA vs w/ Vicuda; (b) w/o diversified instructions vs
w/ diversified prompts; (c) w/ large-scale noisy data vs w/o large-scale
noisy data; (d) prefix-finetuning vs cross-attention.</figcaption>
</figure>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-12.png" /></p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-13.png"
alt="w/ diversified prompts versus w/o diversified prompts." />
<figcaption aria-hidden="true">w/ diversified prompts versus w/o
diversified prompts.</figcaption>
</figure>
<h1 id="related-work">3 - Related Work</h1>
<p><strong>(1) Centralized Multi-modal Interactive System</strong></p>
<ul>
<li>„Äêexisting work„ÄëVisual ChatGPT, MM-REACT, HuggingGPT, InternGPT,
SayCan, InnerMonologue</li>
<li>„Äêadavantage„Äëaddress problems that are well-defined</li>
<li>„Äêlimit„Äëlack zero-shot ability to handle open-ended
instructions</li>
</ul>
<p><strong>(2) End-to-end Multi-modal LLMs</strong></p>
<blockquote>
<p>adding some additional trainable parameters</p>
<ul>
<li>„Äêtype 1„ÄëCross-Attention(Flamingo)</li>
<li>„Äêtype 2„Äëdirectly concatenate visual and textual tokens</li>
<li>BLIP2: Q-Former</li>
<li>PaLM-E: no fixed layers</li>
<li>Mini-GPT4: projection layer</li>
<li>LLaVA: tunes LLM during the instruction finetuning stage</li>
<li>mPLUG-Owl: first stage tunes vision encoder, second stage tunes
LLM</li>
<li>Kosmos-1: train a LLM from scratch</li>
</ul>
</blockquote>
<h1 id="conclusions">4 - Conclusions</h1>
<h2 id="findings-and-takeaways">4.1 - Findings and Takeaways</h2>
<ul>
<li>Prefix-tuning may be the currently best way to multi-modal
adaptation for LLMs.</li>
<li>Multi-modal LLMs are not as instruction-following as LLMs.</li>
<li>The quality of training data is critical to model performance.</li>
<li>Diverse tasks and prompts are crucial for zero-shot abilities.</li>
<li>Balancing the correctness and language generation ability is
important.</li>
</ul>
<h2 id="limitations">4.2 - Limitations</h2>
<ul>
<li><strong>Training Data</strong>.
<ul>
<li>Struggle to <strong>balance different
abilities</strong>(correctness&amp;language generation, long&amp;short
answer)</li>
<li>no available image-text datasets that contain <strong>long
texts</strong> which are ideal for pretraining</li>
<li>do not find the <strong>optimal data combination</strong> strategy
restricted by computational resource</li>
</ul></li>
<li><strong>Safety</strong>. We do not conduct safety checks and
restrictions(e.g., ethical, political, and racism issues).</li>
</ul>
<h2 id="future-work">4.3 - Future Work</h2>
<ul>
<li>Scale up model size</li>
<li>a high-quality VL dataset for training
<ul>
<li>Larger and more diverse instructional tasks</li>
<li>Find optimal data combination</li>
</ul></li>
<li>More comprehensive evaluation</li>
<li>Multi-Lingual</li>
<li>Safety</li>
</ul>
]]></content>
      <categories>
        <category>Paper</category>
        <category>2023</category>
        <category>07</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Multi-modal</tag>
        <tag>Prefix-Tuning</tag>
        <tag>Adapter</tag>
        <tag>Dataset</tag>
        <tag>Instrcution-Following</tag>
        <tag>Evaluation</tag>
      </tags>
  </entry>
  <entry>
    <title>A Survey on Multimodal Large Language Models</title>
    <url>/2023/06/30/MLLM/</url>
    <content><![CDATA[<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-author.png" /></p>
<p>Paper: <a
href="https://arxiv.org/pdf/2306.13549.pdf">https://arxiv.org/pdf/2306.13394.pdf</a></p>
<p>Code: <a
href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models">https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models</a></p>
<aside>
<p>üí° <strong><em>Why we need MLLM?</em></strong></p>
</aside>
<p><strong>MLLM</strong>: the LLM-based model with the ability to
receive and reason with multimodal information</p>
<p><strong>Three Advantages</strong>: (1) more in line with human
perception (2) more user-friendly interface (3) more well-rounded
task-solver</p>
<p><strong>Three Example Capabilities</strong>: (1) write website codes
based on images (2) understand deep meaning of memes (3) OCR-free math
reasoning</p>
<p><strong>Four Key Techniques</strong>: M-IT, M-ICL, M-CoT, LAVR</p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-1.png" /></p>
<hr />
<h2 id="m-it-multimodal-instruction-tuning">M-IT: Multimodal Instruction
Tuning</h2>
<h3 id="preliminaries">Preliminaries</h3>
<p><strong>Instruction</strong>: the description of tasks</p>
<p><strong>Instruction Tuning</strong>: a technique of finetuning LLMs
on instruction-formatted datasets</p>
<p><strong>Instruction Tuning 3 Advantages:</strong></p>
<ol type="1">
<li><p>better understand and respond to various human requests</p></li>
<li><p>zero-shot generalization to new tasks</p></li>
<li><p>non-experts can use natural language to interact with LLMs</p>
<blockquote>
<p><em>The most popular programming language in the future will be
English.</em></p>
</blockquote></li>
</ol>
<p><strong>Instruction Tuning &amp; typical learning paradigms.</strong>
A: large-scale task-specific training data; B: zero-shot performance is
still quite average</p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-2.png" /></p>
<p><strong>Extend from Unimodality to Multimodality</strong>. Predict
the next token of the response. The instruction template is flexible and
subject to manual designs. It can also be generalized to multi-round
instructions.</p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-3.png" /></p>
<h3 id="data">Data</h3>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-4.png" /></p>
<aside>
<p>üëâ <strong>Benchmark Adaptation</strong></p>
</aside>
<p><strong>Instruction templates for VQA datasets</strong>.
&lt;image&gt; and {Question} are the image and the question in the
original VQA datasets, respectively. Utilize existing benchmark datasets
to construct instruction-formatted datasets.</p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-5.png" /></p>
<p><strong>Directly using concise answers may limit the output length of
MLLM</strong>.</p>
<ul>
<li>Modify Instructions: ChatBridge, InstructBLIP</li>
<li>Extend the length of existing answers: <span
class="math inline">\(ùëÄ^3 ùêºùëá\)</span></li>
</ul>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-6.png" /></p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-7.png" /></p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-8.png" /></p>
<aside>
<p>üëâ <strong>Self-Instruction</strong></p>
</aside>
<p><strong>Self-Instruction</strong>: To meet real-world human needs,
such as multiple rounds of conversations.</p>
<ul>
<li><strong>LLaVA-Instruct-150k</strong>: an M-IT dataset</li>
<li><strong>MiniGPT-4, ChatBridge, GPT4Tools, DetGPT</strong>: develop
different M-IT datasets catering for different needs</li>
</ul>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-9.png" /></p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-10.png" /></p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-11.png" /></p>
<aside>
<p>üëâ <strong>Hybrid Composition</strong></p>
</aside>
<p>M-IT data + language-only user-assistant conversation data.</p>
<p><strong>Adavantage</strong>: conversational proficiencies,
instruction-following abilities</p>
<p><strong>Representative work</strong>:</p>
<ul>
<li><strong>LaVIN</strong>: directly constructs a minibatch by randomly
sampling from both data</li>
<li><strong>MultiInstruct</strong>: two transfer learning strategies
<ul>
<li>Mixed instruction tuning: combine both types of data and randomly
shuffle. (The empirical results show that mixed instruction tuning is at
least not worse than solely tuning on multimodal data.)</li>
<li>Sequential instruction tuning: text data followed by multimodal
data</li>
</ul></li>
</ul>
<h3 id="modality-bridging">Modality Bridging</h3>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-12.png" /></p>
<aside>
<p>üëâ <strong>Learnable Interface</strong></p>
</aside>
<p><strong>Adavantages</strong>: little cost, avoid catastrophic
forgetting</p>
<p><strong>Three manners</strong>:</p>
<ol type="1">
<li><strong>Query-Based</strong>: (learnable query tokens) Flamingo,
BLIP-2</li>
<li><strong>Projection-Based</strong>: LLaVA, MedVInTTE</li>
<li><strong>Parameter-Efficient Tuning</strong>: LLaMA-Adapter,
LaVIN</li>
</ol>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-13.png" /></p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-14.png" /></p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-15.png" /></p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-16.png" /></p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-17.png" /></p>
<aside>
<p>üëâ <strong>Expert Model</strong></p>
</aside>
<p><strong>Advantage</strong>: Via expert models, convert multimodal
inputs into languages without training.</p>
<p><strong>Disadvantage</strong>: less flexible, info loss</p>
<p><a
href="https://arxiv.org/pdf/2305.06355.pdf">**VideoChat-Text</a>:**
transforming videos into textual descriptions distorts spatial-temporal
relationships.</p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-18.png" /></p>
<h3 id="evaluation">Evaluation</h3>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-19.png" /></p>
<aside>
<p>üëâ <strong>Closed-set</strong></p>
</aside>
<p><strong>Closed-set questions</strong>: its possible answer options
are predefined and limited</p>
<p><strong>Two evaluation types</strong>:</p>
<ol type="1">
<li><strong>leverage existing datasets</strong>: limited to a small
range of tasks (e.g., acc for VQA, CIDEr score for text2caption). Two
evaluation settings:
<ol type="1">
<li><strong>Zero-shot</strong>: split into held-in/held-out parts</li>
<li><strong>Finetuning</strong>: domain-specific downstream tasks</li>
</ol></li>
<li><strong>develop new benchmarks specially designed for
MLLMs</strong>: more comprehensive
<ol type="1">
<li><a
href="https://arxiv.org/pdf/2306.13394.pdf"><strong>MME</strong></a>: 14
perception and cognition tasks, manually design instruction-answer pairs
to avoid data leakage</li>
<li><strong><a
href="https://arxiv.org/pdf/2306.06687.pdf">LAMM-Benchmark</a></strong>:
various 2D/3D vision tasks</li>
<li><strong><a
href="https://arxiv.org/pdf/2306.05424.pdf">Video-ChatGPT</a></strong>:
(1) video-based generative performance (2) zero-shot QA</li>
</ol></li>
</ol>
<aside>
<p>üëâ <strong>Open-set</strong></p>
</aside>
<p><strong>Open-set questions</strong>: its possible answer options are
open and flexible.</p>
<p>MLLMs act as chatbots and the chat content can be arbitrary.</p>
<ol type="1">
<li><strong>Manual scoring</strong>: require humans to assess specific
dimensions</li>
<li><strong>GPT scoring</strong>: rate with GPT, reduce human labour
(LLaVA)
<ul>
<li>Multimodal interface of GPT-4 is not publicly available. Therefore,
setting GPT-4 as the performance upper bound is not perfect.</li>
</ul></li>
<li><strong>Case study</strong>: (mPLUG-Owl, Video-LLaMA)</li>
</ol>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-20.png" /></p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-21.png" /></p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-22.png" /></p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-23.png" /></p>
<aside>
<p>üëâ <strong>Others</strong></p>
</aside>
<p><strong><a
href="https://arxiv.org/pdf/2212.10773.pdf">MultiInstruct</a></strong>:
sensitivity metric, model‚Äôs robustness to varied instructions</p>
<p><strong><a
href="https://arxiv.org/pdf/2305.10355.pdf">POPE</a></strong>: delve
into the object hallucination problem</p>
<p><strong><a
href="https://arxiv.org/pdf/2305.16934.pdf">AttackVLM</a></strong>:
(safety) the robustness of MLLMs to adversarial attacks</p>
<h2 id="m-icl-multimodal-in-context-learning">M-ICL: Multimodal
In-Context Learning</h2>
<p><strong>ICL</strong>: few-shot, training-free</p>
<p><strong>M-ICL</strong>: Based on M-IT, at inference time, M-ICL adds
a demonstration set (i.e., a set of in-context samples), to the original
sample.</p>
<p><strong>Template example of an M-ICL query</strong>:</p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-24.png" /></p>
<aside>
<p>üëâ <strong>Two Scenarios</strong></p>
</aside>
<p><strong>Scenario 1: solving various visual reasoning
tasks</strong></p>
<p>(<a href="https://arxiv.org/pdf/2303.11381.pdf">**MM-React</a>, <a
href="https://arxiv.org/pdf/2305.03726.pdf">Otter</a>, <a
href="https://arxiv.org/pdf/2204.14198.pdf">Flamingo</a>, <a
href="https://arxiv.org/pdf/2109.05014.pdf">PICa</a>, <a
href="https://arxiv.org/pdf/2106.13884.pdf">Frozen</a>**)</p>
<ul>
<li>Via demonstrations, LLMs get a sense of what the task is doing and
what the output template is.</li>
<li>Thus LLMs learn from a few task-specific examples and generalize to
a new but similar question.</li>
</ul>
<p><strong>Scenario 2: teaching LLMs to use external tools</strong></p>
<p>(<strong><a
href="https://arxiv.org/pdf/2304.09842.pdf">Chameleon</a>, <a
href="https://arxiv.org/pdf/2211.11559.pdf">Visual programming</a>, <a
href="https://arxiv.org/pdf/2303.17580.pdf">HuggingGPT</a></strong>)</p>
<ul>
<li>Demonstrations are often text-only and more fine-grained.</li>
<li>They typically comprise a chain of steps that could be sequentially
executed to fulfill the task. (related to CoT)</li>
</ul>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-25.png" /></p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-26.png" /></p>
<h2 id="m-cot-multimodal-chain-of-thought">M-CoT: Multimodal Chain of
Thought</h2>
<p><strong>CoT</strong>: a series of intermediate reasoning steps</p>
<p><strong>CoT Scenario</strong>: complex reasoning</p>
<p><strong>CoT Main Idea</strong>: output both reasoning process and
final answer</p>
<p><strong>M-CoT</strong> (<strong><a
href="https://arxiv.org/pdf/2201.11903.pdf">CoT-PT</a>, <a
href="https://arxiv.org/pdf/2302.00923.pdf">MM-CoT</a>, <a
href="https://arxiv.org/pdf/2305.02317.pdf">VCoT</a>, <a
href="https://arxiv.org/pdf/2305.13903.pdf">VideoCoT</a></strong>)</p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-27.png" /></p>
<p><strong>Modality Bridging</strong></p>
<ul>
<li><p><strong>Expert Model</strong>: transforming visual input into
textual descriptions (<a
href="https://arxiv.org/pdf/2209.09513.pdf">ScienceQA</a>: concatenate
image captions and original textual imput to LLMs)</p></li>
<li><p><strong>Learnable Interface</strong>: feature fusion (<a
href="https://arxiv.org/pdf/2302.00923.pdf">MM-CoT</a>: adopts a
two-stage framework)</p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-28.png" /></p></li>
</ul>
<p><strong>Learning Paradigms</strong></p>
<ul>
<li><strong>Finetuning</strong>: often for specific datasets(e.g., <a
href="https://arxiv.org/pdf/2209.09513.pdf">ScienceQA</a>)</li>
<li><strong>Few-shot</strong>: requires hand-crafting some in-context
examples</li>
<li><strong>Zero-shot</strong>: require no specific example</li>
</ul>
<p><strong>Chain Config‚Äî‚Äî</strong>(When to stop the chains?)</p>
<ul>
<li><strong>Adaptive</strong>: requires LLMs to decide when to halt the
reasoning chains</li>
<li><strong>Pre-defined</strong>: stops the chains with a pre-defined
length</li>
</ul>
<p><strong>Generation Pattens‚Äî‚Äî</strong>(How the chain is
constructed?)</p>
<p>The generated steps should be consistent and correct.</p>
<ul>
<li><p><strong>Predicting</strong>: extending the reasoning chains given
conditions such as instructions and previous reasoning history</p></li>
<li><p><strong>Infilling</strong>: deducing steps between surrounding
context (previous and following steps) to fill the logical gaps
(VideoCoT)</p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-29.png" /></p></li>
</ul>
<h2 id="lavr-llm-aided-visual-reasoning">LAVR: LLM-Aided Visual
Reasoning</h2>
<p><strong>LAVR</strong>: use external tools or visual foundation
models.</p>
<p><strong>Three advantages</strong> compared with conventional visual
reasoning models: <strong>Strong generalization, Emergent abilities,
Better interactivity and control</strong></p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-30.png" /></p>
<aside>
<p>üëâ <strong>Training Paradigms</strong></p>
</aside>
<p><a href="https://arxiv.org/pdf/2305.18752.pdf">GPT4Tools</a>:
generate a new tool-related instruction dataset</p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-31.png" /></p>
<aside>
<p>üëâ <strong>Functions</strong></p>
</aside>
<p><em>The primary roles that LLMs play in LAVR.</em></p>
<ul>
<li><strong>Controller</strong>: (single-round) breaks down a complex
task [via CoT], assign subtasks to right tools/modules</li>
<li><strong>Decision Maker</strong>: (multi-round) summarize history and
current info, decide whether the info is sufficient to finish the task,
friendly present answer</li>
<li><strong>Semantics Refiner</strong>: integrate info into fluent
scentences, generate texts according to different specific needs</li>
</ul>
<h2 id="challenges-and-future-directions">Challenges and Future
Directions</h2>
<ol type="1">
<li><p><strong>Perception Capabilities</strong>: compromise between info
capacity and computation burden</p></li>
<li><p><strong>Consistent Reasoning Chain</strong>: right reasoning
chain delivers right answer</p></li>
<li><p><strong>Instruction-following Abilities</strong>: (e.g., yes/no
type)</p></li>
<li><p><strong>Object Hallucination</strong>: more fine-grained modality
alignment</p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-32.png" /></p>
<p>(The object does not appear in the image)</p></li>
<li><p><strong>Parameter-Efficient Training</strong></p></li>
</ol>
]]></content>
      <categories>
        <category>Paper</category>
        <category>2023</category>
        <category>06</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Multi-modal</tag>
        <tag>Instruction-Following</tag>
        <tag>In-Context Learning</tag>
        <tag>Chain of Thought</tag>
        <tag>Tool Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>MIMIC-IT: Multi-Modal In-Context Instruction Tuning</title>
    <url>/2023/07/04/MIMIC-IT/</url>
    <content><![CDATA[<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-0.png" /></p>
<p>MIMIC-IT Paper: <a
href="https://arxiv.org/pdf/2306.05425.pdf">https://arxiv.org/pdf/2306.05425.pdf</a></p>
<p>Otter Paper: <a
href="https://arxiv.org/pdf/2305.03726.pdf">https://arxiv.org/pdf/2305.03726.pdf</a></p>
<p>Code: <a
href="https://github.com/Luodian/Otter">https://github.com/Luodian/Otter</a></p>
<p>Project Page: <a
href="https://otter-ntu.github.io/">https://otter-ntu.github.io/</a></p>
<aside>
<p>üëá Problems</p>
</aside>
<p>Current VL instruction-response pairs in terms of quantity, diversity
and creativity are limited, posing challenges to the generalization of
interactive VLMs.</p>
<aside>
<p>üëá Contributions</p>
</aside>
<ul>
<li><strong>MIMIC-IT</strong>: dataset with 2.8M instruction-response
pairs</li>
<li><strong>Syphus</strong>: automatic instruction-response generation
pipeline &gt; Its filtering does not consider whether the visual info
and textual info are paired or not. &gt;</li>
<li><strong>Otter</strong>: improve OpenFlamingo in perception,
reasoning, and planning via finetuning it on MIMIC-IT</li>
</ul>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT%20Overview.jpg"
alt="MIMIC-IT overview. Each instruction is accompanied by multi-modal conversational context." />
<figcaption aria-hidden="true">MIMIC-IT overview. Each instruction is
accompanied by multi-modal conversational context.</figcaption>
</figure>
<h1 id="related-workmulti-modal-instruction-tuning-dataset">1 - Related
Work(Multi-modal Instruction Tuning Dataset)</h1>
<blockquote>
<p>One potential reason for the zero-shot performance gain by
instruction tuning is that it internalizes the <a
href="https://arxiv.org/pdf/2209.15189.pdf">context</a>, which is
preferred in user interactions especially when user input skips
commonsense context.</p>
</blockquote>
<p><a href="https://arxiv.org/pdf/2212.10773.pdf">Multi-Instruct</a>:
Initially introduces the notion of instruction tuning in multi-modal
models</p>
<p><a href="https://arxiv.org/pdf/2304.10592.pdf">Mini-GPT4</a>: creates
its instruction-based dataset by merging Conceptual Caption, SBU, and
LAION with handwritten instruction templates.</p>
<aside>
<p>üëá <a
href="https://arxiv.org/pdf/2304.08485.pdf">LLaVA-Instruct-150K</a>
elevates the quality of VL instruction-following datasets.</p>
</aside>
<p>Utilizing self-instruct and handwritten seed instructions,
<strong>LLaVA-Instruct-150K</strong> obtains instructions and responses
from GPT-4 based on COCO image captions and object bounding boxes.</p>
<p><strong><em>Its three limitations:</em></strong></p>
<ol type="1">
<li><strong>Limited visual diversity</strong>: only COCO images.</li>
<li><strong>Single image as visual data</strong>: cannot process
multiple images or videos.</li>
<li><strong>Language-only in-context information</strong>: no
multi-modal in-context info</li>
</ol>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-1.png"
alt="LLaVA-Instruct-150K: Language-only In-context." />
<figcaption aria-hidden="true">LLaVA-Instruct-150K: Language-only
In-context.</figcaption>
</figure>
<aside>
<p>üëá Three characteristics of MIMIC-IT</p>
</aside>
<blockquote>
<p>While these previous works focuse on general scene images, MIMIC-IT
categorizes our data sources into indoor scenes, outdoor scenes,
conversations, and egocentric videos.</p>
</blockquote>
<ol type="1">
<li><strong>Diverse visual scenes:</strong> incorporating images and
videos from general scenes, egocentric view scenes, and indoor RGB-D
images.</li>
<li><strong>Multiple images (or a video) as visual data</strong>:
supporting instruction-response pairs accompanied by any number of
images or videos.</li>
<li><strong>Multi-modal in-context information</strong>: including
multiple instruction-response pairs and multiple images or videos</li>
</ol>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-2.png"
alt="MIMIC-IT: Multi-modal In-context. Left part is in-context, right part is the query." />
<figcaption aria-hidden="true">MIMIC-IT: Multi-modal In-context. Left
part is in-context, right part is the query.</figcaption>
</figure>
<h1 id="multi-modal-in-context-instruction-tuning-dataset">2 -
Multi-modal In-Context Instruction Tuning Dataset</h1>
<h2 id="mimic-it-data-format">2.1 - MIMIC-IT Data Format</h2>
<p><span class="math inline">\(I_q\)</span> denotes the <span
class="math inline">\(q\)</span>-th instruction, <span
class="math inline">\(R_q\)</span> is the response, <span
class="math inline">\(X_q\)</span> refers to images or videos.</p>
<blockquote>
<p>Videos can be viewed as ordered sequences of images.</p>
</blockquote>
<p>A <strong>query example</strong> is a tuple <span
class="math inline">\((I_q, R_q, X_q)\)</span>, where <span
class="math inline">\(\{x_{j=1}^N\}\in X_q\)</span>, <span
class="math inline">\(N\)</span> is image num.</p>
<aside>
<p>üëÜ <span class="math inline">\(p_{\theta}(R_q|(I_q,X_q))\)</span> is
the <strong>standard instruction-tuning process</strong> with trainable
params <span class="math inline">\(\theta\)</span>.</p>
</aside>
<p><span
class="math inline">\(C_{\psi}:(I_q,X_q)\mapsto\{(I_k,X_k)\}_{k=1}^M\)</span>
are <span class="math inline">\(M\)</span> <strong>in-context
examples</strong> of current query example.(The function is
task-dependent.)</p>
<p><strong>MIMIC-IT Data Format</strong>: <span
class="math inline">\(d_q=(I_q,R_q,X_q,C_{\psi}(I_q,X_q)),d_q\sim
D_{MIMIC-IT}\)</span>.</p>
<aside>
<p>üëÜ <span
class="math inline">\(p_{\theta}(R_q|(I_q,X_q,C_{\psi}(I_q,X_q)))\)</span>
incorporates in-context examples.</p>
</aside>
<h2 id="sythus-automatic-instruction-response-generation-pipeline">2.2 -
Sythus: Automatic Instruction-Response Generation Pipeline</h2>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-3.png"
alt="Sythus overview. Step 4 expands pairs into 8 languages via GPT‚Äî‚Äî Chinese (zh), Japanese (ja), Spanish (es), German (de), French (fr), Korean (ko), and Arabic (ar)." />
<figcaption aria-hidden="true">Sythus overview. Step 4 expands pairs
into 8 languages via GPT‚Äî‚Äî Chinese (zh), Japanese (ja), Spanish (es),
German (de), French (fr), Korean (ko), and Arabic (ar).</figcaption>
</figure>
<p><strong>System messages</strong> define the desired tone and style of
the generated instruction-response(IR) pairs.</p>
<p><strong>Visual annotations</strong> provide essential image
information such as bounding boxes, captions, timestamps.</p>
<p><strong>In-context examples</strong> assist ChatGPT in learning
within the context.</p>
<p><strong>Cold-start stage</strong> is before the large-scale query.
This stage identifies the optimal system message and in-context example
for each specific task via prompting ChatGPT.</p>
<p><strong>Safety and Ethical Filtering</strong>: The GPT content policy
eliminates output that is suspicious for unfair opportunities,
stereotyping, overrepresentation/underrepresentation, explicit content,
disinformation, or unreliable information.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-4.png"
alt="System message for TV show Captions (TVC) query." />
<figcaption aria-hidden="true">System message for TV show Captions (TVC)
query.</figcaption>
</figure>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-5.png"
alt="In-context exemplars for TV show Captions (TVC) query." />
<figcaption aria-hidden="true">In-context exemplars for TV show Captions
(TVC) query.</figcaption>
</figure>
<h2 id="visual-data-exploration">2.3 - Visual Data Exploration</h2>
<p><strong>Why we leverage existing datasets?</strong></p>
<ol type="1">
<li>high-quality visual annotations (2) align with real-world
distribution</li>
</ol>
<blockquote>
<p>In each sub-task, we elaborate on the process of organizing various
data into an in-context instruction tuning format.</p>
</blockquote>
<h3 id="general-scene-understanding">2.3.1 - General Scene
Understanding</h3>
<aside>
<p>üëá <strong>LLaVA-Interleaved (LA-I)</strong>.</p>
</aside>
<p>Retrieve ten in-context examples for each instruction-response pair
in <a
href="https://arxiv.org/pdf/2304.08485.pdf">LLaVA-Instruct-150K</a>
based on instruction text2text similarity(<strong>TTS</strong>) or
image2image similarity(<strong>IIS</strong>).</p>
<p>Trained on the LA task, the model exhibits exceptional scene
comprehension, reasoning abilities, and multi-round conversation
capabilities.</p>
<aside>
<p>üëá <strong>Spot The Difference (SD)</strong>. Identify scene
differences between the paired images with varying complexity
levels.</p>
</aside>
<ul>
<li><strong>General Scene Difference</strong>: (1) create image pairs
from the <a href="https://arxiv.org/pdf/1405.0312.pdf">COCO2017</a> via
<strong>IIS</strong> (2) use image captions and object detection
annotations</li>
<li><strong>Subtle Difference</strong>: (1) image pairs from <a
href="https://arxiv.org/pdf/1808.10584.pdf">Spot-the-Diff</a>(extracted
from surveillance footage) (2) employ natural language difference
descriptions as annotations</li>
</ul>
<aside>
<p>üëá <strong>Visual Story Telling (VIST)</strong>. Generate coherent
and engaging narratives based on visual input.</p>
</aside>
<p><a href="https://arxiv.org/pdf/1604.03968.pdf">Visual
Storytelling</a> includes event-based image sequences and corresponding
inquiry questions.</p>
<p>Given that image annotations often contain narratives and timelines
not directly observable, we instruct ChatGPT to act as a viewer
answering questions about the images.</p>
<p>The prompts also incorporate thought-provoking inquiries to promote
creativity.</p>
<aside>
<p>üëá <strong>Dense Captions (DC)</strong>.</p>
</aside>
<p>Expanding the scope of video understanding, DC features dense
captions from <a
href="https://arxiv.org/pdf/1705.00754.pdf">DenseCaption/Activity
caption</a> corresponding to clips within longer videos. The
instructions pose a diverse set of questions, addressing the general
visual content of the video, human actions, and behaviors, the
chronological sequence of events, and causal relationships. This
approach encourages VLMs to delve deeper into the intricacies of video
content.</p>
<aside>
<p>üëá <strong>TV Show Captions (TVC)</strong>.</p>
</aside>
<p>The primary purpose of incorporating TV show clips with high-level
captions into the training process of VLMs is to enhance their social
reasoning abilities and deepen their understanding of complex character
dynamics. By organizing drama clips from <a
href="https://arxiv.org/pdf/2001.09099.pdf">TVR</a> to analyze character
relationships and motivations, we aim to challenge VLMs to move beyond
mere perception and demonstrate their reasoning capabilities within the
context of TV show narratives. This focused approach is crucial for
fostering advanced VLMs capable of effectively handling diverse
real-world situations and user queries.</p>
<h3 id="egocentric-view-understanding">2.3.2 - Egocentric View
Understanding</h3>
<aside>
<p>üëá <strong>Indoor Event Planning (IEP)</strong>. Planning
capabilities for diverse 2D room photos.</p>
</aside>
<p><strong>2D Photos Sampling</strong>: We gather indoor scene RGB-D
images from <a href="https://arxiv.org/pdf/1702.04405.pdf">ScanNetv2</a>
and sample them into multiple 2D visual inputs, representing a room‚Äôs
layout from a first-person perspective.</p>
<p><strong>Instructions Generation</strong> via prompting ChatGPT:
direct humans to perform various activities in indoor spaces.</p>
<ul>
<li>Initially, we have ChatGPT create a personality for the room
owner.</li>
<li>Subsequently, the planning should be intimately related to the
room‚Äôs layout and the generated room owner, underlining the importance
of context awareness in VLMs.</li>
</ul>
<aside>
<p>üëá <a href="https://arxiv.org/pdf/2110.07058.pdf">Ego4D (E4D,
videos)</a>. For first-person augmented reality (AR) headset assistant
applications.</p>
</aside>
<p>By prompting ChatGPT to generate instructions based on visual
descriptions, our goal is to simulate practical interactions between
users and AR assistants.</p>
<p>An instance of assistant-related questions and context-aware
responses:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Instruction: What should I do now?</span><br><span class="line">Response: Based on my observation, you can now proceed to do....</span><br></pre></td></tr></table></figure>
<h2 id="dataset-statistics">2.4 - Dataset Statistics</h2>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-6.png"
alt="Comparison between MIMIC-IT and other multi-modal instruction datasets. MIMICIT features: (1) The largest. (2) Including video data. (3) In-context scenarios. (4) Multilingual." />
<figcaption aria-hidden="true">Comparison between MIMIC-IT and other
multi-modal instruction datasets. MIMICIT features: (1) The largest. (2)
Including video data. (3) In-context scenarios. (4)
Multilingual.</figcaption>
</figure>
<p><strong>2.8M instruction-response pairs</strong>(2.2M unique
instructions): each pair includes at least one multi-modal in-context
example and one language-only in-context example.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-7.png"
alt="The data statistics of MIMIC-IT. (c) retains 25% of Ego4D instructions for a more balanced distribution." />
<figcaption aria-hidden="true">The data statistics of MIMIC-IT. (c)
retains 25% of Ego4D instructions for a more balanced
distribution.</figcaption>
</figure>
<ol type="a">
<li><p>and (b) examine the characteristics and diversity using <a
href="https://github.com/explosion/spacy-models/releases/tag/en_core_web_md-3.5.0">spaCy</a>
to get top 20 most frequent root verbs alongside their top 4 direct noun
objects.</p></li>
<li><p>demonstrates diversity in terms of instructions/responses length,
image num per instruction, and in-context examples num per
instruction.</p></li>
</ol>
<h1 id="empricial-evaluation">3 - Empricial Evaluation</h1>
<h2 id="otter-a-multi-modal-in-context-instruction-tuned-model">3.1 -
Otter: A Multi-Modal In-Context Instruction Tuned Model</h2>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Otter.png" /></p>
<p><strong>Language Encoder</strong>: LLaMA-7B(frozen)</p>
<p><strong>Vision Encoder</strong>: CLIP ViT-L/14(frozen)</p>
<p><strong>Perceiver Resampler Module</strong>: ~1.3B trainable
parameters</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># [] indicates special word, &lt;&gt; indicates data slot</span><br><span class="line"># role label: User/GPT</span><br><span class="line"># format the training data as follows(chatbot-like format):</span><br><span class="line">&lt;context&gt; [image] User:&lt;instruction&gt; GPT:[answers] &lt;answer&gt;.[endofchunk]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>To support user-assistant conversations, we adopt "GPT" as the role
label because it does not have any specific semantic meaning in
vocabulary.</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Hyperparameters</span><br><span class="line">optimizer: AdamW</span><br><span class="line">starting learning rate(lr): 10^&#123;-5&#125;</span><br><span class="line">batch size: 4</span><br><span class="line">epoch: 6</span><br><span class="line">lr scheduler: cosine annealing scheduler</span><br><span class="line">gradient clipping threshold: 1.0</span><br><span class="line">loss: cross-entropy</span><br></pre></td></tr></table></figure>
<aside>
<p>üëá HuggingFace</p>
</aside>
<ul>
<li><a
href="https://huggingface.co/luodian/OTTER-Image-LLaMA7B-LA-InContext">OTTER-Image-LLaMA7B-LA-InContext</a>:
as described in paper</li>
<li><a
href="https://huggingface.co/luodian/OTTER-Image-MPT7B">OTTER-Image-MPT7B</a>:
Tune OpenFlamingv2 to enable generation abilities for both long and
short answers.</li>
<li><a
href="https://huggingface.co/luodian/OTTER-Video-LLaMA7B-DenseCaption">OTTER-Video-LLaMA7B-DenseCaption</a></li>
<li><a
href="https://huggingface.co/luodian/OTTER-Video-LLaMA7B-FunQA">OTTER-Video-LLaMA7B-FunQA</a></li>
</ul>
<blockquote>
<p><strong>Otter-Image</strong>¬†supports multiple images input as
in-context examples, which is¬†the first multi-modal instruction tuned
model¬†that supports to organize inputs this way.</p>
</blockquote>
<blockquote>
<p><strong>Otter-Video</strong>¬†supports videos inputs (frames are
arranged as original Flamingo's implementation) and multiple images
inputs (they serve as in-context examples for each other).</p>
</blockquote>
<aside>
<p>üëá Engineering Work</p>
</aside>
<p>We have <a
href="https://huggingface.co/luodian/OTTER-9B-INIT">integrated Otter
into Hugging Face Transformers</a> and trained it using the <a
href="https://huggingface.co/docs/accelerate/index">Hugging Face
Accelerator</a>.</p>
<p>We use <strong>bf16 mixed precision</strong> and train Otter on
<strong>4√óRTX-3090 GPUs(24GB)</strong>.</p>
<blockquote>
<p>reduced from 1√ó A100 GPU</p>
</blockquote>
<p>We provide the support of Fully Sharded Data Parallel (FSDP) and
DeepSpeed.</p>
<p>We provide a <strong>script for converting</strong> the original
OpenFlamingo-9B checkpoint into the Hugging Face Model format.(<a
href="https://huggingface.co/luodian/openflamingo-9b-hf">luodian/openflamingo-9b-hf</a>)</p>
<h2 id="usage-examples-and-demonstrations">3.2 - Usage Examples and
Demonstrations</h2>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Otter-example.png"
alt="Otter‚Äôs response examples in different scenarios. Otter is able to serve for situation understanding and reasoning, learning with in-context examples, and egocentric visual assistant." />
<figcaption aria-hidden="true"><strong>Otter‚Äôs response examples in
different scenarios</strong>. Otter is able to serve for situation
understanding and reasoning, learning with in-context examples, and
egocentric visual assistant.</figcaption>
</figure>
<p><strong>Otter‚Äôs response examples in different scenarios</strong>.
Otter is able to serve for situation understanding and reasoning,
learning with in-context examples, and egocentric visual assistant.</p>
<p><strong>Scene Understanding and Reasoning</strong>.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;image&gt;Human:&#123;instruction&#125; Assistant:&lt;answer&gt;&#123;model-generated response&#125;&lt;endofchunk&gt;</span><br></pre></td></tr></table></figure>
<p><strong>Learning with In-context Examples</strong>.</p>
<p>The organized input data format(on the LA-T2T task) is as
follows:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Multiple in-context example with similar instructions</span><br><span class="line">&lt;image&gt;Human:&#123;instruction&#125; Assistant:&lt;answer&gt;&#123;response&#125;&lt;|endofchunk|&gt;</span><br><span class="line"># ....</span><br><span class="line">&lt;image&gt;Human:&#123;instruction&#125; Assistant:&lt;answer&gt;&#123;response&#125;&lt;|endofchunk|&gt;</span><br><span class="line"># Query example</span><br><span class="line">&lt;image&gt;Human:&#123;instruction&#125; Assistant:&lt;answer&gt;</span><br></pre></td></tr></table></figure>
<p><strong>Egocentric Visual Assistant(Otter-E)</strong>.</p>
<p>Otter-E is specifically designed for AR headset applications.</p>
<p>In real-life scenarios, you are not encouraged to consult visual
assistants for such hazardous actions.</p>
<h2 id="chatgpt-evaluation">3.3 - ChatGPT Evaluation</h2>
<blockquote>
<p>Current evaluation metrics for VL models, like VQAv2 acc, exhibit
shortcomings in terms of robustness.</p>
</blockquote>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-8.png"
alt="MMAGIBench evaluation results judged by ChatGPT." />
<figcaption aria-hidden="true"><strong><a
href="https://github.com/open-mmlab/mmagibench">MMAGIBench</a>
evaluation results</strong> judged by ChatGPT.</figcaption>
</figure>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-9.png"
alt="(a) ChatGPT evaluation. (b) Human evaluation comparison. (c) Few-shot in-context learning evaluation on COCO caption (CIDEr)." />
<figcaption aria-hidden="true">(a) ChatGPT evaluation. (b) Human
evaluation comparison. (c) Few-shot in-context learning evaluation on <a
href="https://arxiv.org/pdf/1405.0312.pdf">COCO caption
(CIDEr)</a>.</figcaption>
</figure>
<h2 id="human-evaluation">3.4 - Human Evaluation</h2>
<p><a
href="https://github.com/OpenGVLab/Multi-Modality-Arena">Multi-Modality
Arena</a> uses an Elo rating(higher is better) system to evaluate
<strong>the usefulness and alignment</strong> of VLM responses.</p>
<p>The system calculates the relative skill levels of players.</p>
<p>This system works well for evaluating conversational AI models,
because multiple models can have pairwise "battles" responding to the
same inputs in a user-blind evaluation.</p>
<h2 id="few-shot-in-context-learning-metric-evaluation">3.5 - Few-shot
In-Context Learning Metric Evaluation</h2>
<p>As expected, the finetuning also brings marginal performance gain on
zero-shot evaluation.</p>
<h1 id="conclusion">4 - Conclusion</h1>
<aside>
<p>üëá <strong>Limitations</strong></p>
</aside>
<p>For MIMIC-IT:</p>
<p>ChatGPT is prone to language hallucinations, therefore it might
generate incorrect responses.</p>
<p>For Otter:</p>
<ul>
<li><strong>Object hallucinations</strong>(objects mentioned in text
inputs do not appeare in images or videos)</li>
<li><strong>Language hallucinations</strong> from OpenFlamingo is
inherited by Otter.</li>
</ul>
<aside>
<p>üëá <strong>Future Works</strong></p>
</aside>
<p>For MIMIC-IT:</p>
<ul>
<li>More trustworthy LMs or generation techniques for self-instruct data
generation.</li>
<li>More embodied AI datasets such as <a
href="https://arxiv.org/pdf/2210.06407.pdf">LanguageTable</a> and <a
href="https://arxiv.org/pdf/2204.01691.pdf">SayCan</a>.</li>
</ul>
<p>For Otter:</p>
<ul>
<li>introducing negative examples in the training data to reduce
hallucination issue</li>
<li>more efficient training schemas(e.g., LoRA)</li>
<li>more modalities.</li>
</ul>
]]></content>
      <categories>
        <category>Paper</category>
        <category>2023</category>
        <category>06</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Multi-modal</tag>
        <tag>Dataset</tag>
        <tag>Instrcution-Following</tag>
        <tag>In-Context Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Shikra: Unleashing Multimodal LLM‚Äôs Referential Dialogue Magic</title>
    <url>/2023/07/09/Shikra/</url>
    <content><![CDATA[<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-0.png" /></p>
<p>Paper: <a
href="https://arxiv.org/pdf/2306.15195.pdf">https://arxiv.org/pdf/2306.15195.pdf</a></p>
<p>Code: <a
href="https://github.com/shikras/shikra">https://github.com/shikras/shikra</a></p>
<aside>
<p>üëá Problems</p>
</aside>
<p>In human conversations, individuals can indicate relevant regions
within a scene while addressing others.</p>
<p>This natural referential ability in dialogue remains absent in
current MLLMs.</p>
<aside>
<p>üëá Contributions</p>
</aside>
<ul>
<li><p>Study referential ability in existing MLLMs.</p></li>
<li><p>Define new task‚Äî‚ÄîReferential dialogue(RD)</p>
<p>Potential applications of this ability</p>
<ul>
<li>aiding AI assistants in Mixed Reality headsets</li>
<li>facilitating precise communication in online shopping scenery</li>
</ul></li>
<li><p>Propose Shikra</p>
<ul>
<li>Shikra can comprehend and output <strong>spatial
coordinates</strong> in natural language.</li>
<li>With a <strong>simple architecture</strong>, Shikra shows no need
for extra vocabularies, position encoder, pre-/post-detection modules,
or external plugin models.</li>
<li>Shikra enables numerous <strong>exciting applications</strong>, like
providing mentioned objects‚Äô coordinates in chains of thoughts and
comparing user-pointed regions similarities.</li>
<li>It achieves <strong>promising performance</strong> on conventional
VL tasks w/o finetuning.</li>
</ul></li>
</ul>
<h1 id="referential-dialogue">1 - Referential Dialogue</h1>
<ul>
<li><strong>Current MLLM Can</strong>: VQA, Image Caption, and
multimodal dialogue</li>
<li><strong>Current MLLM Cannot</strong>: REC, REG(referring expression
generation), and PointQA</li>
</ul>
<p><strong>Shikra can do them all.</strong></p>
<p>The model demonstrates proficiency in tasks not in the training set,
such as identifying similarities between two indicated objects, or
counting somethings, and providing their positions.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-1.png"
alt="Demo of Referential Dialogue (RD). Users can point to specific areas and ask questions. In turn, Shikra will indicate the specific regions when replying, if necessary." />
<figcaption aria-hidden="true">Demo of Referential Dialogue (RD). Users
can point to specific areas and ask questions. In turn, Shikra will
indicate the specific regions when replying, if necessary.</figcaption>
</figure>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-2.png"
alt="Referential Dialogues between real users and Shikra-7B." />
<figcaption aria-hidden="true">Referential Dialogues between real users
and Shikra-7B.</figcaption>
</figure>
<h1 id="chessboard-test-for-current-mllm">2 - Chessboard Test for
Current MLLM</h1>
<blockquote>
<p>The current MLLMs cannot directly output coordinates. We need to
explore appropriate coordinate representations and finer-grained
training data.</p>
</blockquote>
<p><strong>Chessboard Test</strong>:</p>
<p>Simplifies the object grounding into a part choice task:</p>
<ol type="1">
<li><p>Divide a image into a 2 √ó 2 chessboard.</p></li>
<li><p>Next, ask:</p>
<p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># &lt;image&gt;: image tokens, &lt;expr&gt; class name</span><br><span class="line">‚Äú&lt;image&gt; Which part is &lt;expr&gt; in if the picture is divided equally into four 2 by 2 parts? Choose from: (A) Top-left (B) Top-right (C) Bottom-left (D) Bottom-right.‚Äù</span><br></pre></td></tr></table></figure></p></li>
</ol>
<p><strong>Test Data Construction</strong>:</p>
<ul>
<li>Based on <a href="https://arxiv.org/pdf/1908.03195.pdf">LVIS</a>,
which is a perception detection with over 1000 entry-level object
categories.</li>
<li>We choose objects that are completely within a certain part (i.e.,
ambiguous positions are not considered).</li>
<li>Select 600 images per part, resulting in 2,400 images across 945
categories.</li>
</ul>
<p><strong>Tested Model</strong>: <a
href="https://arxiv.org/pdf/2304.08485.pdf">LLaVA-13B</a></p>
<p><strong>Test Results</strong>: 25.96% acc, comparable to random
selection.</p>
<h1 id="breeding-shikra">3 - Breeding Shikra</h1>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">User Question: </span><br><span class="line">‚ÄúHow many other clothes in the &lt;image&gt; are of the same color as the jacket [0.268, 0.372]?‚Äù. </span><br><span class="line">Shikra reply: </span><br><span class="line">‚ÄúThe jacket [0.268, 0.372] is green. We can find a T-shirt [0.653, 0.532] and cropped pants [0.569, 0.101] a with same green color. So the answer is two.‚Äù</span><br></pre></td></tr></table></figure>
<h2 id="architecture">3.1 - Architecture</h2>
<p><strong>Frozen Visual Encoder</strong>: pre-trained ViT-L/14 CLIP,
its output embedding denoted as <span
class="math inline">\(\mathbf{V}\in\mathbb{R}^{16\times16\times1024}\)</span></p>
<p><strong>Trainable LLM</strong>: Vicuna-7/13B</p>
<p><strong>One Trainable fully connected layer</strong>: map <span
class="math inline">\(\mathbf{V}\)</span> to <span
class="math inline">\(\mathbf{V&#39;}\in\mathbb{R}^{256\times
D}\)</span>, <span class="math inline">\(D\)</span> is 4096 for
Vicuna-7B and 5120 for 13B</p>
<p>Visual embedding can be inserted into anywhere of input sequence.</p>
<p>We do not introduce any vocabulary or special encoder for encoding
position information.</p>
<p>We have not introduced additional pre-/post-detectors for points or
bounding boxes.</p>
<h2 id="numerical-representation-of-position">3.2 - Numerical
representation of position</h2>
<p>x, y is normalized(3 decimal places) according to image size.</p>
<p><strong>Bounding box</strong>: <span class="math inline">\([x_{min},
y_{min},x_{max}, y_{max}]\)</span></p>
<p><strong>Region center point</strong>: <span
class="math inline">\([x_{center}, y_{center}]\)</span></p>
<p>The coordinates can appear anywhere in the input and output sequence
of the model.</p>
<p>Like regular text, tokenizing without discrimination.</p>
<h2 id="instruction-data-construction">3.3 - Instruction data
construction</h2>
<h3 id="reorganization-of-public-data">3.3.1 - Reorganization of public
data</h3>
<p><strong>Spotting Captioning Definition</strong>: describe the image
and spots the mentioned objects or regions using points or boxes. We use
Flickr30K Entities for this task.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-3.png"
alt="All training data used by Shikra. The asterisk indicates that this data is only used in the second stage." />
<figcaption aria-hidden="true">All training data used by Shikra. The
asterisk indicates that this data is only used in the second
stage.</figcaption>
</figure>
<p>We have <strong>excluded images present in the test and validation
data</strong> from the training data to prevent potential data leakage,
despite their distinction in terms of image-text pairs.</p>
<h3 id="generated-data">3.3.2 - Generated Data</h3>
<blockquote>
<p>Data in 3.3.1 lack CoT data with <strong>positional
annotations</strong>, natural communication data with <strong>positional
annotations</strong>, etc.</p>
</blockquote>
<p><strong>Flickr30K Entities:</strong> has five descriptions for each
image. These mentioned objects appearing in the image will be labeled
using bounding box.</p>
<p><strong>Shikra-RD</strong>: High-quality RD data built from Flickr30K
Entities using GPT-4.</p>
<p><strong>Shikra-RD</strong> contains 5,922 QA pairs with
<strong>positional annotations</strong> and will continues expanding in
the future.</p>
<p><strong>GPT-4 Usage</strong>: We explained the format of the bounding
boxes to GPT-4 and asked it to understand the image through these five
sentences and boxes. Next, we require GPT-4 to design Q&amp;A pairs.</p>
<h3 id="task-prompts">3.3.3 - Task prompts</h3>
<ol type="1">
<li>Write <strong>a sample template</strong> of a specific task</li>
<li>Have GPT-4 rewrite it in rich language, expanding it into
<strong>hundreds of variations</strong>.</li>
</ol>
<p><strong>During training</strong>, we can randomly choose from
them.</p>
<p><strong>During inference</strong>, users can describe their needs in
their own format.</p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-4.png" /></p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-5.png"
alt="Examples of task templates used by Shikra on different types of training data.  represents visual tokens,  denotes coordinates of region center points,  is the expression in REC." />
<figcaption aria-hidden="true">Examples of task templates used by Shikra
on different types of training data. <image> represents visual tokens,
<objs> denotes coordinates of region center points, <expr> is the
expression in REC.</figcaption>
</figure>
<h2 id="tuning-details">3.4 - Tuning details</h2>
<ul>
<li>First stage: train Shikra on the reorganized VL dataset for 100,000
steps (around 1.5 epoch)</li>
<li>Second stage: raise the sampling ratio to 50% on LLaVA-Instruct-150K
and Shikra-RD</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">optimizer: AdamW</span><br><span class="line">learning rate scheduler: cosine annealing scheduler</span><br><span class="line">initial learning rate: 2e-5</span><br><span class="line">global batch size: 64</span><br><span class="line">device: 8 NVIDIA A100 GPUs</span><br><span class="line">time cost: ~100h for first stage, ~20h for second stage</span><br></pre></td></tr></table></figure>
<h1 id="experiment-and-analysis">4 - Experiment and Analysis</h1>
<h2 id="grounding-cot-or-verbal-cot">4.1 - Grounding CoT or verbal
CoT?</h2>
<aside>
<p>üí° Investigate whether <strong>Grounding CoT</strong>(GCoT, CoT with
position annotations) can reduce visual hallucinations and improve model
performance.</p>
</aside>
<p>We train three toy models of Shikra-7B (without using additional
datasets) on the CLEVR dataset.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-6.png"
alt="Comparing different forms of CoTs. Q, A, C, and C^{Point} denote the Question, final Answer, Chain of thoughts, and Chain of thoughts with center points [x_{center},y_{center}]." />
<figcaption aria-hidden="true">Comparing different forms of CoTs. <span
class="math inline">\(Q, A, C\)</span>, and <span
class="math inline">\(C^{Point}\)</span> denote the Question, final
Answer, Chain of thoughts, and Chain of thoughts with center points
<span
class="math inline">\([x_{center},y_{center}]\)</span>.</figcaption>
</figure>
<h2 id="location-tokens-or-just-numbers">4.2 - Location tokens or just
numbers?</h2>
<p>We train two toy Shikra using two different representations with REC
data.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-7.png"
alt="Comparing different position representations. Vocab., e.g., , ¬∑ ¬∑ ¬∑ , . Numerical directly uses numbers." />
<figcaption aria-hidden="true">Comparing different position
representations. Vocab., e.g., <bin_0>, ¬∑ ¬∑ ¬∑ , <bin_1000>. Numerical
directly uses numbers.</figcaption>
</figure>
<p><strong>Numerical representation Advantages</strong>:</p>
<ul>
<li>better performance</li>
<li>does not retrain vocabularies for localization tasks</li>
<li>freely control the precision</li>
</ul>
<p><strong>Numerical representation Disadvantages</strong>:</p>
<ul>
<li>longer token length brings more computational costs</li>
</ul>
<h2 id="quantitative-results-on-conventional-tasks">4.3 - Quantitative
results on conventional tasks</h2>
<aside>
<p>üëá <strong>REC task</strong>: ground the object described with an
expression.</p>
</aside>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-8.png"
alt="Results on standard REC task. OFA-L* refers to the OFA-Large checkpoint without finetuning. GRIT refexp is the ablation split." />
<figcaption aria-hidden="true">Results on standard REC task. OFA-L*
refers to the OFA-Large checkpoint without finetuning. GRIT refexp is
the ablation split.</figcaption>
</figure>
<aside>
<p>üëá <strong>PointQA task</strong></p>
</aside>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-9.png"
alt="Comparing pointQA capabilities on the Visual-7W. Accuracy (%) is used for evaluation." />
<figcaption aria-hidden="true">Comparing pointQA capabilities on the
Visual-7W. Accuracy (%) is used for evaluation.</figcaption>
</figure>
<p><strong>Visual-7W</strong>: given a question and four box options,
model should choose one as the answer.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-10.png"
alt="Comparing pointQA capabilities on the LookTwice-QA. We use Shikra-13B and Accuracy (%) for evaluation." />
<figcaption aria-hidden="true">Comparing pointQA capabilities on the
LookTwice-QA. We use Shikra-13B and Accuracy (%) for
evaluation.</figcaption>
</figure>
<p><strong>LookTwice-QA</strong>: answer question based on the input
point/box.</p>
<p>[**Pronoun/Superclass/Class]** indicate different levels of
referential clarity in the question, e.g., ‚ÄúHow many of these
<strong>[‚àÖ/fruits/apples]</strong> <obj>?"</p>
<p><strong><obj></strong> denotes the coordinates of input point/box</p>
<aside>
<p>üëá <strong>conventional VL tasks</strong></p>
</aside>
<blockquote>
<p>It‚Äôs worth noting that these task configurations are just some
subsets of Referential Dialogue.</p>
</blockquote>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-11.png"
alt="Comparing generalist models on VQA and Image Captioning. Use Shikra-13B. FM: Flamingo. Acc for VQA, CIDEr for Caption." />
<figcaption aria-hidden="true">Comparing generalist models on VQA and
Image Captioning. Use Shikra-13B. FM: Flamingo. Acc for VQA, CIDEr for
Caption.</figcaption>
</figure>
<p>We also provide <span class="math inline">\(VQAv2^{val}\)</span>
(83.3) and OK-VQA (53.8) results on LVLM-eHub toolbox for easy
comparison.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-12.png"
alt="Object hallucination benchmark using POPE evaluation pipeline." />
<figcaption aria-hidden="true">Object hallucination benchmark using POPE
evaluation pipeline.</figcaption>
</figure>
<p><strong>Accuracy</strong> denotes the accuracy of predictions.</p>
<p><strong>Precision</strong> signifies the true positive samples among
the predicted positives.</p>
<p><strong>Recall</strong> indicates the correct identification of all
true positive samples.</p>
<p><strong>‚ÄúYes‚Äù</strong> represents the probability of the model
outputting a positive answer.</p>
<h1 id="related-work">5 - Related Work</h1>
<h2 id="vision-language-positioning-tasks">5.1 - Vision-Language
Positioning Tasks</h2>
<p>Many vision-language tasks require localization representation.</p>
<h3 id="tasks-with-output-boxes">5.1.1 - Tasks with output boxes</h3>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-13.png"
alt="REC." />
<figcaption aria-hidden="true">REC.</figcaption>
</figure>
<p><strong>Referring Expression Comprehension (REC)</strong> aims to
localize <strong>a</strong> target object in an image described by a
referring expression.</p>
<p><strong>Described Object Detection</strong> extends REC to more
realistic scenarios where the object may <strong>not exist</strong> or
there may be <strong>multiple</strong> objects.</p>
<p><strong>VQA Grounding</strong> aims to answer visual questions and
associate the answers with specific visual regions or objects.</p>
<h3 id="tasks-with-input-boxes">5.1.2 - Tasks with input boxes</h3>
<p><strong>Grounding Caption (GC)</strong>: Given an image and a
location box, <strong>GC</strong> aims to generate a description for
this location by considering the surrounding environment.</p>
<p><strong>Referring Expression Generation (REG)</strong>: Compared to
<strong>GC</strong>, <strong>REG</strong> requires the generated
description to indicate that it describes this region specifically, not
others, making it necessary for the description to be
discriminative.</p>
<p><strong>PointQA</strong> requires a model answer for a visual
question where the questioner queries a specific position in the
picture.</p>
<h3 id="shikra-both-input-and-output">5.1.3 - Shikra: both input and
output</h3>
<p>Our model is not only compatible with the above tasks, but also can
handles the input and output of position representation flexibly and
simultaneously.</p>
<h2 id="position-representation">5.2 - Position Representation</h2>
<p>RoI: regions of interest</p>
<h3 id="inputting-roi">5.2.1 - Inputting RoI</h3>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-14.png"
alt="Method 1: directly concatenate cropped image patches with the original image as model input." />
<figcaption aria-hidden="true"><strong>Method 1:</strong> directly
concatenate cropped image patches with the original image as model
input.</figcaption>
</figure>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-15.png"
alt="Method 2: use 0/1 mask or Gaussian map input with the original image to emphasize the area of user interest." />
<figcaption aria-hidden="true"><strong>Method 2</strong>: use 0/1 mask
or Gaussian map input with the original image to emphasize the area of
user interest.</figcaption>
</figure>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-16.png"
alt="Method 3: first encode points and boxes to positional encodings then add them to intermediate features or learned queries." />
<figcaption aria-hidden="true"><strong>Method 3</strong>: first encode
points and boxes to positional encodings then add them to intermediate
features or learned queries.</figcaption>
</figure>
<h3 id="outputting-roi">5.2.2 - Outputting RoI</h3>
<p><strong>Anchor-based methods</strong> utilize predefined sliding
windows and proposal candidate regions for classification., e.g., Fast
R-CNN.</p>
<p>Some methods directly regress four values for bounding box
coordinates, e.g., FCOS.</p>
<p>Some methods adopt one-to-one label assignment to evolve object
detection into an end-to-end manner, e.g., DETR and POTP.</p>
<p><strong>Pix2seq</strong> formalizes the detection task as a sequence
generation task:</p>
<ul>
<li>It desires the spatial position of the image in 1,000 bins and uses
a 1,000-token vocabulary to represent it.</li>
<li>For detection, Pix2seq performs classification on the coordinate
vocabulary in an auto-regressive manner.</li>
</ul>
<p>Following Pix2seq, several methods, e.g., OFA, Unified-IO, UniTab,
GIT, and VisionLLM introduce similar coordinate vocabulary alongside the
language vocabulary for object detection and REC tasks.</p>
<h3 id="shikra">5.2.3 - Shikra</h3>
<p>Shikra formulates position input/output as the most natural and
flexible form of language and compare it with the extra coordinate
vocabulary in Section 4.2.</p>
<h1 id="conclusion">6 - Conclusion</h1>
<p><strong>Limitations</strong></p>
<ul>
<li>Shikra is unsuitable for dense object detection and segmentation
tasks.</li>
<li>Shikra may produce harmful and counterfactual responses.</li>
</ul>
<p><strong>Future Work</strong></p>
<ul>
<li>Exploring improved coordinate representations for these tasks.</li>
<li>Improve GCoT</li>
<li>Multilingual</li>
</ul>
]]></content>
      <categories>
        <category>Paper</category>
        <category>2023</category>
        <category>06</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Multi-modal</tag>
        <tag>Instrcution-Following</tag>
        <tag>Fine-grained MLLM</tag>
        <tag>Position Representation</tag>
      </tags>
  </entry>
  <entry>
    <title>How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources</title>
    <url>/2023/06/15/T%C3%BClu/</url>
    <content><![CDATA[<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/open-instruct-1.jpg"
alt="authors" />Paper: <a
href="https://arxiv.org/pdf/2306.04751.pdf">https://arxiv.org/pdf/2306.04751.pdf</a></p>
<p>Code: <a
href="https://github.com/allenai/open-instruct">https://github.com/allenai/open-instruct</a></p>
<blockquote>
<p>Evaluation for instruction-tuned models remains inconsistent and
difficult. Therefore, this work covers extensive evaluations on a large
range of models and datasets.</p>
</blockquote>
<p><strong><em>When reading this note, you can think about the following
questions:</em></strong></p>
<ol type="1">
<li>What instruction datasets, pretrained models and <strong>evaluation
metrics</strong> are used?</li>
<li>What are the evaluation results?</li>
</ol>
<h2 id="background">Background</h2>
<h3 id="instruction-tuning">Instruction Tuning</h3>
<p><strong>Definition</strong>: finetuning pretrained LMs to better
understand and respond to various human requests that are expressed in
natural language.</p>
<p><strong>Advantages</strong>: (1) zero-shot generalization to new
tasks; (2) non-experts can use natural language to interact with
LLMs.</p>
<blockquote>
<p>The most popular programming language in the future will be
English.</p>
</blockquote>
<p><strong>Training Paradigms</strong>: (1) supervised
learning(demonstrations); (2) reinforcement learning (feedback data)</p>
<p><strong>Key Components</strong>: (1) pretrained LMs; (2) instruction
datasets(diversity, task num)</p>
<h3 id="evaluation-method">Evaluation Method</h3>
<p><strong>Benchmark-based evaluation</strong></p>
<ul>
<li><a href="https://arxiv.org/abs/2211.09110">HELM</a>, <a
href="https://doi.org/10.5281/zenodo.5371628">LM Evaluation Harness</a>:
suitable for various NLP models</li>
<li><a href="https://arxiv.org/pdf/2210.11416.pdf">Flan-T5 work</a>:
focus on factuality and reasoning abilities</li>
<li><a href="https://arxiv.org/abs/2303.08774">GPT-4</a>, <a
href="https://arxiv.org/abs/2305.10403">PaLM v2</a>: proprietary models
with comprehensive evaluations</li>
</ul>
<p><strong>Open-ended instruction-following ability
evaluation</strong></p>
<ul>
<li><a href="https://arxiv.org/abs/2305.14387">Alpaca Farm</a>: leverage
other models as annotators for judging model generations</li>
<li><a href="https://lmsys.org/blog/2023-05-03-arena/">Chatbot
Arena</a>: leverage humans</li>
</ul>
<p>This work involves traditional benchmarks, model-based evaluation,
and human-based evaluation.</p>
<h2 id="training-models-with-various-datasets">Training Models with
Various Datasets</h2>
<h3 id="datasets-and-format-unity">Datasets and Format Unity</h3>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/open-instruct-2.png" />
<strong>Datasets</strong>: Only CoT and Code-Alpaca are built for
specific skills. <a
href="https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/tree/main/HTML_cleaned_raw_dataset">ShareGPT</a>
is a collection of user interactions with various chat systems publicly
shared.</p>
<p><strong>Human data mixture</strong>: FLAN V2, CoT, Dolly, Open
Assistant 1</p>
<p><strong>Human+GPT data mixture</strong>: Human data mixture +
GPT4-Alpaca, Code-Alpaca, ShareGPT</p>
<p><strong>Format Unity</strong>: It aims at representing arbitrary
rounds as one sentence.</p>
<ul>
<li><span class="math inline">\(N\)</span>: instance num in a
dataset</li>
<li><span class="math inline">\(i\)</span>: round num in each
example</li>
<li><span class="math inline">\(\{(x_1^j, y_1^j,x_2^j, y_2^j,...,x_i^j,
y_i^j)\}_{j=1}^N\)</span>: an instruction dataset</li>
</ul>
<h3 id="models-training">Models Training</h3>
<ul>
<li><span class="math inline">\(X:\{(x_1^j,
x_2^j,...,x_i^j)\}_{j=1}^N\)</span></li>
<li><span class="math inline">\(Y:\{(y_1^j,
y_2^j,...,y_i^j)\}_{j=1}^N\)</span></li>
<li><span class="math inline">\(t_n\)</span>: the <span
class="math inline">\(n\)</span>-th input token(belonging to X or
Y)</li>
<li>loss function <span class="math inline">\(L=-\sum\limits_n \log
p_{\theta}(t_n|t_{&lt;n})\times\left\{\begin{array}{}1 &amp; if\space
t_n\in Y \\ 0 &amp; otherwise\end{array}\right.\)</span></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># hyperparams</span><br><span class="line">max_seq_len: 1024 for 30B and 65B, 2048 for others</span><br><span class="line">epoch: 2</span><br><span class="line">learning rate: 1e-5 for 30B and 65B, 2e-5 for others. (linear decay and linear warmup only used for 3% of total steps)</span><br></pre></td></tr></table></figure>
<p><strong>T√ºlu</strong>: a suite of 7B to 65B LLaMA models
fully-instruction-tuned on Human+GPT data mixture.</p>
<h2 id="evaluation-setup">Evaluation Setup</h2>
<p>Load models using <strong><a
href="https://arxiv.org/pdf/2208.07339.pdf">8-bit mode</a></strong>
provided in the Huggingface Transformers library.</p>
<p>When doing generation, we use greedy decoding and a max length of 512
tokens, unless otherwise specified.</p>
<h3 id="facets-of-evaluation">Facets of Evaluation</h3>
<p><strong>(1) Specific model capabilities</strong>:</p>
<ul>
<li><strong>Factual knowledge</strong>: <a
href="https://arxiv.org/abs/2009.03300">MMLU</a>. [<a
href="https://github.com/hendrycks/test">Its official evaluation scripts
and prompts</a>]. Modify it to allow for batch processing. We evaluate
using 0 and 5 few-shot examples, following the original setup of
MMLU.</li>
<li><strong>Reasoning</strong>. We evaluate with and without
chain-of-thought (CoT vs Direct). Subsample GSM and BBH to a reasonable
size to improve the efficiency of doing CoT reasoning.
<ul>
<li><a href="https://arxiv.org/abs/2110.14168">GSM</a>(test split) for
mathematical reasoning capabilities(8-shot). Because all answers in GSM
are numbers, we extract the last number in the model response as the
final answer. Sampled 200 from the 1319 examples.</li>
<li><a href="https://arxiv.org/abs/2210.09261">BBH</a> for general
reasoning capabilities(3-shot). For the CoT setup, we extract the first
word after the phrase ‚ÄòSo the answer is‚Äô, or the entire response if
there is no such substring present.</li>
</ul></li>
<li><strong>Multilinguality</strong>: <a
href="https://arxiv.org/abs/2003.05002">TyDiQA</a> for multilingual QA.
Follow <a href="https://arxiv.org/pdf/2305.10403.pdf">PaLM 2</a>'s
setup. We use the gold-passage setup where one passage containing the
reference answer is given.</li>
<li><strong>Coding</strong>: <a
href="https://arxiv.org/abs/2107.03374">Codex-Eval(HumanEval)</a> for
abilities of generating functionally correct programs from docstrings.
Following the original paper, we compute unbiased estimates of pass@k to
measure the functional correctness of models‚Äô outputs. We report both
pass@1 and pass@10. The pass@1 results were obtained by sampling with a
temperature of 0.1 and the pass@10 results with a temperature of
0.8.</li>
</ul>
<p><strong>(2) Open-ended instruction following</strong>: model-based
evaluation and human evaluation</p>
<h3 id="model-based-evaluation-using-gpt-4">Model-Based Evaluation using
GPT-4</h3>
<p><strong>Dataset</strong>: Use a test set of 805 instructions.</p>
<p><strong>Code</strong>: Adopt <a
href="https://arxiv.org/abs/2305.14387">AlpacaFarm</a>'s code, but
slightly alter prompts to fit our message format.</p>
<p><strong>A GPT-4 annotator('greedy_gpt4')</strong>: Compare the
testing model with Davinci-003.</p>
<p><strong>Win-rate</strong>: The percentage of model generations that
GPT-4 reports as being preferred over the generations from
Davinci-003.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Hyperparameters</span><br><span class="line">temperature: 0</span><br><span class="line">batch: 5 (reduce it if the 5 examples exceed the 8192 token context window limit)</span><br><span class="line">max_output_token_len: extended from 300 to 2048 (in order to avoid cut-off generations)</span><br></pre></td></tr></table></figure>
<h3 id="human-evaluation">Human Evaluation</h3>
<p>The model information is anonymized and their outputs are put in
random order.</p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/open-instruct-8.jpg" /></p>
<p><strong>Use 332 instructions</strong>: 252 from <a
href="https://arxiv.org/abs/2212.10560">Self-Instruct</a> and 80 from <a
href="https://lmsys.org/blog/2023-03-30-vicuna/">Vicuna</a> evaluation
set.</p>
<p><strong>Compare 3 pairs of models</strong>: (1) T√úLU 65B vs ChatGPT
(2) T√úLU 65B vs T√úLU 7B (3) T√úLU 65B vs a 65B LLaMA model trained on the
Human data mixture.</p>
<p><strong>Interface for human judgements</strong>:</p>
<ul>
<li><strong>Indivisual acceptability</strong>: "yes"/"no", a 2-way
decision. For "yes", if and only if the response answered the request in
the query, had no significant errors, and did not have repetitive
information.</li>
<li><strong>Pairwise preference</strong>: "A is clearly better"/"A is
slightly better"/"Tie"/"B is slightly better"/"B is clearly better", a
5-way decision, select which one is more helpful.</li>
</ul>
<p><strong>Recruited 18 expert annotators</strong>, which are
researchers at AI2 or students at UW for the annotation. All these
annotators are fluent English speakers and hold bachelor‚Äôs degrees or
above. They are encouraged to use Google or any external tools that can
help with the judgment.</p>
<p><strong>Inter-Annotator Agreement</strong>:</p>
<ul>
<li>We measure the agreement of our annotators on a subset of
<strong>119 examples</strong> (63 instances randomly sampled from the
ChatGPT3 vs T√úLU 65B comparison, and 59 instances randomly sampled from
the T√úLU 65B vs T√úLU 7B comparison).</li>
<li><strong>Indivisual acceptability</strong>: an agreement of
0.84.</li>
<li><strong>Pairwise preference</strong>: an agreement of 0.72.
Following <a href="https://arxiv.org/pdf/2305.11206.pdf">Lima</a>, we
report a tie-discounted accuracy, which assigns one point if both
annotators agreed, half a point if either annotator (but not both)
labeled a tie, and zero point otherwise. We also merged ‚Äúclearly better‚Äù
and ‚Äúslightly better‚Äù together, so our final options will be simply
comparing which of A and B is better, or a tie.</li>
</ul>
<h2 id="results">Results</h2>
<blockquote>
<p>The best model in any given evaluation reaches on average 83% of
ChatGPT performance, and 68% of GPT-4 performance.</p>
</blockquote>
<h3
id="analysis-of-instruction-tuning-datasets-and-base-models">Analysis of
Instruction Tuning Datasets and Base Models</h3>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/open-instruct-3.jpg" />
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/open-instruct-4.jpg" />
"1.4T" means <span class="math inline">\(1.4\times 10^{12}\)</span>
tokens are used to train the model. "180B" means <span
class="math inline">\(180\times 10^{9}\)</span></p>
<ul>
<li>There is not a single best instruction tuning dataset across all
tasks</li>
<li>Combining datasets results in the best overall performance on the
benchmark tasks</li>
<li>Base model quality is extremely important for downstream
performance</li>
</ul>
<h3 id="pushing-the-limits-of-open-models">Pushing the Limits of Open
Models</h3>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/open-instruct-5.jpg" /></p>
<ul>
<li>Instrcution tuning brings large benefits on top of LLaMA models at
all sizes</li>
<li>Smaller models benefit most from instruction tuning</li>
<li>T√úLU still lags behind SOTA proprietary models</li>
</ul>
<h3
id="model-basedhuman-evaluation-results-for-open-ended-generation">Model-Based/Human
Evaluation Results for Open-ended Generation</h3>
<p><strong>Model-Based Evaluation</strong> <img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/open-instruct-6.jpg" /></p>
<ul>
<li>Models trained on mixtures based on traditional NLP datasets perform
poorly</li>
<li>Datasets that encourage long, diverse generations perform best</li>
<li>ShareGPT performs best</li>
</ul>
<blockquote>
<p>The judge model(has bias) may not always reveal the testing model
deficiencies.</p>
</blockquote>
<p><strong>Human Evaluation</strong> <img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/open-instruct-7.jpg" /></p>
<ul>
<li>Human evaluation results largely correlate with the AlpacaFarm and
benchmark-based evaluation</li>
<li>Making use of distilled datasets provides a large performance
boost</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p><strong><em>Future Work</em></strong></p>
<ul>
<li>explore instruction-tuning methods that use reinforcement
learning</li>
<li>explore more recent strong base models and other instruction
datasets</li>
<li>design more versatile model(generality)
<ul>
<li>better dataset mixing</li>
<li>instruction-tuning modular models (e.g., <a
href="https://arxiv.org/abs/1701.06538">mixture-of-experts</a>)</li>
</ul></li>
<li>improve the reliability and scalability of human evaluation for
instruction-following models</li>
</ul>
<p><strong><em>Limitations</em></strong></p>
<ul>
<li>Small proportions of data may contain personally identifying
details, but this work does not remove them, which may produce toxic or
harmful generations.</li>
<li>Not include evaluations of multi-turn dialogue and summarization
abilities</li>
</ul>
<p><strong><em>Broader Impact</em></strong></p>
<p>Training and releasing large instruction-tuned models need sufficient
testing to limit potential harm.</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>2023</category>
        <category>06</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Evaluation</tag>
        <tag>Instruction-Following</tag>
      </tags>
  </entry>
  <entry>
    <title>Home</title>
    <url>/2023/06/08/home/</url>
    <content><![CDATA[<p>I am currently a master student at <a
href="https://www.ecnu.edu.cn/">ECNU</a>. My recent research interest
mainly focuses on multi-modal learning (especially combined with LLMs).
You can <a href="mailto:ifshine_lx@163.com">email</a> me for further
communication.</p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/home.jpg" /></p>
]]></content>
  </entry>
  <entry>
    <title>[Waiting...]Hexo Usage</title>
    <url>/2023/06/08/hexo-usage/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! Check <a
href="https://hexo.io/docs/">documentation</a> for more info. If you get
any problems when using Hexo, you can find the answer in <a
href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or
you can ask for help on <a
href="https://github.com/hexojs/hexo/issues">GitHub</a>. Win11 is the
default OS in this post.</p>
<h2 id="hexo-install6.3.0">Hexo Install(6.3.0)</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install -g hexo-cli <span class="comment"># cmd with administrator permissions</span></span><br><span class="line">hexo -v <span class="comment"># check whether the installation is successful</span></span><br><span class="line"><span class="built_in">mkdir</span> &lt;root_dir&gt; <span class="comment"># create an empty dir</span></span><br><span class="line"><span class="built_in">cd</span> &lt;root_dir&gt;</span><br><span class="line">hexo init</span><br><span class="line">hexo s <span class="comment"># run server</span></span><br></pre></td></tr></table></figure>
<h2 id="next-theme8.17.0">Next Theme(8.17.0)</h2>
<p>Use <a
href="https://github.com/next-theme/hexo-theme-next">hexo-theme-next</a>
as an example. More info: <a href="https://theme-next.js.org/docs">Theme
Next Doc</a>, <a href="http://t.csdn.cn/Tu6fy">CSDN blog 1</a>, <a
href="http://t.csdn.cn/EmYFJ">CSDN blog 2</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/next-theme/hexo-theme-next themes/next</span><br><span class="line"><span class="comment"># open root_dir/_config.yml, replace &quot;theme: landscape&quot; with &quot;theme: next&quot;</span></span><br></pre></td></tr></table></figure>
<h2 id="deploy-to-github">Deploy to Github</h2>
<p>add ".gitignore" file to blog root dir: <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">.DS_Store</span><br><span class="line">Thumbs.db</span><br><span class="line">db.json</span><br><span class="line">*.log</span><br><span class="line">node_modules/</span><br><span class="line">public/</span><br><span class="line">.deploy*/</span><br><span class="line">_multiconfig.yml</span><br></pre></td></tr></table></figure> open cmd, then
cd blog root dir (win10) <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install hexo-deployer-git --save <span class="comment"># install a plugin</span></span><br><span class="line">git config --global user.email <span class="string">&quot;you@example.com&quot;</span></span><br><span class="line">git config --global user.name <span class="string">&quot;Your Name&quot;</span></span><br></pre></td></tr></table></figure> open root_dir/_config.yml, modify
"deploy" <figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: git@github.com:ifshinelx/ifshinelx.github.io.git</span><br><span class="line">  branch: main</span><br></pre></td></tr></table></figure> deploy (After the cmd execution, it takes several
minutes for the github page to refresh) For the first deployment, you
need to click <a
href="http://ifshinelx.github.io/ifshinelx.github.io">http://ifshinelx.github.io/ifshinelx.github.io</a>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo clean <span class="comment"># clean cache</span></span><br><span class="line">hexo g <span class="comment"># generate static files</span></span><br><span class="line">hexo d <span class="comment"># deploy to remote sites</span></span><br></pre></td></tr></table></figure></p>
<span id="more"></span>
<h2 id="personalization">Personalization</h2>
<h3 id="hexo-basic-info_config.yml">Hexo Basic Info(_config.yml)</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">title: XinLiu&#x27;s Homepage, Welcome!</span><br><span class="line">subtitle: &#x27;&#x27;</span><br><span class="line">description: &#x27;&#x27;</span><br><span class="line">keywords:</span><br><span class="line">author: Xin Liu</span><br><span class="line">language: en</span><br><span class="line">timezone: &#x27;Asia/Shanghai&#x27;</span><br><span class="line"></span><br><span class="line">url: https://ifshinelx.github.io</span><br><span class="line">math:</span><br><span class="line">  every_page: false</span><br><span class="line">  mathjax:</span><br><span class="line">    enable: true</span><br></pre></td></tr></table></figure>
<h3 id="next-theme-settings-basic"><a
href="https://theme-next.js.org/docs/theme-settings/">NexT Theme
Settings Basic</a></h3>
<p>root_dir/themes/next/_config.yml add "little star.jpg" to
root_dir/themes/next/source/images/ <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cache:</span><br><span class="line">  enable: true</span><br><span class="line"></span><br><span class="line"># Remove unnecessary files after hexo generate.</span><br><span class="line">minify: true</span><br><span class="line"></span><br><span class="line">scheme: Gemini</span><br><span class="line">favicon:</span><br><span class="line">  small: /images/little star.jpg</span><br><span class="line">  medium: /images/little star.jpg</span><br><span class="line">  # small: /images/favicon-16x16-next.png</span><br><span class="line">  # medium: /images/favicon-32x32-next.png</span><br><span class="line">creative_commons:</span><br><span class="line">  size: small</span><br><span class="line">  sidebar: true</span><br><span class="line">  post: true</span><br><span class="line">  language: deed.en</span><br><span class="line">menu:</span><br><span class="line">  home: / || fa fa-home</span><br><span class="line">  archives: /archives/ || fa fa-archive</span><br><span class="line">menu_settings:</span><br><span class="line">  icons: true</span><br><span class="line">  badges: true</span><br></pre></td></tr></table></figure> More Info: <a
href="https://theme-next.js.org/docs/third-party-services/math-equations.html">Math
Equations</a>, <a
href="https://theme-next.js.org/docs/tag-plugins/label.html">Label</a>,
<a href="https://theme-next.js.org/docs/tag-plugins/note.html">Note</a>,
<a
href="https://theme-next.js.org/docs/tag-plugins/tabs.html">Tabs</a></p>
<h3 id="sidebar"><a
href="https://theme-next.js.org/docs/theme-settings/sidebar.html">Sidebar</a></h3>
<p>NexT config file <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sidebar:</span><br><span class="line">  position: right</span><br><span class="line">avatar:</span><br><span class="line">  url: /images/little star.jpg #/images/avatar.gif</span><br><span class="line">social:</span><br><span class="line">  GitHub: https://ifshinelx.github.io || fab fa-github</span><br><span class="line">  E-Mail: mailto:ifshine_lx@163.com || fa fa-envelope</span><br><span class="line">recent_posts_title: Recent Posts</span><br><span class="line">recent_posts_layout: block</span><br><span class="line">recent_posts: true</span><br></pre></td></tr></table></figure> In
root_dir/themes/next/layout/_partials/sidebar/site-overview.njk, add the
code block: <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;%- if theme.social %&#125;</span><br><span class="line">...</span><br><span class="line">&#123;%- endif %&#125;</span><br><span class="line"></span><br><span class="line">&#123;# recent posts #&#125;</span><br><span class="line">&#123;% if theme.recent_posts %&#125;</span><br><span class="line">  &lt;div class=&quot;links-of-blogroll motion-element &#123;&#123; &quot;links-of-blogroll-&quot; + theme.recent_posts_layout  &#125;&#125;&quot;&gt;</span><br><span class="line">    &lt;div class=&quot;links-of-blogroll-title&quot;&gt;</span><br><span class="line">      &lt;!-- modify icon to fire by szw --&gt;</span><br><span class="line">      &lt;i class=&quot;fa fa-history fa-&#123;&#123; theme.recent_posts_icon | lower &#125;&#125;&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;</span><br><span class="line">      &#123;&#123; theme.recent_posts_title &#125;&#125;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">    &lt;ul class=&quot;links-of-blogroll-list&quot; style=&quot;padding: 0px 12px;&quot;&gt;</span><br><span class="line">      &#123;% set posts = site.posts.sort(&#x27;-date&#x27;) %&#125;</span><br><span class="line">      &#123;% set recent_posts = posts.slice(0, 5).toArray() %&#125;</span><br><span class="line">      &#123;% for post in recent_posts %&#125;</span><br><span class="line">        &#123;% if post.title != &quot;Home&quot; %&#125;</span><br><span class="line">          &lt;li class=&quot;recent_posts_li&quot;&gt;</span><br><span class="line">            &lt;a href=&quot;&#123;&#123; url_for(post.path) &#125;&#125;&quot; title=&quot;&#123;&#123; post.title &#125;&#125;&quot; target=&quot;_blank&quot;&gt;&#123;&#123;date(post.date, &#x27;MM-DD&#x27;) &#125;&#125; &#123;&#123; post.title &#125;&#125; &lt;/a&gt;</span><br><span class="line">          &lt;/li&gt;</span><br><span class="line">        &#123;% endif %&#125;</span><br><span class="line">      &#123;% endfor %&#125;</span><br><span class="line">    &lt;/ul&gt;</span><br><span class="line">  &lt;/div&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br><span class="line"></span><br><span class="line">&#123;%- if theme.creative_commons.license and theme.creative_commons.sidebar %&#125;</span><br><span class="line">...</span><br><span class="line">&#123;%- endif %&#125;</span><br></pre></td></tr></table></figure> In root_dir/themes/next/source/css/main.styl,
add the code block in the file end. <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">li.recent_posts_li &#123;</span><br><span class="line">    text-align: left;</span><br><span class="line">    display: block;</span><br><span class="line">    word-break: keep-all;</span><br><span class="line">    white-space: nowrap;</span><br><span class="line">    overflow: hidden;</span><br><span class="line">    text-overflow: ellipsis;</span><br><span class="line"></span><br><span class="line">    &amp;:hover &#123;</span><br><span class="line">      a&#123;</span><br><span class="line">        color: #fc6423;</span><br><span class="line">        border-bottom-color: #fc6423;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="footer"><a
href="https://theme-next.js.org/docs/theme-settings/footer.html">Footer</a></h3>
<p>NexT config file <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">footer:</span><br><span class="line">  since: 2023</span><br><span class="line">  powered: false</span><br><span class="line">busuanzi_count:</span><br><span class="line">  enable: true</span><br></pre></td></tr></table></figure> In
root_dir/themes/next/layout/_partials/footer.njk, delete the code block:
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;div class=&quot;wordcount&quot;&gt;</span><br><span class="line">...</span><br><span class="line">&lt;/div&gt;</span><br></pre></td></tr></table></figure> In root_dir/themes/next/layout/_partials/footer.njk, add
the code block: <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;%- if theme.busuanzi_count.enable %&#125;</span><br><span class="line">&lt;div class=&quot;busuanzi-count&quot;&gt;</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  &#123;%- if config.symbols_count_time.total_symbols %&#125;</span><br><span class="line">  &lt;span class=&quot;post-meta-item&quot;&gt;</span><br><span class="line">    &lt;span class=&quot;post-meta-item-icon&quot;&gt;</span><br><span class="line">      &lt;i class=&quot;fa fa-chart-line&quot;&gt;&lt;/i&gt;</span><br><span class="line">    &lt;/span&gt;</span><br><span class="line">    &#123;%- if theme.symbols_count_time.item_text_total %&#125;</span><br><span class="line">      &lt;span&gt;&#123;&#123; __(&#x27;symbols_count_time.count_total&#x27;) + __(&#x27;symbol.colon&#x27;) &#125;&#125;&lt;/span&gt;</span><br><span class="line">    &#123;%- endif %&#125;</span><br><span class="line">    &lt;span title=&quot;&#123;&#123; __(&#x27;symbols_count_time.count_total&#x27;) &#125;&#125;&quot;&gt;&#123;&#123; symbolsCountTotal(site) &#125;&#125;&lt;/span&gt;</span><br><span class="line">  &lt;/span&gt;</span><br><span class="line">  &#123;%- endif %&#125;</span><br><span class="line"></span><br><span class="line">&lt;/div&gt;</span><br><span class="line">&#123;%- endif %&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="posts"><a
href="https://theme-next.js.org/docs/theme-settings/posts.html">Posts</a></h3>
<p>a <strong>Read More</strong> button in a post: <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;!-- more --&gt;</span><br></pre></td></tr></table></figure> a
plug-in <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm i hexo-word-counter --save</span><br><span class="line">hexo clean</span><br></pre></td></tr></table></figure> NexT config file <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tag_icon: true</span><br></pre></td></tr></table></figure></p>
<h3 id="custom-pagestags-categories-home"><a
href="https://theme-next.js.org/docs/theme-settings/custom-pages.html">Custom
Pages(Tags, Categories, Home)</a></h3>
<p><a
href="https://hexo.io/docs/front-matter#Categories-amp-Tags">Hexo's Docs
of Categories &amp; Tags</a> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> root_dir</span><br><span class="line">hexo new page tags</span><br><span class="line">hexo new page categories</span><br></pre></td></tr></table></figure> NexT config file
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">menu:</span><br><span class="line">  tags: /tags/ || fa fa-tags</span><br><span class="line">  categories: /categories/ || fa fa-th</span><br></pre></td></tr></table></figure></p>
<p>root_dir/source/tags/index.md <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">date: 2023-06-10 21:55:15</span><br><span class="line">type: &quot;tags&quot;</span><br><span class="line">comments: false</span><br><span class="line">---</span><br></pre></td></tr></table></figure>
root_dir/source/categories/index.md <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">date: 2023-06-10 21:55:25</span><br><span class="line">type: &quot;categories&quot;</span><br><span class="line">comments: false</span><br><span class="line">---</span><br></pre></td></tr></table></figure> tag color setting
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// create themes\next\layout\tag-color.njk</span><br><span class="line">&lt;script type=&quot;text/javascript&quot;&gt;</span><br><span class="line">     var alltags = document.getElementsByClassName(&#x27;tag-cloud-tags&#x27;);</span><br><span class="line">     var tags = alltags[0].getElementsByTagName(&#x27;a&#x27;);</span><br><span class="line">     for (var i = tags.length - 1; i &gt;= 0; i--) &#123;</span><br><span class="line">       var golden_ratio = 0.618033988749895;</span><br><span class="line">       var s = 0.5;</span><br><span class="line">       var v = 0.999;</span><br><span class="line">       var h = golden_ratio + Math.random()*0.8 - 0.5;</span><br><span class="line">       var h_i = parseInt(h * 6);</span><br><span class="line">       var f = h * 6 - h_i;</span><br><span class="line">       var p = v * (1 - s);</span><br><span class="line">       var q = v * (1 - f * s);</span><br><span class="line">       var t = v * (1 - (1 - f) * s);</span><br><span class="line">       var r, g, b;</span><br><span class="line">       switch (h_i) &#123;</span><br><span class="line">          case 0:</span><br><span class="line">              r = v;</span><br><span class="line">              g = t;</span><br><span class="line">              b = p;</span><br><span class="line">              break;</span><br><span class="line">          case 1:</span><br><span class="line">              r = q;</span><br><span class="line">              g = v;</span><br><span class="line">              b = p;</span><br><span class="line">              break;</span><br><span class="line">          case 2:</span><br><span class="line">              r = p;</span><br><span class="line">              g = v;</span><br><span class="line">              b = t;</span><br><span class="line">              break;</span><br><span class="line">          case 3 :</span><br><span class="line">              r = p;</span><br><span class="line">              g = q;</span><br><span class="line">              b = v;</span><br><span class="line">              break;</span><br><span class="line">          case 4:</span><br><span class="line">              r = t;</span><br><span class="line">              g = p;</span><br><span class="line">              b = v;</span><br><span class="line">              break;</span><br><span class="line">          case 5:</span><br><span class="line">              r = v;</span><br><span class="line">              g = p;</span><br><span class="line">              b = q;</span><br><span class="line">              break;</span><br><span class="line">          default:</span><br><span class="line">              r = 1;</span><br><span class="line">              g = 1;</span><br><span class="line">              b = 1;</span><br><span class="line">        &#125;</span><br><span class="line">       tags[i].style.background = &quot;rgba(&quot;+parseInt(r*255)+&quot;,&quot;+parseInt(g*255)+&quot;,&quot;+parseInt(b*255)+&quot;,&quot;+0.5+&quot;)&quot;;</span><br><span class="line">     &#125;</span><br><span class="line">&lt;/script&gt;</span><br><span class="line"></span><br><span class="line">&lt;style&gt;</span><br><span class="line">  .tag-cloud-tags&#123;</span><br><span class="line">    text-align: center;</span><br><span class="line">    counter-reset: tags;</span><br><span class="line">  &#125;</span><br><span class="line">  .tag-cloud-tags a&#123;</span><br><span class="line">    display: inline-block;</span><br><span class="line">    border: 0px;</span><br><span class="line">    border-radius: 10px;</span><br><span class="line">    padding: 0px 10px;</span><br><span class="line">    margin: 8px;</span><br><span class="line">    color: rgba(34, 34, 34, 0.8);</span><br><span class="line">    </span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  .tag-cloud-tags a:hover&#123;</span><br><span class="line">     box-shadow: 0px 5px 15px 0px rgba(0,0,0,.4);</span><br><span class="line">     transform: scale(1.1);</span><br><span class="line">     transition-duration: 0.15s;</span><br><span class="line">  &#125;</span><br><span class="line">&lt;/style&gt;</span><br></pre></td></tr></table></figure> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// modify themes\next\layout\_partials\page\tags.njk</span><br><span class="line">&lt;div class=&quot;tag-cloud&quot;&gt;</span><br><span class="line">  ...</span><br><span class="line">&lt;/div&gt;</span><br><span class="line">&#123;% include &#x27;tag-color.njk&#x27; %&#125;</span><br></pre></td></tr></table></figure> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// modify themes\next\layout\_macro\post.njk</span><br><span class="line">      &lt;header&gt;</span><br><span class="line">        ...</span><br><span class="line">        </span><br><span class="line">        &#123;%- if post.tags and post.tags.length %&#125;</span><br><span class="line">          &#123;%- set tag_indicate = &#x27;&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt;&#x27; if theme.tag_icon else &#x27;#&#x27; %&#125;</span><br><span class="line">          &lt;div class=&quot;post-tags&quot; style=&quot;margin-top: 5px;&quot;&gt;</span><br><span class="line">            &#123;%- for tag in post.tags.toArray() %&#125;</span><br><span class="line">              &lt;a href=&quot;&#123;&#123; url_for(tag.path) &#125;&#125;&quot; rel=&quot;tag&quot; style=&quot;border: 0px; border-radius: 10px; padding: 0px 10px;&quot;&gt;&#123;&#123; tag_indicate &#125;&#125; &#123;&#123; tag.name &#125;&#125;&lt;/a&gt;</span><br><span class="line">            &#123;%- endfor %&#125;</span><br><span class="line">          &lt;/div&gt;</span><br><span class="line">          &lt;script type=&quot;text/javascript&quot;&gt;</span><br><span class="line">              var tagsall=document.getElementsByClassName(&quot;post-tags&quot;)</span><br><span class="line">              for (var i = tagsall.length - 1; i &gt;= 0; i--)&#123;</span><br><span class="line">                  var tags=tagsall[i].getElementsByTagName(&quot;a&quot;);</span><br><span class="line">                  for (var j = tags.length - 1; j &gt;= 0; j--) &#123;</span><br><span class="line">                      var r=Math.floor(Math.random()*75+200);</span><br><span class="line">                      var g=Math.floor(Math.random()*75+200);</span><br><span class="line">                      var b=Math.floor(Math.random()*75+200);</span><br><span class="line">                      tags[j].style.background = &quot;rgb(&quot;+r+&quot;,&quot;+g+&quot;,&quot;+b+&quot;)&quot;;</span><br><span class="line">                  &#125;</span><br><span class="line">              &#125;                        </span><br><span class="line">            &lt;/script&gt;</span><br><span class="line">        &#123;%- endif %&#125;</span><br><span class="line">      &lt;/header&gt;</span><br></pre></td></tr></table></figure></p>
<p><strong>Home Page</strong>: root_dir/themes/next/layout/index.njk:
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;% block content %&#125;</span><br><span class="line"></span><br><span class="line">  &#123;%- set postlen = site.posts.toArray().length %&#125;</span><br><span class="line">  &#123;%- set post = site.posts.sort(&#x27;-date&#x27;).toArray()[postlen-1] %&#125;</span><br><span class="line">  &#123;&#123; partial(&#x27;_macro/home.njk&#x27;, &#123;post: post, is_index: true&#125;) &#125;&#125;</span><br><span class="line"></span><br><span class="line">&#123;% endblock %&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="search-services"><a
href="https://theme-next.js.org/docs/third-party-services/search-services.html">Search
Services()</a></h3>
<p>Details of Algolia Search are in <a
href="https://theme-next.js.org/docs/third-party-services/search-services.html#Algolia-Search">here</a>.
<a
href="https://github.com/next-theme/hexo-generator-searchdb">Searchdb</a>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm i hexo-generator-searchdb --save</span><br></pre></td></tr></table></figure> Hexo config: <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># search hexo-generator-searchdb</span><br><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br><span class="line">  content: true</span><br><span class="line">  format: html</span><br><span class="line">  limit: 100</span><br></pre></td></tr></table></figure> Next config:
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">local_search:</span><br><span class="line">  enable: true</span><br></pre></td></tr></table></figure></p>
<h3 id="comment-systemsgitalk"><a
href="https://theme-next.js.org/docs/third-party-services/comments.html">Comment
Systems(Gitalk)</a></h3>
<p>Click here to sign up for a <a
href="https://github.com/settings/applications/new">new OAuth
Application</a>. Other content can be filled in at will, but be sure to
fill in the correct callback URL (usually the domain name corresponding
to the comment page). Then you will get a Client ID and a Client
secret.</p>
<p>Create a public repository you want to store Gitalk comments in your
GitHub.</p>
<p>Set the value <code>enable</code> to <code>true</code>, add Client ID
(<code>client_id</code>) and Client secret (<code>client_secret</code>)
in step 1, add your Github username (<code>github_id</code> and
<code>admin_user</code>) and the created repository name
(<code>repo</code>) in step 2, and edit other configurations in
<code>gitalk</code> section in the <strong><em>NexT config
file</em></strong> as following:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Gitalk</span><br><span class="line"># For more information: https://gitalk.github.io</span><br><span class="line">gitalk:</span><br><span class="line">  enable: false</span><br><span class="line">  github_id: # GitHub repo owner</span><br><span class="line">  repo: # Repository name to store issues</span><br><span class="line">  client_id: # GitHub Application Client ID</span><br><span class="line">  client_secret: # GitHub Application Client Secret</span><br><span class="line">  admin_user: # GitHub repo owner and collaborators, only these guys can initialize gitHub issues</span><br><span class="line">  distraction_free_mode: true # Facebook-like distraction free mode</span><br><span class="line">  # When the official proxy is not available, you can change it to your own proxy address</span><br><span class="line">  proxy: https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token # This is official proxy address</span><br><span class="line">  # Gitalk&#x27;s display language depends on user&#x27;s browser or system environment</span><br><span class="line">  # If you want everyone visiting your site to see a uniform language, you can set a force language value</span><br><span class="line">  # Available values: en | es-ES | fr | ru | zh-CN | zh-TW</span><br><span class="line">  language:</span><br></pre></td></tr></table></figure>
<h3 id="waiting...-statistics-and-analyticsumami">[Waiting...] <a
href="https://theme-next.js.org/docs/third-party-services/statistics-and-analytics.html">Statistics
and Analytics(Umami)</a></h3>
<p><a href="http://t.csdn.cn/OCEnG">MySQL</a> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">l</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>Software</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention</title>
    <url>/2023/06/09/llama-adapter-v1/</url>
    <content><![CDATA[<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-1.jpg"
alt="authors" />Paper: <a
href="https://arxiv.org/pdf/2303.16199.pdf">https://arxiv.org/pdf/2303.16199.pdf</a></p>
<p>Code: <a
href="https://github.com/OpenGVLab/LLaMA-Adapter">https://github.com/OpenGVLab/LLaMA-Adapter</a></p>
<p>More Info: <a
href="https://github.com/Lightning-AI/lit-parrot">Lighting AI |
Lit-Parrot: lightweight update of llama</a></p>
<p><strong><em>When reading this note, you can think about the following
questions:</em></strong></p>
<ol type="1">
<li>What is learnable adaption prompts?</li>
<li>What is zero-init attention?</li>
<li>How to extend LLaMA-Adapter to multi-modal input?</li>
</ol>
<hr />
<h2 id="method">Method</h2>
<p><em>4 characteristics of LLaMA-Adapter:</em></p>
<ol type="1">
<li>1.2M Parameters</li>
<li>1 Hour: 8 A100 GPUs, three times faster than Alpaca</li>
<li>Plug with Expertise: insert their respective adapters and endow
LLaMA with different expert knowledge</li>
<li>Multi-modal: simply add images tokens into adaption prompts</li>
</ol>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1--2.jpg"
alt="LLaMA-Adapter&#39;s 4 characteristics and details" /> <strong>The
idea of LLaMA-Adapter is model-agnostic and can be applied to other
LLMs.</strong></p>
<h3 id="learnable-adaption-prompts">Learnable Adaption Prompts</h3>
<ul>
<li><span class="math inline">\(K\)</span>: prompt length for each
layer</li>
<li><span class="math inline">\(C\)</span>: feature dim of
transformer</li>
<li><span class="math inline">\(N\)</span>: total transformer layer num
of LLaMA</li>
<li><strong><span class="math inline">\(\{ P_l \}_{l=1}^{L}\)</span>
(<span class="math inline">\(P_l\in\mathbb{R}^{K\times C}\)</span>) :
learnable adaption prompts</strong> for topmost <span
class="math inline">\(L (L\le N)\)</span> transformer layers with
higher-level semantic representations</li>
<li><span class="math inline">\(T_l\in\mathbb{R}^{M\times C}\)</span>:
<span class="math inline">\(M\)</span>-length word tokens in <span
class="math inline">\(l\)</span>-th inserted layer</li>
<li><span class="math inline">\([P_l;\space
T_l]\in\mathbb{R}^{(K+M)\times C}\)</span>: concatenate <span
class="math inline">\(P_l\)</span> and <span
class="math inline">\(T_l\)</span>, the learned <strong>instruction
knowledge</strong> in <span class="math inline">\(P_l\)</span> guides
<span class="math inline">\(T_l\)</span> to generate contextual
responses</li>
</ul>
<h3 id="zero-init-attention">Zero-init Attention</h3>
<p>If the adaption prompts are randomly initialized, they will bring
noise to the word tokens and damage original knowledge in LLaMA at the
early training stage, which harms stablity and effectiveness.</p>
<p><em><span class="math inline">\(t_l\in\mathbb{R}^{1\times
C}\)</span>: generate the (M+1)-th word <span
class="math inline">\(t_l\)</span> on top of <span
class="math inline">\([P_l;\space T_l]\)</span></em> at the <span
class="math inline">\(l\)</span>-th inserted layer</p>
<ol type="1">
<li>linear projection: queries <span
class="math inline">\(Q_l=Linear_q(t_l)\)</span>, keys <span
class="math inline">\(K_l=Linear_k([P_l;T_l;t_l])\)</span>, values <span
class="math inline">\(V_l=Linear_v([P_l;T_l;t_l])\)</span></li>
<li>attention scores: <span
class="math inline">\(S_l=\frac{Q_lK_l^T}{\sqrt{C}}\in\mathbb{R}^{1\times(K+M+1)}\)</span>,
records the feature similarities between <span
class="math inline">\(t_l\)</span> and all <span
class="math inline">\(K+M+1\)</span> tokens</li>
<li>reformulation: <span class="math inline">\(S_l=[S_l^K;S_l^{M+1}]^T,
S_l^K\in\mathbb{R}^{K\times 1}, S_l^{M+1}\in\mathbb{R}^{(M+1)\times
1}\)</span>
<ul>
<li><span class="math inline">\(S_l^K\)</span> represents how much
information the <span class="math inline">\(P_l\)</span> contribute to
<span class="math inline">\(t_l\)</span>, which probably causes noise in
the early training stage</li>
</ul></li>
<li>softmax operation: <span
class="math inline">\(S_l^g=[Softmax(S_l^K)¬∑g_l;Softmax(S_l^{M+1})]^T\)</span>
<ul>
<li><span class="math inline">\(g_l\)</span> is a learnable gating
factor initialized by zero, adaptively controls the importance of <span
class="math inline">\(S_l^K\)</span></li>
<li>in practice, each head of attention has an independent <span
class="math inline">\(g_l\)</span></li>
</ul></li>
<li>output of the attention layer: <span
class="math inline">\(t_l^o=Linear_o(S_l^gV_l)\in\mathbb{R}^{1\times
C}\)</span></li>
</ol>
<h3 id="multi-modal-reasoning">Multi-modal Reasoning</h3>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-3.jpg" />
Task textual input for ScienceQA: question + textual context + options.
(We utilize "Generate caption for this image" as the textual instruction
input for LLaMA-Adapter)</p>
<ol type="1">
<li>multi-scale global visual features: <span
class="math inline">\(\{I_m\}_{m=1}^{M},I_m\in\mathbb{R}^{1\times
C_m}\)</span>, <span class="math inline">\(M\)</span> is the scale num
<ul>
<li>from pre-trained CLIP</li>
</ul></li>
<li>overall image token: <span
class="math inline">\(I_p=Projection\Big(Concat\left(\{I_m\}_{m=1}^M\right)\Big)\in\mathbb{R}^{1\times
C}\)</span>
<ul>
<li>concatenate along the channel dim</li>
<li>utilize cascaded MLPs as the learnable projection network with 0.6M
parameters</li>
</ul></li>
<li>repeat <span class="math inline">\(I_p\)</span> for <span
class="math inline">\(K\)</span> times</li>
<li>multi-modal prompt: <span
class="math inline">\(P_l^v=P_l+Repeat(I_p)\in\mathbb{R}^{K\times
C}\)</span></li>
</ol>
<p><strong>Future work: using pre-trained modal-specific encoders, we
can integrate instructional signals of different modalities into the
adaption prompts</strong></p>
<h3
id="zero-initialized-attention-for-other-large-models">Zero-initialized
Attention for other Large Models</h3>
<p>In addition to instruction-following models, our zero-initialized
attention can be generalized to other vision and language models for
parameter-efficient fine-tuning.</p>
<p><strong>Vision Model</strong>. We insert the adaption prompts as
prefix into the topmost L transformer layers in the pre-trained <a
href="https://arxiv.org/pdf/2010.11929.pdf">ViT</a>, and modify the
attention operations to be zero-initialized at all inserted layers.</p>
<p><strong>Language Model</strong>. We evaluate our fine-tuning efficacy
on <a href="https://arxiv.org/pdf/1907.11692.pdf">RoBERTa</a>, and
implement the zero-initialized attention on top of <a
href="https://arxiv.org/pdf/2110.07602.pdf">P-tuning v2</a>, a prompt
tuning method for efficiently adapting large language models. Likewise,
we only enable the prompt tokens in P-tuning v2 and our zero gating
factors to be learnable during fine-tuning.</p>
<p><strong>Vision-Language Model</strong>. In detail, we adopt CLIP with
a ViT-B/16 as the visual encoder and a 12-layer transformer as the
textual encoder. We freeze the entire CLIP and insert the adaption
prompts with zero-initialized attention into CLIP‚Äôs visual encoder.</p>
<h2 id="experiment">Experiment</h2>
<h3 id="instruction-following-evaluation">Instruction-following
Evaluation</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">epochs: 5</span><br><span class="line">warmup epochs: 2</span><br><span class="line">batch size: 64</span><br><span class="line">learning rate: 0.009</span><br><span class="line">weight decay: 0.02</span><br><span class="line"># LLaMA-7B</span><br><span class="line">N: 32</span><br><span class="line">K: 10</span><br><span class="line">L: 30</span><br></pre></td></tr></table></figure>
<p><strong>Generation Stage Decoding Method</strong>: top-p sampling
with a temperature 0.1 and a top-p = 0.75</p>
<p><strong>Dataset</strong>: Alpaca-52K self-instruct data.
LLaMA-Adapter paper wrongly denotes as Alphaca-52K</p>
<p><strong>Evaluation Metric</strong>: (1) quantitative evaluation, ask
GPT-4 to assess the response quality on 80 questions(Since we observed
that GPT-4 has a preference to give higher scores to the first response
in comparison, we also switch the position of two responses, resulting
in a total of 160 evaluation items.); (2) simply show some response
examples</p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-15.jpg" /></p>
<p>Comparison of Instruction-Following Capability, LLaMA-Adapter is
comparable to Alpaca with fully fine-tuned 7B parameters <img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-4.jpg"
alt="Comparison of Instruction-Following Capability" /></p>
<p>Comparison with Instruct LLaMA (LLaMA-I, LLaMA-65B fine-tuned on
large-scale instructional data), LLaMA-Adapter can be further enhanced
with larger LLaMA, larger data, larger learnable parameters <img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-5.jpg"
alt="Comparison with Instruct LLaMA (LLaMA-I)" /></p>
<h3 id="multi-modal-evaluation">Multi-modal Evaluation</h3>
<p><strong>Generation Stage Decoding Method</strong>: greedy search</p>
<p>Other hyperparameters are the same as instruction-following
LLaMA-Adapter.</p>
<p><strong>Dataset</strong>: ScienceQA, COCO Caption</p>
<p><strong>Result on ScienceQA</strong>: MM-CoT relies on the complex
two-stage inference. <strong>Future Work: leverage CoT to boost
LLaMA-Adapter.</strong> <img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-7.jpg"
alt="ScienceQA" /></p>
<p><strong>Result on COCO Caption</strong>: Both BLIP and BLIP-2 adopt a
costly pre-training stage on additional datasets for superior
performance. In contrast, our LLaMA-Adapter only requires COCO
Catption‚Äôs training set of 0.6M data and attains better accuracy than
ClipCap. <img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-16.jpg" /></p>
<h3 id="ablation-study">Ablation Study</h3>
<p>(ScienceQA for example)</p>
<p>Result: table 6 shows robustness to over-fitting on the small
dataset. Even if LLaMA-Adapter has over-fitted the fine-tuning data(val
loss), the val acc is still increasing. <img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-8.jpg"
alt="Ablation Study" /></p>
<h3
id="zero-initialized-attention-for-other-large-models-1">Zero-initialized
Attention for other Large Models</h3>
<p><strong>Vision Model‚Äî‚ÄîViT</strong>: Image classification.(Table 7,
Table 9)</p>
<p><strong>Language Model‚Äî‚ÄîRoBERTa</strong>: (1) Extractive question
answering(Table 8), Exact Match (EM) and F1 scores on the dev set are
reported. (2) NER and SRL.(Table 10)</p>
<p><strong>Vision-Language Model‚Äî‚ÄîCLIP</strong>: The model is trained
only on the base classes in a few-shot setting and evaluated on both
base and novel categories.(Table 11)</p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-17.jpg" />
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-18.jpg" />
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-19.jpg" />
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-20.jpg" /></p>
<h2 id="conclusion">Conclusion</h2>
<p>Future direction:</p>
<ul>
<li>wider multi-modal inputs(audio, video, point clouds)
<ul>
<li>using pre-trained modal-specific encoders, we can integrate
instructional signals of different modalities into the adaption
prompts</li>
</ul></li>
<li>larger LLaMA(33B, 65B)</li>
<li>other LLMs</li>
<li>diverse benchmarks(VQA v2, OK-VQA, TVQA, DocVQA)
<ul>
<li>ScienceQA is only an understanding task</li>
</ul></li>
<li>leverage CoT to boost LLaMA-Adapter</li>
</ul>
<h2 id="code-implementation">Code Implementation</h2>
<h3 id="params">Params</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">accum_iter: 1. Accumulate gradient iterations (for increasing the effective batch size under memory constraints)</span><br><span class="line">batch_size: 4(per GPU). effective_batch_size = batch_size * accum_iter * gpu_num</span><br><span class="line">epoch: 5</span><br><span class="line">adapter_layer: 30. the num of adapter layer L</span><br><span class="line">adapter_len: 10. the adapter length K</span><br><span class="line">max_seq_len: 512. specifies the maximum number of input tokens. token num &gt;= word num.</span><br><span class="line">max_batch_size: 32.</span><br><span class="line">dim: 4096.</span><br><span class="line">n_heads: 32.</span><br><span class="line">n_layers: 32.</span><br><span class="line">weight_decay: 0.02.</span><br><span class="line">blr: 9e-3. base learning rate.</span><br><span class="line">lr: learning_rate(absolute lr), lr = blr * total_batch_size / 256</span><br><span class="line">min_lr: 0.0. lower lr bound for cyclic schedulers that hit 0</span><br><span class="line">warmup_epochs: 2.</span><br><span class="line">seed: 0.</span><br></pre></td></tr></table></figure>
<p>Why we need max_seq_len? For absolute position embedding(e.g., BERT,
Roberta, BART), it uses the index of each token to calculate and its
length is limited(max_seq_len). When the input token length exceeds
max_seq_len, <strong>"index error"</strong> will be caused. For other
position embedding methods(e.g., XLNet, T5), they have no limit of input
token length. But longer input token length brings <strong>heavier
memory burden</strong>, which may not necessarily lead to better
performance.</p>
<p>LLaMA uses Rotary Position Embedding: <a
href="https://zhuanlan.zhihu.com/p/627536105">ÂàÜÊûê |
ROPEÁöÑ‰∏çÂêåÂÆûÁé∞Ôºöllama&amp;palm</a>, <a
href="http://t.csdn.cn/U3RXo">blog 2.3 RoPEÊóãËΩ¨‰ΩçÁΩÆÁºñÁ†Å</a></p>
<h3 id="dataset">Dataset</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">InstructionDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_path, model_path, max_seq_len, partition=<span class="string">&quot;train&quot;</span></span>):</span><br><span class="line">        self.ann = json.load(<span class="built_in">open</span>(data_path))</span><br><span class="line">        <span class="keyword">if</span> partition == <span class="string">&quot;train&quot;</span>:</span><br><span class="line">            self.ann = self.ann</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># It seems that the val set is a sub set of the train set.(data_path is same)</span></span><br><span class="line">            self.ann = self.ann[:<span class="number">200</span>]</span><br><span class="line">        ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        ann = self.ann[index]</span><br><span class="line">        <span class="keyword">if</span> ann.get(<span class="string">&quot;input&quot;</span>, <span class="string">&quot;&quot;</span>) == <span class="string">&quot;&quot;</span>:</span><br><span class="line">            prompt = PROMPT_DICT[<span class="string">&quot;prompt_no_input&quot;</span>].format_map(ann)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            prompt = PROMPT_DICT[<span class="string">&quot;prompt_input&quot;</span>].format_map(ann)</span><br><span class="line">        example = prompt + ann[<span class="string">&quot;output&quot;</span>]</span><br><span class="line">        example = torch.tensor(self.tokenizer1.encode(example, bos=<span class="literal">True</span>, eos=<span class="literal">True</span>), dtype=torch.int64)</span><br><span class="line">        padding = self.max_words - example.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># prompt = Propmt(template, ann[&#x27;instruction&#x27;], ann[&#x27;input&#x27;])</span></span><br><span class="line">        <span class="comment"># max_seq_len refers to tokenizer([prompt, ann[&#x27;output&#x27;]]).length, not tokenizer(prompt).length</span></span><br><span class="line">        <span class="keyword">if</span> padding &gt; <span class="number">0</span>:</span><br><span class="line">            example = torch.cat((example, torch.zeros(padding, dtype=torch.int64) - <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">elif</span> padding &lt; <span class="number">0</span>:</span><br><span class="line">            example = example[: self.max_words]</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<h3 id="learnable-adaption-prompts-1">Learnable Adaption Prompts</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module): <span class="comment"># Decoder</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, params: ModelArgs</span>):</span><br><span class="line">        ...</span><br><span class="line">        <span class="comment"># randomly initialise the adaption prompts</span></span><br><span class="line">        <span class="comment"># github.com/OpenGVLab/LLaMA-Adapter/issues/9#issuecomment-1501705647</span></span><br><span class="line">        self.adapter_query = nn.Embedding(params.adapter_len * params.adapter_layer, params.dim)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, examples, labels</span>):</span><br><span class="line">        _bsz, seqlen = examples.shape</span><br><span class="line">        ...</span><br><span class="line">        mask = torch.full((<span class="number">1</span>, <span class="number">1</span>, seqlen, seqlen), <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>), device=h.device)</span><br><span class="line">        mask = torch.triu(mask, diagonal=<span class="number">0</span> + <span class="number">1</span>).type_as(h) <span class="comment"># Upper triangular matrix, and diagonal val is 0</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers[: -<span class="number">1</span> * self.adapter_layer]:</span><br><span class="line">            h = layer(h, start_pos, freqs_cis, mask)</span><br><span class="line">        adapter_index = <span class="number">0</span></span><br><span class="line">        <span class="comment"># adapter.shape: (30, 1, 10, 4096)</span></span><br><span class="line">        adapter = self.adapter_query.weight.reshape(-<span class="number">1</span>, self.adapter_len, <span class="number">4096</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers[-<span class="number">1</span> * self.adapter_layer :]:</span><br><span class="line">            h = layer(h, start_pos, freqs_cis, mask, adapter[adapter_index].half())</span><br><span class="line">            adapter_index = adapter_index + <span class="number">1</span></span><br><span class="line">        output = self.output(self.norm(h))</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<ul>
<li>linear projection: queries <span
class="math inline">\(Q_l=Linear_q(t_l)\)</span>, keys <span
class="math inline">\(K_l=Linear_k([P_l;T_l;t_l])\)</span>, values <span
class="math inline">\(V_l=Linear_v([P_l;T_l;t_l])\)</span></li>
<li>attention scores: <span
class="math inline">\(S_l=\frac{Q_lK_l^T}{\sqrt{C}}\in\mathbb{R}^{1\times(K+M+1)}\)</span></li>
<li>reformulation: <span class="math inline">\(S_l=[S_l^K;S_l^{M+1}]^T,
S_l^K\in\mathbb{R}^{K\times 1}, S_l^{M+1}\in\mathbb{R}^{(M+1)\times
1}\)</span></li>
<li>softmax operation: <span
class="math inline">\(S_l^g=[Softmax(S_l^K)¬∑g_l;Softmax(S_l^{M+1})]^T\)</span></li>
<li>output of the attention layer: <span
class="math inline">\(t_l^o=Linear_o(S_l^gV_l)\in\mathbb{R}^{1\times
C}\)</span></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, args: ModelArgs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        ...</span><br><span class="line">        <span class="comment"># zero-init attention</span></span><br><span class="line">        self.gate = torch.nn.Parameter(torch.zeros(<span class="number">1</span>, self.n_local_heads, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor, start_pos: <span class="built_in">int</span>, freqs_cis: torch.Tensor, mask: <span class="type">Optional</span>[torch.Tensor], adapter=<span class="literal">None</span></span>):</span><br><span class="line">        bsz, seqlen, _ = x.shape</span><br><span class="line">        <span class="comment"># 1. three Linears for x</span></span><br><span class="line">        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)</span><br><span class="line">        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)</span><br><span class="line">        xk = xk.view(bsz, seqlen, self.n_local_heads, self.head_dim)</span><br><span class="line">        xv = xv.view(bsz, seqlen, self.n_local_heads, self.head_dim)</span><br><span class="line">        <span class="comment"># 2. add position info via Rotary Position Embedding</span></span><br><span class="line">        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> adapter <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            adapter_len = adapter.shape[<span class="number">1</span>] <span class="comment"># adapter.shape: (1, 10, 4096)</span></span><br><span class="line">            <span class="comment"># linear projection</span></span><br><span class="line">            <span class="comment"># adapter_k.shape: (bsz, adapter_len, self.n_local_heads, self.head_dim)</span></span><br><span class="line">            adapter_k = self.wk(adapter).view(<span class="number">1</span>, adapter_len, self.n_local_heads, self.head_dim).repeat(bsz, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            adapter_v = self.wv(adapter).view(<span class="number">1</span>, adapter_len, self.n_local_heads, self.head_dim).repeat(bsz, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            xk = torch.cat([adapter_k, xk], dim=<span class="number">1</span>)</span><br><span class="line">            xv = torch.cat([adapter_v, xv], dim=<span class="number">1</span>)</span><br><span class="line">            extra_mask = torch.zeros(<span class="number">1</span>, <span class="number">1</span>, seqlen, adapter_len).to(mask)</span><br><span class="line">            mask = torch.cat([extra_mask, mask], dim=-<span class="number">1</span>) <span class="comment"># (1, 1, seqlen, adapter_len + seqlen)</span></span><br><span class="line">        keys = xk</span><br><span class="line">        values = xv</span><br><span class="line"></span><br><span class="line">        xq = xq.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        keys = keys.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        values = values.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 3. attention scores</span></span><br><span class="line">        scores = torch.matmul(xq, keys.transpose(<span class="number">2</span>, <span class="number">3</span>)) / math.sqrt(self.head_dim)</span><br><span class="line">        <span class="comment"># for decoder type, mask is needed to avoid using the information in the future.</span></span><br><span class="line">        <span class="comment"># the predictions for position i can depend only on the known outputs at positions less than i</span></span><br><span class="line">        <span class="comment"># mask.shape: (1, 1, seqlen, adapter_len + seqlen)</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            scores = scores + mask  <span class="comment"># (bsz, n_local_heads, seqlen, adapter_len + seqlen)</span></span><br><span class="line">        <span class="comment"># 4. softmax</span></span><br><span class="line">        <span class="keyword">if</span> adapter <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            scores = torch.cat(</span><br><span class="line">                [</span><br><span class="line">                    <span class="comment"># zero-init attention</span></span><br><span class="line">                    self.gate.tanh().half() * F.softmax(scores[:, :, :, :adapter_len].<span class="built_in">float</span>(), dim=-<span class="number">1</span>).type_as(xq),</span><br><span class="line">                    F.softmax(scores[:, :, :, adapter_len:].<span class="built_in">float</span>(), dim=-<span class="number">1</span>).type_as(xq),</span><br><span class="line">                ],</span><br><span class="line">                dim=-<span class="number">1</span>,</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            scores = F.softmax(scores.<span class="built_in">float</span>(), dim=-<span class="number">1</span>).type_as(xq)</span><br><span class="line">        output = torch.matmul(scores, values)  <span class="comment"># (bs, n_local_heads, slen, head_dim)</span></span><br><span class="line">        output = output.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(bsz, seqlen, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.wo(output)</span><br></pre></td></tr></table></figure>
<p>(images below are from <a
href="http://jalammar.github.io/illustrated-gpt2/">jalammar.github.io/illustrated-gpt2/</a>)
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-9.png" /><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-10.png" /><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-11.png" /></p>
<h2 id="related-work">Related Work</h2>
<h3 id="parameter-efficient-fine-tuningpeft">Parameter-Efficient
Fine-Tuning(PEFT)</h3>
<p><img
src="https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/classic-flowchart-1536x535.png"
alt="Three conventional approaches of finetuing" /> <a
href="https://lightning.ai/pages/community/article/understanding-llama-adapters/">Three
conventional approaches of finetuing.(the pre-trained model is not too
large)</a> They are all compatible with encoder and decoder style. When
finetune generative models, we work with and build on the embeddings
they create instead of the generated output texts. But in-context
learning only applies to decoder style. <img
src="https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/classic-performance-1024x377.png"
alt="traininig efficiency and modeling performance" /></p>
<p><a href="https://github.com/huggingface/peft">PEFT</a> methods freeze
most parameters of pre-trained models, and can still exhibit comparable
capabilities on downstream tasks. It is needed when we want to get a
similar modeling quality as finetuning II on LLMs. (e.g., Prompt-Tuning,
Adapter, LoRA)</p>
<p>Prompt tuning appends a collection of trainable prompt tokens to
pre-trained large models, which are inserted either to the input
embeddings only, or to all of the intermediate layers. (e.g., Hard/Soft
Prompt-Tuning, Prefix-Tuning)</p>
<p>Hard Prompt-Tuning: directly change the discrete input tokens, which
are not differentiable: <img
src="https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/hard-prompting-1024x214.png" /></p>
<p>Soft Prompt-Tuning: concatenates the embeddings of the input tokens
with a trainable tensor that can be optimized via backpropagation.</p>
<p><a href="https://arxiv.org/pdf/2101.00190.pdf">Prefix Tuning</a>: add
a trainable tensor to each transformer block instead of only the input
embeddings, as in Soft Prompt-Tuning: <img
src="https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/prefix-tuning-1536x907.png" /></p>
<p><a href="https://arxiv.org/pdf/1902.00751.pdf">Adapter</a>: adds
adapter layers in two places. The hidden dim in each adapter layer is
low(e.g., 1024--&gt;24, params 1024√ó24+24√ó1024=49,512 &lt;
1024√ó1024=1,048,576). <img
src="https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/adapter-outline-1024x548.png" /></p>
<p><a
href="https://lightning.ai/pages/community/tutorial/lora-llm/">LoRA</a>:
introduces trainable rank decomposition matrices into each network
weights: <img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-13.jpg" /></p>
<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-12.png" />
<a href="https://aclanthology.org/2022.acl-long.433.pdf">(UniPELT: A
Unified Framework for Parameter-Efficient Language Model Tuning)</a></p>
<p>LLaMA-Adapter is distinct from regular Prefix-Tuning: 1. L topmost
layer(not all) 2. zero-init attention(stablity improvement) 3. unified
multi-modal tuning(unimodal to multi-modal) <img
src="https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/popular-methods-1024x282.png" />
<img
src="https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/llama-adapter-1447x1536.png" /></p>
<h3 id="instruction-following-models">Instruction-Following Models</h3>
<p>Instruction-following capabilities: understand user intentions and
follow instructions accurately.</p>
<p>Closed-source restriction and high training costs imped
instruction-following models' development.</p>
<p>Compared to a concurrent work <a
href="https://github.com/tloen/alpaca-lora">Alpaca-LoRA</a>, our
approach further reduces the computational demands, and can be
generalized to follow visual instructions for multi-modal reasoning.</p>
<p><em>Language Modality:</em></p>
<ul>
<li><a href="https://arxiv.org/pdf/2109.01652.pdf">FLAN</a>: introduces
an instruction tuning method</li>
<li><a href="https://arxiv.org/pdf/2202.01279.pdf">PromptSource</a>: a
web-based GUI for creating and managing natural language prompt</li>
<li><a
href="https://aclanthology.org/2022.emnlp-main.340.pdf">SUP-NATINST</a>:
an benchmark of instructions on 1,616 NLP tasks</li>
<li><a href="https://arxiv.org/pdf/2203.02155.pdf">InstructGPT</a>:
RLHF, significant performance improvements</li>
<li><strong><a
href="https://github.com/tatsu-lab/stanford_alpaca">Alpaca</a>:
data-efficient(self-instruction), high costs(fine-tuning)</strong></li>
<li><a href="https://lmsys.org/blog/2023-03-30-vicuna/">Vicuna</a> and
<a href="https://arxiv.org/pdf/2304.03277.pdf">GPT-4-LLM</a>: reveal
that dialog and enhanced instruction-following capabilities can be
ignited by fine-tuning on either user-shared ChatGPT conversations or
instruction-following data generated by the GPT-4 API</li>
</ul>
<p><em>Multi-modality:</em></p>
<p>Robot</p>
<ul>
<li>2020/03/31 <a
href="https://arxiv.org/pdf/1912.01734.pdf">ALFRED</a>: a benchmark for
robotics instruction following</li>
<li>2022/03/16 <a href="https://arxiv.org/pdf/2110.07342.pdf">FILM</a>:
a modular method for robotics instruction following</li>
</ul>
<p>Video</p>
<ul>
<li>2023/05/10 <a href="https://arxiv.org/pdf/2305.06355.pdf">VideoChat:
Chat-Centric Video Understanding</a>: (1) integrates video foundation
models and large language models via a learnable neural interface; (2)
propose a video-centric instruction dataset based on WebVid-10M; (3)
spatiotemporal reasoning, event localization, and causal relationship
inference</li>
<li>2023/06/08 <a
href="https://arxiv.org/pdf/2306.05424.pdf">Video-ChatGPT: Towards
Detailed Video Understanding via Large Vision and Language Models</a>:
(1) merges a video-adapted visual encoder with a LLM; (2) introduce a
new dataset of 100,000 video-instruction pairs; (3) develop a
quantitative evaluation framework for video-based dialogue models; (4)
understanding and generating detailed conversations about videos</li>
<li>2023/06/12 <a
href="https://arxiv.org/pdf/2306.02858.pdf">Video-LLaMA An
Instruction-tuned Audio-Visual Language Model for Video
Understanding</a>: (1) propose a Video Q-former for temporal info; (2)
propose an Audio Q-former based on ImageBind for audio-visual
signals</li>
</ul>
<p>Image(Model)</p>
<ul>
<li>2023/04/27 <a
href="https://arxiv.org/pdf/2302.14045v1.pdf">Kosmos-1</a>, <a
href="https://arxiv.org/pdf/2304.10592.pdf">MiniGPT4</a>, <a
href="https://arxiv.org/pdf/2304.08485.pdf">LLaVA</a>, <a
href="https://arxiv.org/pdf/2304.14178.pdf">mPLUG-Owl</a> <img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-14.jpg" /></li>
<li>2023/05/05 <a href="https://arxiv.org/pdf/2305.03726.pdf">Otter: A
Multi-Modal Model with In-Context Instruction Tuning</a></li>
<li>2023/05/11 <a
href="https://arxiv.org/pdf/2305.06500.pdf">InstructBLIP: Towards
General-purpose Vision-Language Models with Instruction Tuning</a>: (1)
gather a wide variety of 26 publicly available datasets, transform them
into instruction tuning format; (2) instruction-aware visual feature
extraction method</li>
<li>2023/05/22 <a href="https://arxiv.org/pdf/2305.04160.pdf">X-LLM:
Bootstrapping Advanced Large Language Models by Treating
Multi-Modalities as Foreign Languages</a>: propose X2L interface to
convert other modality(image, speech, video) into foreign language via 3
training stages</li>
<li>2023/05/24 <a href="https://arxiv.org/pdf/2305.15023.pdf">Cheap and
Quick: Efficient Vision-Language Instruction Tuning for Large Language
Models</a>: (1) Mixture-of-Modality Adaptation(MMA); (2) a routing
algorithm for MMA, which enables an automatic shift between single- and
multi-modal instructions; (3) MMA+LLaMA=LaVIN(efficient), 1.4 training
hours with 3.8M trainable params</li>
<li>2023/05/25 <a href="https://arxiv.org/pdf/2305.16355.pdf">PandaGPT:
One Model To Instruction-Follow Them All</a>: (1) visual and auditory
instruction-following capabilities(detailed image description
generation, writing stories inspired by videos, answering questions
about audios); (2) combines the multimodal encoders from ImageBind and
the large language models from Vicuna; (3) displays emergent, i.e.
zero-shot, cross-modal behaviors for data other than image and text
(e.g., video, audio, depth, thermal, and IMU)</li>
<li>2023/05/25 <a
href="https://arxiv.org/pdf/2305.16103.pdf">ChatBridge: Bridging
Modalities with Large Language Model as a Language Catalyst</a>: (1)
show that only language-paired two-modality data is sufficient to
connect all modalities; (2) propose a new multi-modal instruction tuning
dataset MULTIS, which covers a wide range of 16 multimodal tasks of
text, image, video, and audio modalities; (3) a two-stage training,
firstly aligns each modality with language, secondly aligns model with
user intent</li>
<li>2023/05/30 <a href="https://arxiv.org/pdf/2305.18752.pdf">GPT4Tools:
Teaching Large Language Model to Use Tools via Self-instruction</a>: (1)
enable open-source LLMs to use multimodal tools; (2) generates an
instruction-following dataset by prompting an advanced teacher with
various multi-modal contexts; (3) provide a benchmark to evaluate the
ability of LLMs to use tools</li>
<li>2023/06/02 <a href="https://arxiv.org/pdf/2305.05662.pdf">InternGPT:
Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond
Language</a>: integrates chatbots with non-verbal instructions(e.g.,
point movements like gestures and cursors), which requires fine-grained
control, editing, and generation of visual content</li>
<li>2023/06/13 <a
href="https://arxiv.org/pdf/2305.04790.pdf">MultiModal-GPT: A Vision and
Language Model for Dialogue with Humans</a>: (1) capable of generating
detailed captions, counting specific objects, and addressing general
inquiries posed by users; (2) OpenFlamingo+LoRA; (3) construct
multi-modal instruction templates; (4) also employ language-only
instruction-following data for dialogue performance improvement</li>
</ul>
<p>Image(Dataset)</p>
<ul>
<li>2023/06/08 <a href="https://arxiv.org/pdf/2306.05425.pdf">MIMIC-IT:
Multi-Modal In-Context Instruction Tuning</a>: (1) 2.8 million
multimodal instruction-response pairs; (2) 8 languages; (3) contains
videos</li>
<li>2023/06/08 <a href="https://arxiv.org/pdf/2306.04387.pdf">M3IT: A
Large-Scale Dataset towards Multi-Modal Multilingual Instruction
Tuning</a>: (1) comprises 40 carefully curated datasets, including 2.4
million instances and 400 manually written task instructions, which
surpasses previous datasets regarding task coverage; (2) 80 languages;
(3) Ying-VLM model</li>
<li>2023/06/10 <a
href="https://arxiv.org/pdf/2212.10773.pdf">MultiInstruct: Improving
Multi-Modal Zero-Shot Learning via Instruction Tuning</a>: (1) a
multimodal instruction tuning benchmark dataset that consists of 62
multimodal tasks; (2) a new evaluation metric, Sensitivity, to evaluate
how sensitive the model is to the variety of instructions</li>
<li>2023/06/11 <a href="https://arxiv.org/pdf/2306.06687.pdf">LAMM:
Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and
Benchmark</a>: (1) extend MLLMs to point clouds; (2) LAMM-Dataset and
LAMM-Benchmark for 2D image and 3D point cloud understanding</li>
</ul>
<h3 id="large-vision-language-models">Large Vision-Language Models</h3>
<p>Recently, some researchers adopt pre-trained unimodal models as
initialization and only train the newly introduced parameters. They use
mapping networks or cross-attention layers to connect two
modalities.</p>
<p>As a new method, LLaMA-Adapter also belongs to this line of work.</p>
<ul>
<li><a href="https://arxiv.org/pdf/2106.13884.pdf">Frozen</a>:
fine-tunes an image encoder to transform visual tokens into LLM‚Äôs soft
prompts</li>
<li><a href="https://arxiv.org/pdf/2111.07991.pdf">LiT</a>: utilizes
pretrained image encoder to speed up CLIP training</li>
<li><a href="https://arxiv.org/pdf/2111.09734.pdf">CLIPCap</a>: proposes
a mapping network to connect the pre-trained image encoder with
LLMs</li>
<li><a href="https://arxiv.org/pdf/2110.04544.pdf">CLIP-Adapter</a>, <a
href="https://arxiv.org/pdf/2111.03930.pdf">Tip-Adapter</a> and <a
href="https://arxiv.org/pdf/2112.02413.pdf">PointCLIP</a>: introduce
customized adapters upon CLIP for 2D and 3D few-shot learning</li>
<li><a href="https://arxiv.org/pdf/2204.14198.pdf">Flamingo</a>: inserts
several cross-attention layers to inject visual knowledge into LLMs</li>
<li><a href="https://arxiv.org/pdf/2301.12597.pdf">BLIP2</a>: connects
pre-trained image encoders and LLMs with a Q-Former</li>
</ul>
]]></content>
      <categories>
        <category>Paper</category>
        <category>2023</category>
        <category>03</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>PEFT</tag>
        <tag>Multi-modal</tag>
        <tag>Prefix-Tuning</tag>
        <tag>Adapter</tag>
        <tag>Instruction-Following</tag>
      </tags>
  </entry>
  <entry>
    <title>LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model</title>
    <url>/2023/06/19/llama-adapter-v2/</url>
    <content><![CDATA[<p><img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-0.png" /></p>
<p>Paper: <a
href="https://arxiv.org/pdf/2304.15010.pdf">https://arxiv.org/pdf/2304.15010.pdf</a></p>
<p>Code: <a
href="https://github.com/OpenGVLab/LLaMA-Adapter">https://github.com/OpenGVLab/LLaMA-Adapter</a></p>
<blockquote>
<p>After trained on language instruction data, LLaMA-Adapter-V1 is
fine-tuned on COCO Caption, which introducing new visual cues but
damaging instruction-following abilities.</p>
</blockquote>
<aside>
<p>‚§¥Ô∏è Therefore, LLaMA-Adapter-V2 further improves the multi-modal
instruction-following abilities of LLaMA by introducing 14M parameters
over LLaMA.</p>
</aside>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-1.png"
alt="Training Pipeline of LLaMA-Adapter V2." />
<figcaption aria-hidden="true">Training Pipeline of LLaMA-Adapter
V2.</figcaption>
</figure>
<hr />
<h2 id="related-work">Related Work</h2>
<h3 id="instruction-following-models">Instruction-following Models</h3>
<blockquote>
<p>Details can be seen in my previous note of LLaMA-Adapter V1: the
'Related Work/Instruction-Following Models' section.</p>
</blockquote>
<p>LLaMA-Adapter V2 can function effectively using just language
instruction data and image-text pairs, without relying on multi-modal
instruction data.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-2.png"
alt="Table 1. Training Comparison of Different Methods. CC, VG and L400 represent Conceptual Caption, Visual Genome and LAION 400M, respectively. ‚àó denotes the filtered dataset." />
<figcaption aria-hidden="true">Table 1. Training Comparison of Different
Methods. CC, VG and L400 represent Conceptual Caption, Visual Genome and
LAION 400M, respectively. ‚àó denotes the filtered dataset.</figcaption>
</figure>
<h3 id="parameter-efficient-fine-tuning">Parameter-efficient
Fine-tuning</h3>
<blockquote>
<p>Details can be seen in my previous note of LLaMA-Adapter V1: the
'Related Work/Parameter-Efficient Fine-Tuning(PEFT)' section.</p>
</blockquote>
<p>By utilizing an <strong>early fusion strategy</strong> and
<strong>bias tuning</strong>, LLaMA-Adapter V2 injects visual features
into LLMs, with only <strong>0.04%</strong> parameters of LLaMA.</p>
<h3 id="integration-of-expert-systems">Integration of Expert
Systems</h3>
<p>LLMs act as a core controller for external experts systems to boost
its overall performance.</p>
<ul>
<li><strong>Vision Tasks</strong>: visual models as experts. <a
href="https://arxiv.org/abs/2303.17580">HuggingGPT</a>, <a
href="https://arxiv.org/abs/2303.04671">Visual ChatGPT</a>, <a
href="https://arxiv.org/abs/2304.09842">Chameleon</a>, <a
href="https://arxiv.org/abs/2303.11381">MMReACT</a> and <a
href="https://arxiv.org/abs/2303.08128">ViperGPT</a></li>
<li><strong>Robotics</strong>: real-world sensors as experts. <a
href="https://arxiv.org/abs/2303.03378">PaLM-E</a>, <a
href="https://arxiv.org/abs/2207.05608">Inner Monologue</a> and <a
href="https://arxiv.org/abs/2303.12153">Text2Motion</a></li>
</ul>
<p>For LLaMA-Adapter V2, experts integration compensates for the
shortcomings brought by small training data.</p>
<h2 id="llama-adapter-v2">LLaMA-Adapter V2</h2>
<p>LLaMA-Adapter V2 is based on finetuned LLaMA-Adapter V1.</p>
<h3 id="bias-tuning-of-linear-layers">Bias Tuning of Linear Layers</h3>
<aside>
<p>üëâ To enhance its language instruction-following ability</p>
</aside>
<p>The added params accounts for <strong>0.04%(~5M) of LLaMA</strong>
and its initialization is helpful for stable training at early
stages.</p>
<p><strong>We first unfreeze all the normalization layers in LLaMA, and
then</strong>:</p>
<ul>
<li><span class="math inline">\(x\)</span>: input</li>
<li><span class="math inline">\(W\)</span>: pre-trained weights(frozen)
of a certain linear layer</li>
<li><span class="math inline">\(b=Init(0)\)</span>: learnable bias</li>
<li><span class="math inline">\(s=Init(1)\)</span>: learnable scale
factor</li>
<li><span class="math inline">\(y=W¬∑x\rightarrow y=s¬∑(W¬∑x+b)\)</span>:
<strong>modify each linear layer in the Transformer</strong></li>
</ul>
<p><strong>Our bias tuning method compared with related
work:</strong></p>
<ul>
<li><strong>BitFit, SSF</strong>: experiments on 80M parameters scale
<ul>
<li><strong>Ours</strong>: from 7B to 65B</li>
</ul></li>
<li><strong>LoRA</strong>: input-aware bias
<ul>
<li><p><strong>Ours</strong>: input-agnostic, less fine-tuning cost</p>
<aside>
<p>‚ùì why less cost</p>
</aside></li>
</ul></li>
</ul>
<h3 id="joint-training-with-disjoint-parameters">Joint Training with
Disjoint Parameters</h3>
<aside>
<p>üëâ For balanced visual instruction tuning</p>
</aside>
<ul>
<li>Trained on <strong>500K image-text pairs</strong>: visual projection
layers, early zero-initialized attention</li>
<li>Trained on <strong>50K language-only instruction data</strong>: late
zero-initialized attention, unfrozen norm, learnable bias and scale
factor (or optional <a
href="https://arxiv.org/pdf/2106.09685.pdf">LoRA</a>)</li>
</ul>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-3.png"
alt="Joint Training Paradigm in LLaMA-Adapter V2." />
<figcaption aria-hidden="true">Joint Training Paradigm in LLaMA-Adapter
V2.</figcaption>
</figure>
<h3 id="early-fusion-of-visual-knowledge">Early Fusion of Visual
Knowledge</h3>
<aside>
<p>üëâ To balance textual and visual understanding</p>
</aside>
<p><span class="math inline">\(N\)</span>: the total number of
Transformer layers</p>
<ul>
<li><p><strong>Visual prompts</strong>: concatenated with the word
tokens at <strong>the first <span class="math inline">\(K\)</span>
Transformer layers</strong> with zero-initialized attention</p>
<ul>
<li><span class="math inline">\(K&lt;N-L\)</span></li>
</ul></li>
<li><p><strong>Static adaptation prompts</strong>: inserted into the
last <span class="math inline">\(L\)</span> layers (e.g., L=31)</p>
<aside>
<p>‚ùì a little hard to understand ‚Äùinserted into‚Äù: add or
concatenate?</p>
</aside></li>
</ul>
<p>Prevent direct interactions between the two via placing them in
different layers.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-4.png"
alt="Early Fusion of Visual Knowledge." />
<figcaption aria-hidden="true">Early Fusion of Visual
Knowledge.</figcaption>
</figure>
<aside>
<p>‚ùì <strong>But in code implementation, textual tokens are not
concatenated with visual_query in the first layer</strong></p>
</aside>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># LLaMA-Adapter/llama_adapter_v2_multimodal/llama/llama_adapter.py</span><br><span class="line"># class LLaMA_adapter:</span><br><span class="line">def forward(self, visual_query, tokens, start_pos: int):</span><br><span class="line">    _bsz, seqlen = tokens.shape</span><br><span class="line">    h = self.llama.tok_embeddings(tokens)</span><br><span class="line">    freqs_cis = self.llama.freqs_cis.to(h.device)</span><br><span class="line">    freqs_cis = freqs_cis[start_pos : start_pos + seqlen]</span><br><span class="line">    mask = None</span><br><span class="line">    mask = torch.full((1, 1, seqlen, seqlen),</span><br><span class="line">                        float(&quot;-inf&quot;), device=h.device)</span><br><span class="line">    mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)</span><br><span class="line"></span><br><span class="line">    for layer in self.llama.layers[:-1 * self.query_layer]:</span><br><span class="line">        h = layer(h, start_pos, freqs_cis, mask)</span><br><span class="line"></span><br><span class="line">    adapter = self.adapter_query.weight.reshape(</span><br><span class="line">        self.query_layer, self.query_len, -1).unsqueeze(1)</span><br><span class="line">    adapter_index = 0</span><br><span class="line">    for layer in self.llama.layers[-1 * self.query_layer:]:</span><br><span class="line">        dynamic_adapter = adapter[adapter_index].repeat(_bsz, 1, 1)</span><br><span class="line">        dynamic_adapter = dynamic_adapter + visual_query</span><br><span class="line">        h = layer(h, start_pos, freqs_cis, mask, dynamic_adapter)</span><br><span class="line">        adapter_index = adapter_index + 1</span><br><span class="line"></span><br><span class="line">    h = self.llama.norm(h)</span><br><span class="line">    output = self.llama.output(h[:, -1, :])</span><br><span class="line">    return output.float()</span><br></pre></td></tr></table></figure>
<h3 id="integration-with-experts">Integration with Experts</h3>
<aside>
<p>üëâ To boost zero-shot multi-modal reasoning</p>
</aside>
<p><strong>Well-trained img2text experts</strong>:</p>
<ul>
<li><strong>default implementation</strong> adopts LLaMA-Adapter v1
pre-trained on COCO Caption for short and accurate image descriptions
generation</li>
<li>can be replaced with a search engine / OCR</li>
</ul>
<p>We can easily switch among different experts based on the specific
downstream task.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-5.png"
alt="Generation Pipeline of LLaMA-Adapter V2." />
<figcaption aria-hidden="true">Generation Pipeline of LLaMA-Adapter
V2.</figcaption>
</figure>
<aside>
<p>‚ùì The order of 3 input parts(Textual Context, Question, Visual
Context)</p>
</aside>
<h2 id="experiments">Experiments</h2>
<h3 id="experimental-setups">Experimental Setups</h3>
<p><strong>Training Data</strong>:</p>
<ul>
<li>52K single-turn instruction data from GPT-4-LLM</li>
<li>567K captioning data from COCO Caption</li>
<li>80K conversation data from ShareGPT (train a chatbot,
multi-round)</li>
</ul>
<p><strong>Implementation Details</strong>:</p>
<ul>
<li>visual prompt length: 20</li>
</ul>
<h3 id="stronger-language-instruction-model">Stronger Language
Instruction Model</h3>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-6.png"
alt="Part of Table 2. LLaMA-Adapter V2 provides more comprehensive answers and detailed explanations." />
<figcaption aria-hidden="true">Part of Table 2. LLaMA-Adapter V2
provides more comprehensive answers and detailed
explanations.</figcaption>
</figure>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-7.png"
alt="A Chatting Example using 7B LLaMA-Adapter V2. But its understanding of context is not very accurate." />
<figcaption aria-hidden="true">A Chatting Example using 7B LLaMA-Adapter
V2. But its understanding of context is not very accurate.</figcaption>
</figure>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-8.png"
alt="A Chatting Example using 65B LLaMA-Adapter V2." />
<figcaption aria-hidden="true">A Chatting Example using 65B
LLaMA-Adapter V2.</figcaption>
</figure>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-9.png"
alt="Response Quality Comparisons(assessed by GPT-4 on 80 questions)." />
<figcaption aria-hidden="true">Response Quality Comparisons(assessed by
GPT-4 on 80 questions).</figcaption>
</figure>
<ul>
<li>Ours: based on LLaMA-65B, fine-tune 14M parameters</li>
<li>Vicuna: based on LLaMA-13B, fine-tune 13B parameters</li>
</ul>
<h3 id="visual-instruction-model">Visual Instruction Model</h3>
<aside>
<p>üëá Image Captioning</p>
</aside>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-10.png"
alt="Comparisons on COCO Caption." />
<figcaption aria-hidden="true">Comparisons on COCO Caption.</figcaption>
</figure>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-11.png"
alt="Comparisons of Image Captioning Results(between LLaMA-Adapter and LLaMA-Adapter V2)." />
<figcaption aria-hidden="true">Comparisons of Image Captioning
Results(between LLaMA-Adapter and LLaMA-Adapter V2).</figcaption>
</figure>
<p>The failure case is intentionally choosen as an out-of-distribution
example (cartoon picture) for testing.</p>
<p>It shows that the image-text alignment ability is not strong
enough.</p>
<p>This motivates us to employ <strong>additional expert
systems</strong> to enhance the image understanding ability.</p>
<aside>
<p>üëá Visual Understanding</p>
</aside>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-12.png"
alt="The Visual Understanding Examples of LLaMA-Adapter V2." />
<figcaption aria-hidden="true">The Visual Understanding Examples of
LLaMA-Adapter V2.</figcaption>
</figure>
<ul>
<li>identify and explain the specific object or feature in the
image</li>
<li>sophisticated reasoning and decision-making</li>
<li>offer a plausible guess or explanation when image cannot provide
sufficient direct info</li>
</ul>
<aside>
<p>üëá Integration with Experts</p>
</aside>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-13.png"
alt="Visual Understanding with the help of Caption Experts(more precise and detailed responses)." />
<figcaption aria-hidden="true">Visual Understanding with the help of
Caption Experts(more precise and detailed responses).</figcaption>
</figure>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-14.png"
alt="Visual Understanding with the help of OCR Experts. The example and OCR context are from DocVQA." />
<figcaption aria-hidden="true">Visual Understanding with the help of OCR
Experts. The example and OCR context are from DocVQA.</figcaption>
</figure>
<h2 id="future-work">Future Work</h2>
<ul>
<li>integrate more expert visual systems</li>
<li>use multi-modal instruction dataset</li>
<li>other PEFT methods (e.g., LoRA)</li>
</ul>
]]></content>
      <categories>
        <category>Paper</category>
        <category>2023</category>
        <category>04</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>PEFT</tag>
        <tag>Multi-modal</tag>
        <tag>Expert Integration</tag>
        <tag>Prefix-Tuning</tag>
        <tag>Adapter</tag>
        <tag>Instruction-Following</tag>
      </tags>
  </entry>
</search>
