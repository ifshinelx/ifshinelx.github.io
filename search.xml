<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>[Waiting...]2022 Machine Learning Specialization</title>
    <url>/2023/06/15/2022-machine-learning-specialization/</url>
    <content><![CDATA[<blockquote>
<p><em>Definition of Machine Learning(informal)</em>: Field of study that gives computers the ability to learn without being explicitly programmed. [1959, Arthur Samuel]</p>
</blockquote>
<ul>
<li>Main Course Content<ul>
<li><strong>Supervised Learning</strong></li>
<li>Unsupervised Learning</li>
</ul>
</li>
<li>Others<ul>
<li>Reinforcement Learning</li>
<li>Practical advice for applying learning algorithms</li>
</ul>
</li>
</ul>
<h1 id="Supervised-Machine-Learning-Regression-and-Classification"><a href="#Supervised-Machine-Learning-Regression-and-Classification" class="headerlink" title="Supervised Machine Learning: Regression and Classification"></a>Supervised Machine Learning: Regression and Classification</h1><h2 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h2><h2 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h2><h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><!-- ## Machine Learning Overview
## Linear Regression with One Variable
## Training Linear Regression
## Linear Regression with Multiple Variables
## Practical Tips for Linear Regression
## Classification
## Cost Function
## Gradient Descent
## Regularization to Reduce Overfitting -->
<h1 id="Advanced-Learning-Algorithm"><a href="#Advanced-Learning-Algorithm" class="headerlink" title="Advanced Learning Algorithm"></a>Advanced Learning Algorithm</h1><h2 id="Neural-Networks"><a href="#Neural-Networks" class="headerlink" title="Neural Networks"></a>Neural Networks</h2><h2 id="Decision-Trees"><a href="#Decision-Trees" class="headerlink" title="Decision Trees"></a>Decision Trees</h2><h2 id="Advice-for-ML"><a href="#Advice-for-ML" class="headerlink" title="Advice for ML"></a>Advice for ML</h2><!-- ## Neural Networks Intuition
## Neural Network Model
## TensorFlow Implementation
## Neural Network Implementation in Python
## Speculations on Artificial General Intelligence(AGI)
## Vectorization(optional)
## Neural Network Training
## Activation Functions
## Multiclass Classification
## Additional Neural Network Concepts
## Advice for Applying Machine Learning
## Bias and Variance
## Machine Learning Development Process
## Skewed datasets(optional)
## Decision Trees
## Decision Tree Learning
## Tree Ensembles -->
<h1 id="Unsupervised-Learning-Recommender-Systems-and-Reinforcement-Learning"><a href="#Unsupervised-Learning-Recommender-Systems-and-Reinforcement-Learning" class="headerlink" title="Unsupervised Learning: Recommender Systems and Reinforcement Learning"></a>Unsupervised Learning: Recommender Systems and Reinforcement Learning</h1><h2 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h2><h2 id="Anomaly-Detection"><a href="#Anomaly-Detection" class="headerlink" title="Anomaly Detection"></a>Anomaly Detection</h2><h2 id="Collaborative-Filtering"><a href="#Collaborative-Filtering" class="headerlink" title="Collaborative Filtering"></a>Collaborative Filtering</h2><h2 id="Content-based-Filtering"><a href="#Content-based-Filtering" class="headerlink" title="Content-based Filtering"></a>Content-based Filtering</h2><h2 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h2><!-- ## Clustering
## Anomaly Detection
## Recommender System
## Recommender Systems Implementation
## Content-based Filtering
## Reinforcement Learning
## State-action Value Function
## Continuous State Spaces -->]]></content>
      <categories>
        <category>Online Course</category>
        <category>Andrew Ng</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>A Survey on Multimodal Large Language Models</title>
    <url>/2023/06/30/MLLM/</url>
    <content><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-author.png"></p>
<p>Paper: <a href="https://arxiv.org/pdf/2306.13549.pdf">https://arxiv.org/pdf/2306.13394.pdf</a></p>
<p>Code: <a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models">https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models</a></p>
<aside>
üí° ***Why we need MLLM?***

</aside>

<p><strong>MLLM</strong>: the LLM-based model with the ability to receive and reason with multimodal information</p>
<p><strong>Three Advantages</strong>: (1) more in line with human perception (2) more user-friendly interface (3) more well-rounded task-solver</p>
<p><strong>Three Example Capabilities</strong>: (1) write website codes based on images (2) understand deep meaning of memes (3) OCR-free math reasoning</p>
<p><strong>Four Key Techniques</strong>: M-IT, M-ICL, M-CoT, LAVR</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-1.png"></p>
<hr>
<h2 id="M-IT-Multimodal-Instruction-Tuning"><a href="#M-IT-Multimodal-Instruction-Tuning" class="headerlink" title="M-IT: Multimodal Instruction Tuning"></a>M-IT: Multimodal Instruction Tuning</h2><h3 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h3><p><strong>Instruction</strong>: the description of tasks</p>
<p><strong>Instruction Tuning</strong>: a technique of finetuning LLMs on instruction-formatted datasets</p>
<p><strong>Instruction Tuning 3 Advantages:</strong></p>
<ol>
<li><p>better understand and respond to various human requests</p>
</li>
<li><p>zero-shot generalization to new tasks</p>
</li>
<li><p>non-experts can use natural language to interact with LLMs</p>
<blockquote>
<p><em>The most popular programming language in the future will be English.</em></p>
</blockquote>
</li>
</ol>
<p><strong>Instruction Tuning &amp; typical learning paradigms.</strong> A: large-scale task-specific training data; B: zero-shot performance is still quite average</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-2.png"></p>
<p><strong>Extend from Unimodality to Multimodality</strong>. Predict the next token of the response. The instruction template is flexible and subject to manual designs. It can also be generalized to multi-round instructions.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-3.png"></p>
<h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-4.png"></p>
<aside>
üëâ **Benchmark Adaptation**

</aside>

<p><strong>Instruction templates for VQA datasets</strong>. &lt;image&gt; and {Question} are the image and the question in the original VQA datasets, respectively. Utilize existing benchmark datasets to construct instruction-formatted datasets.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-5.png"></p>
<p><strong>Directly using concise answers may limit the output length of  MLLM</strong>.</p>
<ul>
<li>Modify Instructions: ChatBridge, InstructBLIP</li>
<li>Extend the length of existing answers: $ùëÄ^3 ùêºùëá$</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-6.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-7.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-8.png"></p>
<aside>
üëâ **Self-Instruction**

</aside>

<p><strong>Self-Instruction</strong>: To meet real-world human needs, such as multiple rounds of conversations.</p>
<ul>
<li><strong>LLaVA-Instruct-150k</strong>:  an M-IT dataset</li>
<li><strong>MiniGPT-4, ChatBridge, GPT4Tools, DetGPT</strong>: develop different M-IT datasets catering for different needs</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-9.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-10.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-11.png"></p>
<aside>
üëâ **Hybrid Composition**

</aside>

<p>M-IT data + language-only user-assistant conversation data.</p>
<p><strong>Adavantage</strong>: conversational proficiencies, instruction-following abilities</p>
<p><strong>Representative work</strong>:</p>
<ul>
<li><strong>LaVIN</strong>: directly constructs a minibatch by randomly sampling from both data</li>
<li><strong>MultiInstruct</strong>: two transfer learning strategies<ul>
<li>Mixed instruction tuning: combine both types of data and randomly shuffle. (The empirical results show that mixed instruction tuning is at least not worse than solely tuning on multimodal data.)</li>
<li>Sequential instruction tuning: text data followed by multimodal data</li>
</ul>
</li>
</ul>
<h3 id="Modality-Bridging"><a href="#Modality-Bridging" class="headerlink" title="Modality Bridging"></a>Modality Bridging</h3><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-12.png"></p>
<aside>
üëâ **Learnable Interface**

</aside>

<p><strong>Adavantages</strong>: little cost, avoid catastrophic forgetting</p>
<p><strong>Three manners</strong>:</p>
<ol>
<li><strong>Query-Based</strong>: (learnable query tokens) Flamingo, BLIP-2</li>
<li><strong>Projection-Based</strong>: LLaVA, MedVInTTE</li>
<li><strong>Parameter-Efficient Tuning</strong>: LLaMA-Adapter, LaVIN</li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-13.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-14.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-15.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-16.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-17.png"></p>
<aside>
üëâ **Expert Model**

</aside>

<p><strong>Advantage</strong>: Via expert models, convert multimodal inputs into languages without training.</p>
<p><strong>Disadvantage</strong>: less flexible, info loss</p>
<p><a href="https://arxiv.org/pdf/2305.06355.pdf">**VideoChat-Text</a>:** transforming videos into textual descriptions distorts spatial-temporal relationships.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-18.png"></p>
<h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h3><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-19.png"></p>
<aside>
üëâ **Closed-set**

</aside>

<p><strong>Closed-set questions</strong>: its possible answer options are predefined and limited</p>
<p><strong>Two evaluation types</strong>:</p>
<ol>
<li><strong>leverage existing datasets</strong>: limited to a small range of tasks (e.g., acc for VQA, CIDEr score for text2caption). Two evaluation settings:<ol>
<li><strong>Zero-shot</strong>: split into held-in&#x2F;held-out parts</li>
<li><strong>Finetuning</strong>: domain-specific downstream tasks</li>
</ol>
</li>
<li><strong>develop new benchmarks specially designed for MLLMs</strong>: more comprehensive<ol>
<li><a href="https://arxiv.org/pdf/2306.13394.pdf"><strong>MME</strong></a>: 14 perception and cognition tasks, manually design instruction-answer pairs to avoid data leakage</li>
<li><strong><a href="https://arxiv.org/pdf/2306.06687.pdf">LAMM-Benchmark</a></strong>: various 2D&#x2F;3D vision tasks</li>
<li><strong><a href="https://arxiv.org/pdf/2306.05424.pdf">Video-ChatGPT</a></strong>: (1) video-based generative performance (2) zero-shot QA</li>
</ol>
</li>
</ol>
<aside>
üëâ **Open-set**

</aside>

<p><strong>Open-set questions</strong>: its possible answer options are open and flexible.</p>
<p>MLLMs act as chatbots and the chat content can be arbitrary.</p>
<ol>
<li><strong>Manual scoring</strong>: require humans to assess specific dimensions</li>
<li><strong>GPT scoring</strong>:  rate with GPT, reduce human labour (LLaVA)<ul>
<li>Multimodal interface of GPT-4 is not publicly available. Therefore, setting GPT-4 as the performance upper bound is not perfect.</li>
</ul>
</li>
<li><strong>Case study</strong>: (mPLUG-Owl, Video-LLaMA)</li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-20.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-21.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-22.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-23.png"></p>
<aside>
üëâ **Others**

</aside>

<p><strong><a href="https://arxiv.org/pdf/2212.10773.pdf">MultiInstruct</a></strong>: sensitivity metric, model‚Äôs robustness to varied instructions</p>
<p><strong><a href="https://arxiv.org/pdf/2305.10355.pdf">POPE</a></strong>: delve into the object hallucination problem</p>
<p><strong><a href="https://arxiv.org/pdf/2305.16934.pdf">AttackVLM</a></strong>: (safety) the robustness of MLLMs to adversarial attacks</p>
<h2 id="M-ICL-Multimodal-In-Context-Learning"><a href="#M-ICL-Multimodal-In-Context-Learning" class="headerlink" title="M-ICL: Multimodal In-Context Learning"></a>M-ICL: Multimodal In-Context Learning</h2><p><strong>ICL</strong>: few-shot, training-free</p>
<p><strong>M-ICL</strong>: Based on M-IT, at inference time, M-ICL adds a demonstration set (i.e., a set of in-context samples), to the original sample.</p>
<p><strong>Template example of an M-ICL query</strong>:</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-24.png"></p>
<aside>
üëâ **Two Scenarios**

</aside>

<p><strong>Scenario 1: solving various visual reasoning tasks</strong></p>
<p>(<a href="https://arxiv.org/pdf/2303.11381.pdf">**MM-React</a>, <a href="https://arxiv.org/pdf/2305.03726.pdf">Otter</a>, <a href="https://arxiv.org/pdf/2204.14198.pdf">Flamingo</a>, <a href="https://arxiv.org/pdf/2109.05014.pdf">PICa</a>, <a href="https://arxiv.org/pdf/2106.13884.pdf">Frozen</a>**)</p>
<ul>
<li>Via demonstrations, LLMs get a sense of what the task is doing and what the output template is.</li>
<li>Thus LLMs learn from a few task-specific examples and generalize to a new but similar question.</li>
</ul>
<p><strong>Scenario 2: teaching LLMs to use external tools</strong></p>
<p>(<strong><a href="https://arxiv.org/pdf/2304.09842.pdf">Chameleon</a>, <a href="https://arxiv.org/pdf/2211.11559.pdf">Visual programming</a>, <a href="https://arxiv.org/pdf/2303.17580.pdf">HuggingGPT</a></strong>)</p>
<ul>
<li>Demonstrations are often text-only and more fine-grained.</li>
<li>They typically comprise a chain of steps that could be sequentially executed to fulfill the task. (related to CoT)</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-25.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-26.png"></p>
<h2 id="M-CoT-Multimodal-Chain-of-Thought"><a href="#M-CoT-Multimodal-Chain-of-Thought" class="headerlink" title="M-CoT: Multimodal Chain of Thought"></a>M-CoT: Multimodal Chain of Thought</h2><p><strong>CoT</strong>: a series of intermediate reasoning steps</p>
<p><strong>CoT Scenario</strong>: complex reasoning</p>
<p><strong>CoT Main Idea</strong>: output both reasoning process and final answer</p>
<p><strong>M-CoT</strong> (<strong><a href="https://arxiv.org/pdf/2201.11903.pdf">CoT-PT</a>, <a href="https://arxiv.org/pdf/2302.00923.pdf">MM-CoT</a>, <a href="https://arxiv.org/pdf/2305.02317.pdf">VCoT</a>, <a href="https://arxiv.org/pdf/2305.13903.pdf">VideoCoT</a></strong>)</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-27.png"></p>
<p><strong>Modality Bridging</strong></p>
<ul>
<li><p><strong>Expert Model</strong>: transforming visual input into textual descriptions (<a href="https://arxiv.org/pdf/2209.09513.pdf">ScienceQA</a>: concatenate image captions and original textual imput to LLMs)</p>
</li>
<li><p><strong>Learnable Interface</strong>: feature fusion (<a href="https://arxiv.org/pdf/2302.00923.pdf">MM-CoT</a>: adopts a two-stage framework)</p>
<p>  <img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-28.png"></p>
</li>
</ul>
<p><strong>Learning Paradigms</strong></p>
<ul>
<li><strong>Finetuning</strong>: often for specific datasets(e.g., <a href="https://arxiv.org/pdf/2209.09513.pdf">ScienceQA</a>)</li>
<li><strong>Few-shot</strong>: requires hand-crafting some in-context examples</li>
<li><strong>Zero-shot</strong>: require no specific example</li>
</ul>
<p><strong>Chain Config‚Äî‚Äî</strong>(When to stop the chains?)</p>
<ul>
<li><strong>Adaptive</strong>: requires LLMs to decide when to halt the reasoning chains</li>
<li><strong>Pre-defined</strong>: stops the chains with a pre-defined length</li>
</ul>
<p><strong>Generation Pattens‚Äî‚Äî</strong>(How the chain is constructed?)</p>
<p>The generated steps should be consistent and correct.</p>
<ul>
<li><p><strong>Predicting</strong>: extending the reasoning chains given conditions such as instructions and previous reasoning history</p>
</li>
<li><p><strong>Infilling</strong>: deducing steps between surrounding context (previous and following steps) to fill the logical gaps (VideoCoT)</p>
<p>  <img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-29.png"></p>
</li>
</ul>
<h2 id="LAVR-LLM-Aided-Visual-Reasoning"><a href="#LAVR-LLM-Aided-Visual-Reasoning" class="headerlink" title="LAVR: LLM-Aided Visual Reasoning"></a>LAVR: LLM-Aided Visual Reasoning</h2><p><strong>LAVR</strong>: use external tools or visual foundation models.</p>
<p><strong>Three advantages</strong> compared with conventional visual reasoning models: <strong>Strong generalization, Emergent abilities, Better interactivity and control</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-30.png"></p>
<aside>
üëâ **Training Paradigms**

</aside>

<p><a href="https://arxiv.org/pdf/2305.18752.pdf">GPT4Tools</a>: generate a new tool-related instruction dataset</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-31.png"></p>
<aside>
üëâ **Functions**

</aside>

<p><em>The primary roles that LLMs play in LAVR.</em></p>
<ul>
<li><strong>Controller</strong>: (single-round) breaks down a complex task [via CoT], assign subtasks to right tools&#x2F;modules</li>
<li><strong>Decision Maker</strong>: (multi-round) summarize history and current info, decide whether the info is sufficient to finish the task, friendly present answer</li>
<li><strong>Semantics Refiner</strong>: integrate info into fluent scentences, generate texts according to different specific needs</li>
</ul>
<h2 id="Challenges-and-Future-Directions"><a href="#Challenges-and-Future-Directions" class="headerlink" title="Challenges and Future Directions"></a>Challenges and Future Directions</h2><ol>
<li><p><strong>Perception Capabilities</strong>: compromise between info capacity and computation burden</p>
</li>
<li><p><strong>Consistent Reasoning Chain</strong>: right reasoning chain delivers right answer</p>
</li>
<li><p><strong>Instruction-following Abilities</strong>: (e.g., yes&#x2F;no type)</p>
</li>
<li><p><strong>Object Hallucination</strong>: more fine-grained modality alignment</p>
<p> <img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-32.png"></p>
<p> (The object does not appear in the image)</p>
</li>
<li><p><strong>Parameter-Efficient Training</strong></p>
</li>
</ol>
]]></content>
      <categories>
        <category>Paper</category>
        <category>2023</category>
        <category>06</category>
      </categories>
      <tags>
        <tag>Multi-modal</tag>
        <tag>LLM</tag>
        <tag>Instruction-Following</tag>
        <tag>In-Context Learning</tag>
        <tag>Chain of Thought</tag>
        <tag>Tool Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Home</title>
    <url>/2023/06/08/home/</url>
    <content><![CDATA[<p>I am currently a master student at <a href="https://www.ecnu.edu.cn/">ECNU</a>. My recent research interest mainly focuses on multi-modal learning (especially combined with LLMs). You can <a href="mailto:ifshine_lx@163.com">email</a> me for further communication.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/home.jpg"></p>
]]></content>
  </entry>
  <entry>
    <title>How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources</title>
    <url>/2023/06/15/T%C3%BClu/</url>
    <content><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/open-instruct-1.jpg" alt="authors">Paper: <a href="https://arxiv.org/pdf/2306.04751.pdf">https://arxiv.org/pdf/2306.04751.pdf</a></p>
<p>Code: <a href="https://github.com/allenai/open-instruct">https://github.com/allenai/open-instruct</a></p>
<blockquote>
<p>Evaluation for instruction-tuned models remains inconsistent and difficult. Therefore, this work covers extensive evaluations on a large range of models and datasets.</p>
</blockquote>
<p><em><strong>When reading this note, you can think about the following questions:</strong></em></p>
<ol>
<li>What instruction datasets, pretrained models and <strong>evaluation metrics</strong> are used?</li>
<li>What are the evaluation results?</li>
</ol>
<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><h3 id="Instruction-Tuning"><a href="#Instruction-Tuning" class="headerlink" title="Instruction Tuning"></a>Instruction Tuning</h3><p><strong>Definition</strong>: finetuning pretrained LMs to better understand and respond to various human requests that are expressed in natural language.</p>
<p><strong>Advantages</strong>: (1) zero-shot generalization to new tasks; (2) non-experts can use natural language to interact with LLMs.</p>
<blockquote>
<p>The most popular programming language in the future will be English.</p>
</blockquote>
<p><strong>Training Paradigms</strong>: (1) supervised learning(demonstrations); (2) reinforcement learning (feedback data)</p>
<p><strong>Key Components</strong>: (1) pretrained LMs; (2) instruction datasets(diversity, task num)</p>
<h3 id="Evaluation-Method"><a href="#Evaluation-Method" class="headerlink" title="Evaluation Method"></a>Evaluation Method</h3><p><strong>Benchmark-based evaluation</strong></p>
<ul>
<li><a href="https://arxiv.org/abs/2211.09110">HELM</a>, <a href="https://doi.org/10.5281/zenodo.5371628">LM Evaluation Harness</a>: suitable for various NLP models</li>
<li><a href="https://arxiv.org/pdf/2210.11416.pdf">Flan-T5 work</a>: focus on factuality and reasoning abilities</li>
<li><a href="https://arxiv.org/abs/2303.08774">GPT-4</a>, <a href="https://arxiv.org/abs/2305.10403">PaLM v2</a>: proprietary models with comprehensive evaluations</li>
</ul>
<p><strong>Open-ended instruction-following ability evaluation</strong></p>
<ul>
<li><a href="https://arxiv.org/abs/2305.14387">Alpaca Farm</a>: leverage other models as annotators for judging model generations</li>
<li><a href="https://lmsys.org/blog/2023-05-03-arena/">Chatbot Arena</a>: leverage humans</li>
</ul>
<p>This work involves traditional benchmarks, model-based evaluation, and human-based evaluation.</p>
<h2 id="Training-Models-with-Various-Datasets"><a href="#Training-Models-with-Various-Datasets" class="headerlink" title="Training Models with Various Datasets"></a>Training Models with Various Datasets</h2><h3 id="Datasets-and-Format-Unity"><a href="#Datasets-and-Format-Unity" class="headerlink" title="Datasets and Format Unity"></a>Datasets and Format Unity</h3><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/open-instruct-2.png"> <strong>Datasets</strong>: Only CoT and Code-Alpaca are built for specific skills. <a href="https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/tree/main/HTML_cleaned_raw_dataset">ShareGPT</a> is a collection of user interactions with various chat systems publicly shared.</p>
<p><strong>Human data mixture</strong>: FLAN V2, CoT, Dolly, Open Assistant 1</p>
<p><strong>Human+GPT data mixture</strong>: Human data mixture + GPT4-Alpaca, Code-Alpaca, ShareGPT</p>
<p><strong>Format Unity</strong>: It aims at representing arbitrary rounds as one sentence.</p>
<ul>
<li>$N$: instance num in a dataset</li>
<li>$i$: round num in each example</li>
<li>${(x_1^j, y_1^j,x_2^j, y_2^j,‚Ä¶,x_i^j, y_i^j)}_{j&#x3D;1}^N$: an instruction dataset</li>
</ul>
<h3 id="Models-Training"><a href="#Models-Training" class="headerlink" title="Models Training"></a>Models Training</h3><ul>
<li>$X:{(x_1^j, x_2^j,‚Ä¶,x_i^j)}_{j&#x3D;1}^N$</li>
<li>$Y:{(y_1^j, y_2^j,‚Ä¶,y_i^j)}_{j&#x3D;1}^N$</li>
<li>$t_n$: the $n$-th input token(belonging to X or Y)</li>
<li>loss function $L&#x3D;-\sum\limits_n \log p_{\theta}(t_n|t_{&lt;n})\times\left{\begin{array}{}1 &amp; if\space t_n\in Y \ 0 &amp; otherwise\end{array}\right.$</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># hyperparams</span><br><span class="line">max_seq_len: 1024 for 30B and 65B, 2048 for others</span><br><span class="line">epoch: 2</span><br><span class="line">learning rate: 1e-5 for 30B and 65B, 2e-5 for others. (linear decay and linear warmup only used for 3% of total steps)</span><br></pre></td></tr></table></figure>

<p><strong>T√ºlu</strong>: a suite of 7B to 65B LLaMA models fully-instruction-tuned on Human+GPT data mixture.</p>
<h2 id="Evaluation-Setup"><a href="#Evaluation-Setup" class="headerlink" title="Evaluation Setup"></a>Evaluation Setup</h2><p>Load models using <strong><a href="https://arxiv.org/pdf/2208.07339.pdf">8-bit mode</a></strong> provided in the Huggingface Transformers library.</p>
<p>When doing generation, we use greedy decoding and a max length of 512 tokens, unless otherwise specified.</p>
<h3 id="Facets-of-Evaluation"><a href="#Facets-of-Evaluation" class="headerlink" title="Facets of Evaluation"></a>Facets of Evaluation</h3><p><strong>(1) Specific model capabilities</strong>: </p>
<ul>
<li><strong>Factual knowledge</strong>: <a href="https://arxiv.org/abs/2009.03300">MMLU</a>. [<a href="https://github.com/hendrycks/test">Its official evaluation scripts and prompts</a>]. Modify it to allow for batch processing. We evaluate using 0 and 5 few-shot examples, following the original setup of MMLU.</li>
<li><strong>Reasoning</strong>. We evaluate with and without chain-of-thought (CoT vs Direct). Subsample GSM and BBH to a reasonable size to improve the efficiency of doing CoT reasoning.<ul>
<li><a href="https://arxiv.org/abs/2110.14168">GSM</a>(test split) for mathematical reasoning capabilities(8-shot). Because all answers in GSM are numbers, we extract the last number in the model response as the final answer. Sampled 200 from the 1319 examples.</li>
<li><a href="https://arxiv.org/abs/2210.09261">BBH</a> for general reasoning capabilities(3-shot). For the CoT setup, we extract the first word after the phrase ‚ÄòSo the answer is‚Äô, or the entire response if there is no such substring present.</li>
</ul>
</li>
<li><strong>Multilinguality</strong>: <a href="https://arxiv.org/abs/2003.05002">TyDiQA</a> for multilingual QA. Follow <a href="https://arxiv.org/pdf/2305.10403.pdf">PaLM 2</a>‚Äòs setup. We use the gold-passage setup where one passage containing the reference answer is given.</li>
<li><strong>Coding</strong>: <a href="https://arxiv.org/abs/2107.03374">Codex-Eval(HumanEval)</a> for abilities of generating functionally correct programs from docstrings. Following the original paper, we compute unbiased estimates of pass@k to measure the functional correctness of models‚Äô outputs. We report both pass@1 and pass@10. The pass@1 results were obtained by sampling with a temperature of 0.1 and the pass@10 results with a temperature of 0.8.</li>
</ul>
<p><strong>(2) Open-ended instruction following</strong>: model-based evaluation and human evaluation</p>
<h3 id="Model-Based-Evaluation-using-GPT-4"><a href="#Model-Based-Evaluation-using-GPT-4" class="headerlink" title="Model-Based Evaluation using GPT-4"></a>Model-Based Evaluation using GPT-4</h3><p><strong>Dataset</strong>: Use a test set of 805 instructions.</p>
<p><strong>Code</strong>: Adopt <a href="https://arxiv.org/abs/2305.14387">AlpacaFarm</a>‚Äòs code, but slightly alter prompts to fit our message format.</p>
<p><strong>A GPT-4 annotator(‚Äògreedy_gpt4‚Äô)</strong>: Compare the testing model with Davinci-003.</p>
<p><strong>Win-rate</strong>: The percentage of model generations that GPT-4 reports as being preferred over the generations from Davinci-003.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Hyperparameters</span><br><span class="line">temperature: 0</span><br><span class="line">batch: 5 (reduce it if the 5 examples exceed the 8192 token context window limit)</span><br><span class="line">max_output_token_len: extended from 300 to 2048 (in order to avoid cut-off generations)</span><br></pre></td></tr></table></figure>

<h3 id="Human-Evaluation"><a href="#Human-Evaluation" class="headerlink" title="Human Evaluation"></a>Human Evaluation</h3><p>The model information is anonymized and their outputs are put in random order.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/open-instruct-8.jpg"></p>
<p><strong>Use 332 instructions</strong>: 252 from <a href="https://arxiv.org/abs/2212.10560">Self-Instruct</a> and 80 from <a href="https://lmsys.org/blog/2023-03-30-vicuna/">Vicuna</a> evaluation set.</p>
<p><strong>Compare 3 pairs of models</strong>: (1) T√úLU 65B vs ChatGPT (2) T√úLU 65B vs T√úLU 7B (3) T√úLU 65B vs a 65B LLaMA model trained on the Human data mixture.</p>
<p><strong>Interface for human judgements</strong>:</p>
<ul>
<li><strong>Indivisual acceptability</strong>: ‚Äúyes‚Äù&#x2F;‚Äúno‚Äù, a 2-way decision. For ‚Äúyes‚Äù, if and only if the response answered the request in the query, had no significant errors, and did not have repetitive information.</li>
<li><strong>Pairwise preference</strong>: ‚ÄúA is clearly better‚Äù&#x2F;‚ÄúA is slightly better‚Äù&#x2F;‚ÄúTie‚Äù&#x2F;‚ÄúB is slightly better‚Äù&#x2F;‚ÄúB is clearly better‚Äù, a 5-way decision, select which one is more helpful.</li>
</ul>
<p><strong>Recruited 18 expert annotators</strong>, which are researchers at AI2 or students at UW for the annotation. All these annotators are fluent English speakers and hold bachelor‚Äôs degrees or above. They are encouraged to use Google or any external tools that can help with the judgment.</p>
<p><strong>Inter-Annotator Agreement</strong>:</p>
<ul>
<li>We measure the agreement of our annotators on a subset of <strong>119 examples</strong> (63 instances randomly sampled from the ChatGPT3 vs T√úLU 65B comparison, and 59 instances randomly sampled from the T√úLU 65B vs T√úLU 7B comparison).</li>
<li><strong>Indivisual acceptability</strong>: an agreement of 0.84.</li>
<li><strong>Pairwise preference</strong>: an agreement of 0.72. Following <a href="https://arxiv.org/pdf/2305.11206.pdf">Lima</a>, we report a tie-discounted accuracy, which assigns one point if both annotators agreed, half a point if either annotator (but not both) labeled a tie, and zero point otherwise. We also merged ‚Äúclearly better‚Äù and ‚Äúslightly better‚Äù together, so our final options will be simply comparing which of A and B is better, or a tie.</li>
</ul>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><blockquote>
<p>The best model in any given evaluation reaches on average 83% of ChatGPT performance, and 68% of GPT-4 performance.</p>
</blockquote>
<h3 id="Analysis-of-Instruction-Tuning-Datasets-and-Base-Models"><a href="#Analysis-of-Instruction-Tuning-Datasets-and-Base-Models" class="headerlink" title="Analysis of Instruction Tuning Datasets and Base Models"></a>Analysis of Instruction Tuning Datasets and Base Models</h3><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/open-instruct-3.jpg"> <img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/open-instruct-4.jpg"> ‚Äú1.4T‚Äù means $1.4\times 10^{12}$ tokens are used to train the model. ‚Äú180B‚Äù means $180\times 10^{9}$</p>
<ul>
<li>There is not a single best instruction tuning dataset across all tasks</li>
<li>Combining datasets results in the best overall performance on the benchmark tasks</li>
<li>Base model quality is extremely important for downstream performance</li>
</ul>
<h3 id="Pushing-the-Limits-of-Open-Models"><a href="#Pushing-the-Limits-of-Open-Models" class="headerlink" title="Pushing the Limits of Open Models"></a>Pushing the Limits of Open Models</h3><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/open-instruct-5.jpg"></p>
<ul>
<li>Instrcution tuning brings large benefits on top of LLaMA models at all sizes</li>
<li>Smaller models benefit most from instruction tuning</li>
<li>T√úLU still lags behind SOTA proprietary models</li>
</ul>
<h3 id="Model-Based-x2F-Human-Evaluation-Results-for-Open-ended-Generation"><a href="#Model-Based-x2F-Human-Evaluation-Results-for-Open-ended-Generation" class="headerlink" title="Model-Based&#x2F;Human Evaluation Results for Open-ended Generation"></a>Model-Based&#x2F;Human Evaluation Results for Open-ended Generation</h3><p><strong>Model-Based Evaluation</strong><br><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/open-instruct-6.jpg"></p>
<ul>
<li>Models trained on mixtures based on traditional NLP datasets perform poorly</li>
<li>Datasets that encourage long, diverse generations perform best</li>
<li>ShareGPT performs best</li>
</ul>
<blockquote>
<p>The judge model(has bias) may not always reveal the testing model deficiencies.</p>
</blockquote>
<p><strong>Human Evaluation</strong><br><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/open-instruct-7.jpg"></p>
<ul>
<li>Human evaluation results largely correlate with the AlpacaFarm and benchmark-based evaluation</li>
<li>Making use of distilled datasets provides a large performance boost</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p><em><strong>Future Work</strong></em></p>
<ul>
<li>explore instruction-tuning methods that use reinforcement learning</li>
<li>explore more recent strong base models and other instruction datasets</li>
<li>design more versatile model(generality)<ul>
<li>better dataset mixing</li>
<li>instruction-tuning modular models (e.g., <a href="https://arxiv.org/abs/1701.06538">mixture-of-experts</a>)</li>
</ul>
</li>
<li>improve the reliability and scalability of human evaluation for instruction-following models</li>
</ul>
<p><em><strong>Limitations</strong></em></p>
<ul>
<li>Small proportions of data may contain personally identifying details, but this work does not remove them, which may produce toxic or harmful generations.</li>
<li>Not include evaluations of multi-turn dialogue and summarization abilities</li>
</ul>
<p><em><strong>Broader Impact</strong></em></p>
<p>Training and releasing large instruction-tuned models need sufficient testing to limit potential harm.</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>2023</category>
        <category>06</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Instruction-Following</tag>
        <tag>Evaluation</tag>
      </tags>
  </entry>
  <entry>
    <title>[Waiting...]Hexo Usage</title>
    <url>/2023/06/08/hexo-usage/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask for help on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>. Win11 is the default OS in this post.</p>
<h2 id="Hexo-Install-6-3-0"><a href="#Hexo-Install-6-3-0" class="headerlink" title="Hexo Install(6.3.0)"></a>Hexo Install(6.3.0)</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install -g hexo-cli <span class="comment"># cmd with administrator permissions</span></span><br><span class="line">hexo -v <span class="comment"># check whether the installation is successful</span></span><br><span class="line"><span class="built_in">mkdir</span> &lt;root_dir&gt; <span class="comment"># create an empty dir</span></span><br><span class="line"><span class="built_in">cd</span> &lt;root_dir&gt;</span><br><span class="line">hexo init</span><br><span class="line">hexo s <span class="comment"># run server</span></span><br></pre></td></tr></table></figure>
<h2 id="Next-Theme-8-17-0"><a href="#Next-Theme-8-17-0" class="headerlink" title="Next Theme(8.17.0)"></a>Next Theme(8.17.0)</h2><p>Use <a href="https://github.com/next-theme/hexo-theme-next">hexo-theme-next</a> as an example. More info: <a href="https://theme-next.js.org/docs">Theme Next Doc</a>, <a href="http://t.csdn.cn/Tu6fy">CSDN blog 1</a>, <a href="http://t.csdn.cn/EmYFJ">CSDN blog 2</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/next-theme/hexo-theme-next themes/next</span><br><span class="line"><span class="comment"># open root_dir/_config.yml, replace &quot;theme: landscape&quot; with &quot;theme: next&quot;</span></span><br></pre></td></tr></table></figure>

<h2 id="Deploy-to-Github"><a href="#Deploy-to-Github" class="headerlink" title="Deploy to Github"></a>Deploy to Github</h2><p>add ‚Äú.gitignore‚Äù file to blog root dir:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">.DS_Store</span><br><span class="line">Thumbs.db</span><br><span class="line">db.json</span><br><span class="line">*.log</span><br><span class="line">node_modules/</span><br><span class="line">public/</span><br><span class="line">.deploy*/</span><br><span class="line">_multiconfig.yml</span><br></pre></td></tr></table></figure>
<p>open cmd, then cd blog root dir (win10)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install hexo-deployer-git --save <span class="comment"># install a plugin</span></span><br><span class="line">git config --global user.email <span class="string">&quot;you@example.com&quot;</span></span><br><span class="line">git config --global user.name <span class="string">&quot;Your Name&quot;</span></span><br></pre></td></tr></table></figure>
<p>open root_dir&#x2F;_config.yml, modify ‚Äúdeploy‚Äù</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: git@github.com:ifshinelx/ifshinelx.github.io.git</span><br><span class="line">  branch: main</span><br></pre></td></tr></table></figure>
<p>deploy (After the cmd execution, it takes several minutes for the github page to refresh)<br>For the first deployment, you need to click <a href="http://ifshinelx.github.io/ifshinelx.github.io">http://ifshinelx.github.io/ifshinelx.github.io</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo clean <span class="comment"># clean cache</span></span><br><span class="line">hexo g <span class="comment"># generate static files</span></span><br><span class="line">hexo d <span class="comment"># deploy to remote sites</span></span><br></pre></td></tr></table></figure>

<span id="more"></span>

<h2 id="Personalization"><a href="#Personalization" class="headerlink" title="Personalization"></a>Personalization</h2><h3 id="Hexo-Basic-Info-config-yml"><a href="#Hexo-Basic-Info-config-yml" class="headerlink" title="Hexo Basic Info(_config.yml)"></a>Hexo Basic Info(_config.yml)</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">title: XinLiu&#x27;s Homepage, Welcome!</span><br><span class="line">subtitle: &#x27;&#x27;</span><br><span class="line">description: &#x27;&#x27;</span><br><span class="line">keywords:</span><br><span class="line">author: Xin Liu</span><br><span class="line">language: en</span><br><span class="line">timezone: &#x27;Asia/Shanghai&#x27;</span><br><span class="line"></span><br><span class="line">url: https://ifshinelx.github.io</span><br><span class="line">math:</span><br><span class="line">  every_page: false</span><br><span class="line">  mathjax:</span><br><span class="line">    enable: true</span><br></pre></td></tr></table></figure>

<h3 id="NexT-Theme-Settings-Basic"><a href="#NexT-Theme-Settings-Basic" class="headerlink" title="NexT Theme Settings Basic"></a><a href="https://theme-next.js.org/docs/theme-settings/">NexT Theme Settings Basic</a></h3><p>root_dir&#x2F;themes&#x2F;next&#x2F;_config.yml<br>add ‚Äúlittle star.jpg‚Äù to root_dir&#x2F;themes&#x2F;next&#x2F;source&#x2F;images&#x2F;</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cache:</span><br><span class="line">  enable: true</span><br><span class="line"></span><br><span class="line"># Remove unnecessary files after hexo generate.</span><br><span class="line">minify: true</span><br><span class="line"></span><br><span class="line">scheme: Gemini</span><br><span class="line">favicon:</span><br><span class="line">  small: /images/little star.jpg</span><br><span class="line">  medium: /images/little star.jpg</span><br><span class="line">  # small: /images/favicon-16x16-next.png</span><br><span class="line">  # medium: /images/favicon-32x32-next.png</span><br><span class="line">creative_commons:</span><br><span class="line">  size: small</span><br><span class="line">  sidebar: true</span><br><span class="line">  post: true</span><br><span class="line">  language: deed.en</span><br><span class="line">menu:</span><br><span class="line">  home: / || fa fa-home</span><br><span class="line">  archives: /archives/ || fa fa-archive</span><br><span class="line">menu_settings:</span><br><span class="line">  icons: true</span><br><span class="line">  badges: true</span><br></pre></td></tr></table></figure>
<p>More Info: <a href="https://theme-next.js.org/docs/third-party-services/math-equations.html">Math Equations</a>, <a href="https://theme-next.js.org/docs/tag-plugins/label.html">Label</a>, <a href="https://theme-next.js.org/docs/tag-plugins/note.html">Note</a>, <a href="https://theme-next.js.org/docs/tag-plugins/tabs.html">Tabs</a></p>
<h3 id="Sidebar"><a href="#Sidebar" class="headerlink" title="Sidebar"></a><a href="https://theme-next.js.org/docs/theme-settings/sidebar.html">Sidebar</a></h3><p>NexT config file</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sidebar:</span><br><span class="line">  position: right</span><br><span class="line">avatar:</span><br><span class="line">  url: /images/little star.jpg #/images/avatar.gif</span><br><span class="line">social:</span><br><span class="line">  GitHub: https://ifshinelx.github.io || fab fa-github</span><br><span class="line">  E-Mail: mailto:ifshine_lx@163.com || fa fa-envelope</span><br><span class="line">recent_posts_title: Recent Posts</span><br><span class="line">recent_posts_layout: block</span><br><span class="line">recent_posts: true</span><br></pre></td></tr></table></figure>
<p>In root_dir&#x2F;themes&#x2F;next&#x2F;layout&#x2F;_partials&#x2F;sidebar&#x2F;site-overview.njk, add the code block:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;%- if theme.social %&#125;</span><br><span class="line">...</span><br><span class="line">&#123;%- endif %&#125;</span><br><span class="line"></span><br><span class="line">&#123;# recent posts #&#125;</span><br><span class="line">&#123;% if theme.recent_posts %&#125;</span><br><span class="line">  &lt;div class=&quot;links-of-blogroll motion-element &#123;&#123; &quot;links-of-blogroll-&quot; + theme.recent_posts_layout  &#125;&#125;&quot;&gt;</span><br><span class="line">    &lt;div class=&quot;links-of-blogroll-title&quot;&gt;</span><br><span class="line">      &lt;!-- modify icon to fire by szw --&gt;</span><br><span class="line">      &lt;i class=&quot;fa fa-history fa-&#123;&#123; theme.recent_posts_icon | lower &#125;&#125;&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;</span><br><span class="line">      &#123;&#123; theme.recent_posts_title &#125;&#125;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">    &lt;ul class=&quot;links-of-blogroll-list&quot; style=&quot;padding: 0px 12px;&quot;&gt;</span><br><span class="line">      &#123;% set posts = site.posts.sort(&#x27;-date&#x27;) %&#125;</span><br><span class="line">      &#123;% set recent_posts = posts.slice(0, 5).toArray() %&#125;</span><br><span class="line">      &#123;% for post in recent_posts %&#125;</span><br><span class="line">        &#123;% if post.title != &quot;Home&quot; %&#125;</span><br><span class="line">          &lt;li class=&quot;recent_posts_li&quot;&gt;</span><br><span class="line">            &lt;a href=&quot;&#123;&#123; url_for(post.path) &#125;&#125;&quot; title=&quot;&#123;&#123; post.title &#125;&#125;&quot; target=&quot;_blank&quot;&gt;&#123;&#123;date(post.date, &#x27;MM-DD&#x27;) &#125;&#125; &#123;&#123; post.title &#125;&#125; &lt;/a&gt;</span><br><span class="line">          &lt;/li&gt;</span><br><span class="line">        &#123;% endif %&#125;</span><br><span class="line">      &#123;% endfor %&#125;</span><br><span class="line">    &lt;/ul&gt;</span><br><span class="line">  &lt;/div&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br><span class="line"></span><br><span class="line">&#123;%- if theme.creative_commons.license and theme.creative_commons.sidebar %&#125;</span><br><span class="line">...</span><br><span class="line">&#123;%- endif %&#125;</span><br></pre></td></tr></table></figure>
<p>In root_dir&#x2F;themes&#x2F;next&#x2F;source&#x2F;css&#x2F;main.styl, add the code block in the file end.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">li.recent_posts_li &#123;</span><br><span class="line">    text-align: left;</span><br><span class="line">    display: block;</span><br><span class="line">    word-break: keep-all;</span><br><span class="line">    white-space: nowrap;</span><br><span class="line">    overflow: hidden;</span><br><span class="line">    text-overflow: ellipsis;</span><br><span class="line"></span><br><span class="line">    &amp;:hover &#123;</span><br><span class="line">      a&#123;</span><br><span class="line">        color: #fc6423;</span><br><span class="line">        border-bottom-color: #fc6423;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h3 id="Footer"><a href="#Footer" class="headerlink" title="Footer"></a><a href="https://theme-next.js.org/docs/theme-settings/footer.html">Footer</a></h3><p>NexT config file</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">footer:</span><br><span class="line">  since: 2023</span><br><span class="line">  powered: false</span><br><span class="line">busuanzi_count:</span><br><span class="line">  enable: true</span><br></pre></td></tr></table></figure>
<p>In root_dir&#x2F;themes&#x2F;next&#x2F;layout&#x2F;_partials&#x2F;footer.njk, delete the code block:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;div class=&quot;wordcount&quot;&gt;</span><br><span class="line">...</span><br><span class="line">&lt;/div&gt;</span><br></pre></td></tr></table></figure>
<p>In root_dir&#x2F;themes&#x2F;next&#x2F;layout&#x2F;_partials&#x2F;footer.njk, add the code block:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;%- if theme.busuanzi_count.enable %&#125;</span><br><span class="line">&lt;div class=&quot;busuanzi-count&quot;&gt;</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  &#123;%- if config.symbols_count_time.total_symbols %&#125;</span><br><span class="line">  &lt;span class=&quot;post-meta-item&quot;&gt;</span><br><span class="line">    &lt;span class=&quot;post-meta-item-icon&quot;&gt;</span><br><span class="line">      &lt;i class=&quot;fa fa-chart-line&quot;&gt;&lt;/i&gt;</span><br><span class="line">    &lt;/span&gt;</span><br><span class="line">    &#123;%- if theme.symbols_count_time.item_text_total %&#125;</span><br><span class="line">      &lt;span&gt;&#123;&#123; __(&#x27;symbols_count_time.count_total&#x27;) + __(&#x27;symbol.colon&#x27;) &#125;&#125;&lt;/span&gt;</span><br><span class="line">    &#123;%- endif %&#125;</span><br><span class="line">    &lt;span title=&quot;&#123;&#123; __(&#x27;symbols_count_time.count_total&#x27;) &#125;&#125;&quot;&gt;&#123;&#123; symbolsCountTotal(site) &#125;&#125;&lt;/span&gt;</span><br><span class="line">  &lt;/span&gt;</span><br><span class="line">  &#123;%- endif %&#125;</span><br><span class="line"></span><br><span class="line">&lt;/div&gt;</span><br><span class="line">&#123;%- endif %&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Posts"><a href="#Posts" class="headerlink" title="Posts"></a><a href="https://theme-next.js.org/docs/theme-settings/posts.html">Posts</a></h3><p>a <strong>Read More</strong> button in a post: </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;!-- more --&gt;</span><br></pre></td></tr></table></figure>
<p>a plug-in</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm i hexo-word-counter --save</span><br><span class="line">hexo clean</span><br></pre></td></tr></table></figure>
<p>NexT config file</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tag_icon: true</span><br></pre></td></tr></table></figure>

<h3 id="Custom-Pages-Tags-Categories-Home"><a href="#Custom-Pages-Tags-Categories-Home" class="headerlink" title="Custom Pages(Tags, Categories, Home)"></a><a href="https://theme-next.js.org/docs/theme-settings/custom-pages.html">Custom Pages(Tags, Categories, Home)</a></h3><p><a href="https://hexo.io/docs/front-matter#Categories-amp-Tags">Hexo‚Äôs Docs of Categories &amp; Tags</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> root_dir</span><br><span class="line">hexo new page tags</span><br><span class="line">hexo new page categories</span><br></pre></td></tr></table></figure>
<p>NexT config file</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">menu:</span><br><span class="line">  tags: /tags/ || fa fa-tags</span><br><span class="line">  categories: /categories/ || fa fa-th</span><br></pre></td></tr></table></figure>

<p>root_dir&#x2F;source&#x2F;tags&#x2F;index.md</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">date: 2023-06-10 21:55:15</span><br><span class="line">type: &quot;tags&quot;</span><br><span class="line">comments: false</span><br><span class="line">---</span><br></pre></td></tr></table></figure>
<p>root_dir&#x2F;source&#x2F;categories&#x2F;index.md</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">date: 2023-06-10 21:55:25</span><br><span class="line">type: &quot;categories&quot;</span><br><span class="line">comments: false</span><br><span class="line">---</span><br></pre></td></tr></table></figure>
<p>tag color setting</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// create themes\next\layout\tag-color.njk</span><br><span class="line">&lt;script type=&quot;text/javascript&quot;&gt;</span><br><span class="line">     var alltags = document.getElementsByClassName(&#x27;tag-cloud-tags&#x27;);</span><br><span class="line">     var tags = alltags[0].getElementsByTagName(&#x27;a&#x27;);</span><br><span class="line">     for (var i = tags.length - 1; i &gt;= 0; i--) &#123;</span><br><span class="line">       var golden_ratio = 0.618033988749895;</span><br><span class="line">       var s = 0.5;</span><br><span class="line">       var v = 0.999;</span><br><span class="line">       var h = golden_ratio + Math.random()*0.8 - 0.5;</span><br><span class="line">       var h_i = parseInt(h * 6);</span><br><span class="line">       var f = h * 6 - h_i;</span><br><span class="line">       var p = v * (1 - s);</span><br><span class="line">       var q = v * (1 - f * s);</span><br><span class="line">       var t = v * (1 - (1 - f) * s);</span><br><span class="line">       var r, g, b;</span><br><span class="line">       switch (h_i) &#123;</span><br><span class="line">          case 0:</span><br><span class="line">              r = v;</span><br><span class="line">              g = t;</span><br><span class="line">              b = p;</span><br><span class="line">              break;</span><br><span class="line">          case 1:</span><br><span class="line">              r = q;</span><br><span class="line">              g = v;</span><br><span class="line">              b = p;</span><br><span class="line">              break;</span><br><span class="line">          case 2:</span><br><span class="line">              r = p;</span><br><span class="line">              g = v;</span><br><span class="line">              b = t;</span><br><span class="line">              break;</span><br><span class="line">          case 3 :</span><br><span class="line">              r = p;</span><br><span class="line">              g = q;</span><br><span class="line">              b = v;</span><br><span class="line">              break;</span><br><span class="line">          case 4:</span><br><span class="line">              r = t;</span><br><span class="line">              g = p;</span><br><span class="line">              b = v;</span><br><span class="line">              break;</span><br><span class="line">          case 5:</span><br><span class="line">              r = v;</span><br><span class="line">              g = p;</span><br><span class="line">              b = q;</span><br><span class="line">              break;</span><br><span class="line">          default:</span><br><span class="line">              r = 1;</span><br><span class="line">              g = 1;</span><br><span class="line">              b = 1;</span><br><span class="line">        &#125;</span><br><span class="line">       tags[i].style.background = &quot;rgba(&quot;+parseInt(r*255)+&quot;,&quot;+parseInt(g*255)+&quot;,&quot;+parseInt(b*255)+&quot;,&quot;+0.5+&quot;)&quot;;</span><br><span class="line">     &#125;</span><br><span class="line">&lt;/script&gt;</span><br><span class="line"></span><br><span class="line">&lt;style&gt;</span><br><span class="line">  .tag-cloud-tags&#123;</span><br><span class="line">    text-align: center;</span><br><span class="line">    counter-reset: tags;</span><br><span class="line">  &#125;</span><br><span class="line">  .tag-cloud-tags a&#123;</span><br><span class="line">    display: inline-block;</span><br><span class="line">    border: 0px;</span><br><span class="line">    border-radius: 10px;</span><br><span class="line">    padding: 0px 10px;</span><br><span class="line">    margin: 8px;</span><br><span class="line">    color: rgba(34, 34, 34, 0.8);</span><br><span class="line">    </span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  .tag-cloud-tags a:hover&#123;</span><br><span class="line">     box-shadow: 0px 5px 15px 0px rgba(0,0,0,.4);</span><br><span class="line">     transform: scale(1.1);</span><br><span class="line">     transition-duration: 0.15s;</span><br><span class="line">  &#125;</span><br><span class="line">&lt;/style&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// modify themes\next\layout\_partials\page\tags.njk</span><br><span class="line">&lt;div class=&quot;tag-cloud&quot;&gt;</span><br><span class="line">  ...</span><br><span class="line">&lt;/div&gt;</span><br><span class="line">&#123;% include &#x27;tag-color.njk&#x27; %&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// modify themes\next\layout\_macro\post.njk</span><br><span class="line">      &lt;header&gt;</span><br><span class="line">        ...</span><br><span class="line">        </span><br><span class="line">        &#123;%- if post.tags and post.tags.length %&#125;</span><br><span class="line">          &#123;%- set tag_indicate = &#x27;&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt;&#x27; if theme.tag_icon else &#x27;#&#x27; %&#125;</span><br><span class="line">          &lt;div class=&quot;post-tags&quot; style=&quot;margin-top: 5px;&quot;&gt;</span><br><span class="line">            &#123;%- for tag in post.tags.toArray() %&#125;</span><br><span class="line">              &lt;a href=&quot;&#123;&#123; url_for(tag.path) &#125;&#125;&quot; rel=&quot;tag&quot; style=&quot;border: 0px; border-radius: 10px; padding: 0px 10px;&quot;&gt;&#123;&#123; tag_indicate &#125;&#125; &#123;&#123; tag.name &#125;&#125;&lt;/a&gt;</span><br><span class="line">            &#123;%- endfor %&#125;</span><br><span class="line">          &lt;/div&gt;</span><br><span class="line">          &lt;script type=&quot;text/javascript&quot;&gt;</span><br><span class="line">              var tagsall=document.getElementsByClassName(&quot;post-tags&quot;)</span><br><span class="line">              for (var i = tagsall.length - 1; i &gt;= 0; i--)&#123;</span><br><span class="line">                  var tags=tagsall[i].getElementsByTagName(&quot;a&quot;);</span><br><span class="line">                  for (var j = tags.length - 1; j &gt;= 0; j--) &#123;</span><br><span class="line">                      var r=Math.floor(Math.random()*75+200);</span><br><span class="line">                      var g=Math.floor(Math.random()*75+200);</span><br><span class="line">                      var b=Math.floor(Math.random()*75+200);</span><br><span class="line">                      tags[j].style.background = &quot;rgb(&quot;+r+&quot;,&quot;+g+&quot;,&quot;+b+&quot;)&quot;;</span><br><span class="line">                  &#125;</span><br><span class="line">              &#125;                        </span><br><span class="line">            &lt;/script&gt;</span><br><span class="line">        &#123;%- endif %&#125;</span><br><span class="line">      &lt;/header&gt;</span><br></pre></td></tr></table></figure>

<p><strong>Home Page</strong>: root_dir&#x2F;themes&#x2F;next&#x2F;layout&#x2F;index.njk:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;% block content %&#125;</span><br><span class="line"></span><br><span class="line">  &#123;%- set postlen = page.posts.toArray().length %&#125;</span><br><span class="line">  &#123;%- set post = page.posts.toArray()[postlen-1] %&#125;</span><br><span class="line">  &#123;&#123; partial(&#x27;_macro/home.njk&#x27;, &#123;post: post, is_index: true&#125;) &#125;&#125;</span><br><span class="line"></span><br><span class="line">&#123;% endblock %&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Search-Services"><a href="#Search-Services" class="headerlink" title="Search Services()"></a><a href="https://theme-next.js.org/docs/third-party-services/search-services.html">Search Services()</a></h3><p>Details of Algolia Search are in <a href="https://theme-next.js.org/docs/third-party-services/search-services.html#Algolia-Search">here</a>.<br><a href="https://github.com/next-theme/hexo-generator-searchdb">Searchdb</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm i hexo-generator-searchdb --save</span><br></pre></td></tr></table></figure>
<p>Hexo config:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># search hexo-generator-searchdb</span><br><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br><span class="line">  content: true</span><br><span class="line">  format: html</span><br><span class="line">  limit: 100</span><br></pre></td></tr></table></figure>
<p>Next config:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">local_search:</span><br><span class="line">  enable: true</span><br></pre></td></tr></table></figure>

<h3 id="Waiting‚Ä¶-Comment-Systems-Isso"><a href="#Waiting‚Ä¶-Comment-Systems-Isso" class="headerlink" title="[Waiting‚Ä¶] Comment Systems(Isso)"></a>[Waiting‚Ä¶] <a href="https://theme-next.js.org/docs/third-party-services/comments.html">Comment Systems(Isso)</a></h3><p>Click <a href="https://www.python.org/ftp/python/3.8.10/python-3.8.10-amd64.exe">link</a> to install python3.8. You can find more versions <a href="https://www.python.org/downloads/windows/">here</a>.<br>Download <em>sqlite-dll-win64-x64-3420000.zip</em> and <em>sqlite-tools-win32-x86</em> from <a href="https://www.sqlite.org/download.html">the official website</a>. Create ‚ÄúSQlite_root_dir‚Äù and move all files from the two zips to ‚ÄúSQlite_root_dir‚Äù. Add ‚ÄúSQlite_root_dir‚Äù to the environment variable ‚ÄúPath‚Äù.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">l</span><br></pre></td></tr></table></figure>

<h3 id="Waiting‚Ä¶-Statistics-and-Analytics-Umami"><a href="#Waiting‚Ä¶-Statistics-and-Analytics-Umami" class="headerlink" title="[Waiting‚Ä¶] Statistics and Analytics(Umami)"></a>[Waiting‚Ä¶] <a href="https://theme-next.js.org/docs/third-party-services/statistics-and-analytics.html">Statistics and Analytics(Umami)</a></h3><p><a href="http://t.csdn.cn/OCEnG">MySQL</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">l</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Software</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention</title>
    <url>/2023/06/09/llama-adapter-v1/</url>
    <content><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-1.jpg" alt="authors">Paper: <a href="https://arxiv.org/pdf/2303.16199.pdf">https://arxiv.org/pdf/2303.16199.pdf</a></p>
<p>Code: <a href="https://github.com/OpenGVLab/LLaMA-Adapter">https://github.com/OpenGVLab/LLaMA-Adapter</a></p>
<p>More Info: <a href="https://github.com/Lightning-AI/lit-parrot">Lighting AI | Lit-Parrot: lightweight update of llama</a></p>
<p><em><strong>When reading this note, you can think about the following questions:</strong></em></p>
<ol>
<li>What is learnable adaption prompts?</li>
<li>What is zero-init attention?</li>
<li>How to extend LLaMA-Adapter to multi-modal input?</li>
</ol>
<hr>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><em>4 characteristics of LLaMA-Adapter:</em></p>
<ol>
<li>1.2M Parameters</li>
<li>1 Hour: 8 A100 GPUs, three times faster than Alpaca</li>
<li>Plug with Expertise: insert their respective adapters and endow LLaMA with different expert knowledge</li>
<li>Multi-modal: simply add images tokens into adaption prompts</li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1--2.jpg" alt="LLaMA-Adapter&#39;s 4 characteristics and details"> <strong>The idea of LLaMA-Adapter is model-agnostic and can be applied to other LLMs.</strong></p>
<h3 id="Learnable-Adaption-Prompts"><a href="#Learnable-Adaption-Prompts" class="headerlink" title="Learnable Adaption Prompts"></a>Learnable Adaption Prompts</h3><ul>
<li>$K$: prompt length for each layer</li>
<li>$C$: feature dim of transformer</li>
<li>$N$: total transformer layer num of LLaMA</li>
<li><strong>${ P_l }_{l&#x3D;1}^{L}$ ($P_l\in\mathbb{R}^{K\times C}$) : learnable adaption prompts</strong> for topmost $L (L\le N)$ transformer layers with higher-level semantic representations</li>
<li>$T_l\in\mathbb{R}^{M\times C}$: $M$-length word tokens in $l$-th inserted layer</li>
<li>$[P_l;\space T_l]\in\mathbb{R}^{(K+M)\times C}$: concatenate $P_l$ and $T_l$, the learned <strong>instruction knowledge</strong> in $P_l$ guides $T_l$ to generate contextual responses</li>
</ul>
<h3 id="Zero-init-Attention"><a href="#Zero-init-Attention" class="headerlink" title="Zero-init Attention"></a>Zero-init Attention</h3><p>If the adaption prompts are randomly initialized, they will bring noise to the word tokens and damage original knowledge in LLaMA at the early training stage, which harms stablity and effectiveness.</p>
<p><em>$t_l\in\mathbb{R}^{1\times C}$: generate the (M+1)-th word $t_l$ on top of $[P_l;\space T_l]$</em> at the $l$-th inserted layer</p>
<ol>
<li>linear projection: queries $Q_l&#x3D;Linear_q(t_l)$, keys $K_l&#x3D;Linear_k([P_l;T_l;t_l])$, values $V_l&#x3D;Linear_v([P_l;T_l;t_l])$</li>
<li>attention scores: $S_l&#x3D;\frac{Q_lK_l^T}{\sqrt{C}}\in\mathbb{R}^{1\times(K+M+1)}$, records the feature similarities between $t_l$ and all $K+M+1$ tokens</li>
<li>reformulation: $S_l&#x3D;[S_l^K;S_l^{M+1}]^T, S_l^K\in\mathbb{R}^{K\times 1}, S_l^{M+1}\in\mathbb{R}^{(M+1)\times 1}$<ul>
<li>$S_l^K$ represents how much information the $P_l$ contribute to $t_l$, which probably causes noise in the early training stage</li>
</ul>
</li>
<li>softmax operation: $S_l^g&#x3D;[Softmax(S_l^K)¬∑g_l;Softmax(S_l^{M+1})]^T$<ul>
<li>$g_l$ is a learnable gating factor initialized by zero, adaptively controls the importance of $S_l^K$</li>
<li>in practice, each head of attention has an independent $g_l$</li>
</ul>
</li>
<li>output of the attention layer: $t_l^o&#x3D;Linear_o(S_l^gV_l)\in\mathbb{R}^{1\times C}$</li>
</ol>
<h3 id="Multi-modal-Reasoning"><a href="#Multi-modal-Reasoning" class="headerlink" title="Multi-modal Reasoning"></a>Multi-modal Reasoning</h3><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-3.jpg"> Task textual input for ScienceQA: question + textual context + options. (We utilize ‚ÄúGenerate caption for this image‚Äù as the textual instruction input for LLaMA-Adapter)</p>
<ol>
<li>multi-scale global visual features: ${I_m}_{m&#x3D;1}^{M},I_m\in\mathbb{R}^{1\times C_m}$, $M$ is the scale num<ul>
<li>from pre-trained CLIP</li>
</ul>
</li>
<li>overall image token: $I_p&#x3D;Projection\Big(Concat\left({I_m}_{m&#x3D;1}^M\right)\Big)\in\mathbb{R}^{1\times C}$<ul>
<li>concatenate along the channel dim</li>
<li>utilize cascaded MLPs as the learnable projection network with 0.6M parameters</li>
</ul>
</li>
<li>repeat $I_p$ for $K$ times</li>
<li>multi-modal prompt: $P_l^v&#x3D;P_l+Repeat(I_p)\in\mathbb{R}^{K\times C}$</li>
</ol>
<p><strong>Future work: using pre-trained modal-specific encoders, we can integrate instructional signals of different modalities into the adaption prompts</strong></p>
<h3 id="Zero-initialized-Attention-for-other-Large-Models"><a href="#Zero-initialized-Attention-for-other-Large-Models" class="headerlink" title="Zero-initialized Attention for other Large Models"></a>Zero-initialized Attention for other Large Models</h3><p>In addition to instruction-following models, our zero-initialized attention can be generalized to other vision and language models for parameter-efficient fine-tuning.</p>
<p><strong>Vision Model</strong>. We insert the adaption prompts as prefix into the topmost L transformer layers in the pre-trained <a href="https://arxiv.org/pdf/2010.11929.pdf">ViT</a>, and modify the attention operations to be zero-initialized at all inserted layers.</p>
<p><strong>Language Model</strong>. We evaluate our fine-tuning efficacy on <a href="https://arxiv.org/pdf/1907.11692.pdf">RoBERTa</a>, and implement the zero-initialized attention on top of <a href="https://arxiv.org/pdf/2110.07602.pdf">P-tuning v2</a>, a prompt tuning method for efficiently adapting large language models. Likewise, we only enable the prompt tokens in P-tuning v2 and our zero gating factors to be learnable during fine-tuning.</p>
<p><strong>Vision-Language Model</strong>. In detail, we adopt CLIP with a ViT-B&#x2F;16 as the visual encoder and a 12-layer transformer as the textual encoder. We freeze the entire CLIP and insert the adaption prompts with zero-initialized attention into CLIP‚Äôs visual encoder.</p>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Instruction-following-Evaluation"><a href="#Instruction-following-Evaluation" class="headerlink" title="Instruction-following Evaluation"></a>Instruction-following Evaluation</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">epochs: 5</span><br><span class="line">warmup epochs: 2</span><br><span class="line">batch size: 64</span><br><span class="line">learning rate: 0.009</span><br><span class="line">weight decay: 0.02</span><br><span class="line"># LLaMA-7B</span><br><span class="line">N: 32</span><br><span class="line">K: 10</span><br><span class="line">L: 30</span><br></pre></td></tr></table></figure>

<p><strong>Generation Stage Decoding Method</strong>: top-p sampling with a temperature 0.1 and a top-p &#x3D; 0.75</p>
<p><strong>Dataset</strong>: Alpaca-52K self-instruct data. LLaMA-Adapter paper wrongly denotes as Alphaca-52K</p>
<p><strong>Evaluation Metric</strong>: (1) quantitative evaluation, ask GPT-4 to assess the response quality on 80 questions(Since we observed that GPT-4 has a preference to give higher scores to the first response in comparison, we also switch the position of two responses, resulting in a total of 160 evaluation items.); (2) simply show some response examples</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-15.jpg"></p>
<p>Comparison of Instruction-Following Capability, LLaMA-Adapter is comparable to Alpaca with fully fine-tuned 7B parameters<br><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-4.jpg" alt="Comparison of Instruction-Following Capability"></p>
<p>Comparison with Instruct LLaMA (LLaMA-I, LLaMA-65B fine-tuned on large-scale instructional data), LLaMA-Adapter can be further enhanced with larger LLaMA, larger data, larger learnable parameters<br><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-5.jpg" alt="Comparison with Instruct LLaMA (LLaMA-I)"></p>
<h3 id="Multi-modal-Evaluation"><a href="#Multi-modal-Evaluation" class="headerlink" title="Multi-modal Evaluation"></a>Multi-modal Evaluation</h3><p><strong>Generation Stage Decoding Method</strong>: greedy search</p>
<p>Other hyperparameters are the same as instruction-following LLaMA-Adapter.</p>
<p><strong>Dataset</strong>: ScienceQA, COCO Caption</p>
<p><strong>Result on ScienceQA</strong>: MM-CoT relies on the complex two-stage inference. <strong>Future Work: leverage CoT to boost LLaMA-Adapter.</strong><br><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-7.jpg" alt="ScienceQA"></p>
<p><strong>Result on COCO Caption</strong>: Both BLIP and BLIP-2 adopt a costly pre-training stage on additional datasets for superior performance. In contrast, our LLaMA-Adapter only requires COCO Catption‚Äôs training set of 0.6M data and attains better accuracy than ClipCap. <img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-16.jpg"></p>
<h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>(ScienceQA for example)</p>
<p>Result: table 6 shows robustness to over-fitting on the small dataset. Even if LLaMA-Adapter has over-fitted the fine-tuning data(val loss), the val acc is still increasing.<br><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-8.jpg" alt="Ablation Study"></p>
<h3 id="Zero-initialized-Attention-for-other-Large-Models-1"><a href="#Zero-initialized-Attention-for-other-Large-Models-1" class="headerlink" title="Zero-initialized Attention for other Large Models"></a>Zero-initialized Attention for other Large Models</h3><p><strong>Vision Model‚Äî‚ÄîViT</strong>: Image classification.(Table 7, Table 9)</p>
<p><strong>Language Model‚Äî‚ÄîRoBERTa</strong>: (1) Extractive question answering(Table 8), Exact Match (EM) and F1 scores on the dev set are reported. (2) NER and SRL.(Table 10)</p>
<p><strong>Vision-Language Model‚Äî‚ÄîCLIP</strong>: The model is trained only on the base classes in a few-shot setting and evaluated on both base and novel categories.(Table 11)</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-17.jpg"> <img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-18.jpg"> <img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-19.jpg"> <img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-20.jpg"></p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Future direction: </p>
<ul>
<li>wider multi-modal inputs(audio, video, point clouds)<ul>
<li>using pre-trained modal-specific encoders, we can integrate instructional signals of different modalities into the adaption prompts</li>
</ul>
</li>
<li>larger LLaMA(33B, 65B)</li>
<li>other LLMs</li>
<li>diverse benchmarks(VQA v2, OK-VQA, TVQA, DocVQA)<ul>
<li>ScienceQA is only an understanding task</li>
</ul>
</li>
<li>leverage CoT to boost LLaMA-Adapter</li>
</ul>
<h2 id="Code-Implementation"><a href="#Code-Implementation" class="headerlink" title="Code Implementation"></a>Code Implementation</h2><h3 id="Params"><a href="#Params" class="headerlink" title="Params"></a>Params</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">accum_iter: 1. Accumulate gradient iterations (for increasing the effective batch size under memory constraints)</span><br><span class="line">batch_size: 4(per GPU). effective_batch_size = batch_size * accum_iter * gpu_num</span><br><span class="line">epoch: 5</span><br><span class="line">adapter_layer: 30. the num of adapter layer L</span><br><span class="line">adapter_len: 10. the adapter length K</span><br><span class="line">max_seq_len: 512. specifies the maximum number of input tokens. token num &gt;= word num.</span><br><span class="line">max_batch_size: 32.</span><br><span class="line">dim: 4096.</span><br><span class="line">n_heads: 32.</span><br><span class="line">n_layers: 32.</span><br><span class="line">weight_decay: 0.02.</span><br><span class="line">blr: 9e-3. base learning rate.</span><br><span class="line">lr: learning_rate(absolute lr), lr = blr * total_batch_size / 256</span><br><span class="line">min_lr: 0.0. lower lr bound for cyclic schedulers that hit 0</span><br><span class="line">warmup_epochs: 2.</span><br><span class="line">seed: 0.</span><br></pre></td></tr></table></figure>

<p>Why we need max_seq_len? For absolute position embedding(e.g., BERT, Roberta, BART), it uses the index of each token to calculate and its length is limited(max_seq_len). When the input token length exceeds max_seq_len, <strong>‚Äúindex error‚Äù</strong> will be caused. For other position embedding methods(e.g., XLNet, T5), they have no limit of input token length. But longer input token length brings <strong>heavier memory burden</strong>, which may not necessarily lead to better performance.</p>
<p>LLaMA uses Rotary Position Embedding: <a href="https://zhuanlan.zhihu.com/p/627536105">ÂàÜÊûê | ROPEÁöÑ‰∏çÂêåÂÆûÁé∞Ôºöllama&amp;palm</a>, <a href="http://t.csdn.cn/U3RXo">blog 2.3 RoPEÊóãËΩ¨‰ΩçÁΩÆÁºñÁ†Å</a></p>
<h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">InstructionDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_path, model_path, max_seq_len, partition=<span class="string">&quot;train&quot;</span></span>):</span><br><span class="line">        self.ann = json.load(<span class="built_in">open</span>(data_path))</span><br><span class="line">        <span class="keyword">if</span> partition == <span class="string">&quot;train&quot;</span>:</span><br><span class="line">            self.ann = self.ann</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># It seems that the val set is a sub set of the train set.(data_path is same)</span></span><br><span class="line">            self.ann = self.ann[:<span class="number">200</span>]</span><br><span class="line">        ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        ann = self.ann[index]</span><br><span class="line">        <span class="keyword">if</span> ann.get(<span class="string">&quot;input&quot;</span>, <span class="string">&quot;&quot;</span>) == <span class="string">&quot;&quot;</span>:</span><br><span class="line">            prompt = PROMPT_DICT[<span class="string">&quot;prompt_no_input&quot;</span>].format_map(ann)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            prompt = PROMPT_DICT[<span class="string">&quot;prompt_input&quot;</span>].format_map(ann)</span><br><span class="line">        example = prompt + ann[<span class="string">&quot;output&quot;</span>]</span><br><span class="line">        example = torch.tensor(self.tokenizer1.encode(example, bos=<span class="literal">True</span>, eos=<span class="literal">True</span>), dtype=torch.int64)</span><br><span class="line">        padding = self.max_words - example.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># prompt = Propmt(template, ann[&#x27;instruction&#x27;], ann[&#x27;input&#x27;])</span></span><br><span class="line">        <span class="comment"># max_seq_len refers to tokenizer([prompt, ann[&#x27;output&#x27;]]).length, not tokenizer(prompt).length</span></span><br><span class="line">        <span class="keyword">if</span> padding &gt; <span class="number">0</span>:</span><br><span class="line">            example = torch.cat((example, torch.zeros(padding, dtype=torch.int64) - <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">elif</span> padding &lt; <span class="number">0</span>:</span><br><span class="line">            example = example[: self.max_words]</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>

<h3 id="Learnable-Adaption-Prompts-1"><a href="#Learnable-Adaption-Prompts-1" class="headerlink" title="Learnable Adaption Prompts"></a>Learnable Adaption Prompts</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module): <span class="comment"># Decoder</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, params: ModelArgs</span>):</span><br><span class="line">        ...</span><br><span class="line">        <span class="comment"># randomly initialise the adaption prompts</span></span><br><span class="line">        <span class="comment"># github.com/OpenGVLab/LLaMA-Adapter/issues/9#issuecomment-1501705647</span></span><br><span class="line">        self.adapter_query = nn.Embedding(params.adapter_len * params.adapter_layer, params.dim)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, examples, labels</span>):</span><br><span class="line">        _bsz, seqlen = examples.shape</span><br><span class="line">        ...</span><br><span class="line">        mask = torch.full((<span class="number">1</span>, <span class="number">1</span>, seqlen, seqlen), <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>), device=h.device)</span><br><span class="line">        mask = torch.triu(mask, diagonal=<span class="number">0</span> + <span class="number">1</span>).type_as(h) <span class="comment"># Upper triangular matrix, and diagonal val is 0</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers[: -<span class="number">1</span> * self.adapter_layer]:</span><br><span class="line">            h = layer(h, start_pos, freqs_cis, mask)</span><br><span class="line">        adapter_index = <span class="number">0</span></span><br><span class="line">        <span class="comment"># adapter.shape: (30, 1, 10, 4096)</span></span><br><span class="line">        adapter = self.adapter_query.weight.reshape(-<span class="number">1</span>, self.adapter_len, <span class="number">4096</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers[-<span class="number">1</span> * self.adapter_layer :]:</span><br><span class="line">            h = layer(h, start_pos, freqs_cis, mask, adapter[adapter_index].half())</span><br><span class="line">            adapter_index = adapter_index + <span class="number">1</span></span><br><span class="line">        output = self.output(self.norm(h))</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<ul>
<li>linear projection: queries $Q_l&#x3D;Linear_q(t_l)$, keys $K_l&#x3D;Linear_k([P_l;T_l;t_l])$, values $V_l&#x3D;Linear_v([P_l;T_l;t_l])$</li>
<li>attention scores: $S_l&#x3D;\frac{Q_lK_l^T}{\sqrt{C}}\in\mathbb{R}^{1\times(K+M+1)}$</li>
<li>reformulation: $S_l&#x3D;[S_l^K;S_l^{M+1}]^T, S_l^K\in\mathbb{R}^{K\times 1}, S_l^{M+1}\in\mathbb{R}^{(M+1)\times 1}$</li>
<li>softmax operation: $S_l^g&#x3D;[Softmax(S_l^K)¬∑g_l;Softmax(S_l^{M+1})]^T$</li>
<li>output of the attention layer: $t_l^o&#x3D;Linear_o(S_l^gV_l)\in\mathbb{R}^{1\times C}$</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, args: ModelArgs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        ...</span><br><span class="line">        <span class="comment"># zero-init attention</span></span><br><span class="line">        self.gate = torch.nn.Parameter(torch.zeros(<span class="number">1</span>, self.n_local_heads, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor, start_pos: <span class="built_in">int</span>, freqs_cis: torch.Tensor, mask: <span class="type">Optional</span>[torch.Tensor], adapter=<span class="literal">None</span></span>):</span><br><span class="line">        bsz, seqlen, _ = x.shape</span><br><span class="line">        <span class="comment"># 1. three Linears for x</span></span><br><span class="line">        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)</span><br><span class="line">        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)</span><br><span class="line">        xk = xk.view(bsz, seqlen, self.n_local_heads, self.head_dim)</span><br><span class="line">        xv = xv.view(bsz, seqlen, self.n_local_heads, self.head_dim)</span><br><span class="line">        <span class="comment"># 2. add position info via Rotary Position Embedding</span></span><br><span class="line">        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> adapter <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            adapter_len = adapter.shape[<span class="number">1</span>] <span class="comment"># adapter.shape: (1, 10, 4096)</span></span><br><span class="line">            <span class="comment"># linear projection</span></span><br><span class="line">            <span class="comment"># adapter_k.shape: (bsz, adapter_len, self.n_local_heads, self.head_dim)</span></span><br><span class="line">            adapter_k = self.wk(adapter).view(<span class="number">1</span>, adapter_len, self.n_local_heads, self.head_dim).repeat(bsz, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            adapter_v = self.wv(adapter).view(<span class="number">1</span>, adapter_len, self.n_local_heads, self.head_dim).repeat(bsz, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            xk = torch.cat([adapter_k, xk], dim=<span class="number">1</span>)</span><br><span class="line">            xv = torch.cat([adapter_v, xv], dim=<span class="number">1</span>)</span><br><span class="line">            extra_mask = torch.zeros(<span class="number">1</span>, <span class="number">1</span>, seqlen, adapter_len).to(mask)</span><br><span class="line">            mask = torch.cat([extra_mask, mask], dim=-<span class="number">1</span>) <span class="comment"># (1, 1, seqlen, adapter_len + seqlen)</span></span><br><span class="line">        keys = xk</span><br><span class="line">        values = xv</span><br><span class="line"></span><br><span class="line">        xq = xq.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        keys = keys.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        values = values.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 3. attention scores</span></span><br><span class="line">        scores = torch.matmul(xq, keys.transpose(<span class="number">2</span>, <span class="number">3</span>)) / math.sqrt(self.head_dim)</span><br><span class="line">        <span class="comment"># for decoder type, mask is needed to avoid using the information in the future.</span></span><br><span class="line">        <span class="comment"># the predictions for position i can depend only on the known outputs at positions less than i</span></span><br><span class="line">        <span class="comment"># mask.shape: (1, 1, seqlen, adapter_len + seqlen)</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            scores = scores + mask  <span class="comment"># (bsz, n_local_heads, seqlen, adapter_len + seqlen)</span></span><br><span class="line">        <span class="comment"># 4. softmax</span></span><br><span class="line">        <span class="keyword">if</span> adapter <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            scores = torch.cat(</span><br><span class="line">                [</span><br><span class="line">                    <span class="comment"># zero-init attention</span></span><br><span class="line">                    self.gate.tanh().half() * F.softmax(scores[:, :, :, :adapter_len].<span class="built_in">float</span>(), dim=-<span class="number">1</span>).type_as(xq),</span><br><span class="line">                    F.softmax(scores[:, :, :, adapter_len:].<span class="built_in">float</span>(), dim=-<span class="number">1</span>).type_as(xq),</span><br><span class="line">                ],</span><br><span class="line">                dim=-<span class="number">1</span>,</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            scores = F.softmax(scores.<span class="built_in">float</span>(), dim=-<span class="number">1</span>).type_as(xq)</span><br><span class="line">        output = torch.matmul(scores, values)  <span class="comment"># (bs, n_local_heads, slen, head_dim)</span></span><br><span class="line">        output = output.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(bsz, seqlen, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.wo(output)</span><br></pre></td></tr></table></figure>
<p>(images below are from <a href="http://jalammar.github.io/illustrated-gpt2/">jalammar.github.io&#x2F;illustrated-gpt2&#x2F;</a>)<br><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-9.png"><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-10.png"><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-11.png"></p>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><h3 id="Parameter-Efficient-Fine-Tuning-PEFT"><a href="#Parameter-Efficient-Fine-Tuning-PEFT" class="headerlink" title="Parameter-Efficient Fine-Tuning(PEFT)"></a>Parameter-Efficient Fine-Tuning(PEFT)</h3><p><img src="https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/classic-flowchart-1536x535.png" alt="Three conventional approaches of finetuing"> <a href="https://lightning.ai/pages/community/article/understanding-llama-adapters/">Three conventional approaches of finetuing.(the pre-trained model is not too large)</a> They are all compatible with encoder and decoder style. When finetune generative models, we work with and build on the embeddings they create instead of the generated output texts. But in-context learning only applies to decoder style. <img src="https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/classic-performance-1024x377.png" alt="traininig efficiency and modeling performance"></p>
<p><a href="https://github.com/huggingface/peft">PEFT</a> methods freeze most parameters of pre-trained models, and can still exhibit comparable capabilities on downstream tasks. It is needed when we want to get a similar modeling quality as finetuning II on LLMs. (e.g., Prompt-Tuning, Adapter, LoRA)</p>
<p>Prompt tuning appends a collection of trainable prompt tokens to pre-trained large models, which are inserted either to the input embeddings only, or to all of the intermediate layers. (e.g., Hard&#x2F;Soft Prompt-Tuning, Prefix-Tuning)</p>
<p>Hard Prompt-Tuning: directly change the discrete input tokens, which are not differentiable: <img src="https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/hard-prompting-1024x214.png"></p>
<p>Soft Prompt-Tuning: concatenates the embeddings of the input tokens with a trainable tensor that can be optimized via backpropagation.</p>
<p><a href="https://arxiv.org/pdf/2101.00190.pdf">Prefix Tuning</a>: add a trainable tensor to each transformer block instead of only the input embeddings, as in Soft Prompt-Tuning: <img src="https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/prefix-tuning-1536x907.png"></p>
<p><a href="https://arxiv.org/pdf/1902.00751.pdf">Adapter</a>: adds adapter layers in two places. The hidden dim in each adapter layer is low(e.g., 1024‚Äì&gt;24, params 1024√ó24+24√ó1024&#x3D;49,512 &lt; 1024√ó1024&#x3D;1,048,576). <img src="https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/adapter-outline-1024x548.png"></p>
<p><a href="https://lightning.ai/pages/community/tutorial/lora-llm/">LoRA</a>: introduces trainable rank decomposition matrices into each network weights: <img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-13.jpg"></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-12.png"> <a href="https://aclanthology.org/2022.acl-long.433.pdf">(UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning)</a></p>
<p>LLaMA-Adapter is distinct from regular Prefix-Tuning: 1. L topmost layer(not all) 2. zero-init attention(stablity improvement) 3. unified multi-modal tuning(unimodal to multi-modal) <img src="https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/popular-methods-1024x282.png"> <img src="https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/llama-adapter-1447x1536.png"></p>
<h3 id="Instruction-Following-Models"><a href="#Instruction-Following-Models" class="headerlink" title="Instruction-Following Models"></a>Instruction-Following Models</h3><p>Instruction-following capabilities: understand user intentions and follow instructions accurately.</p>
<p>Closed-source restriction and high training costs imped instruction-following models‚Äô development.</p>
<p>Compared to a concurrent work <a href="https://github.com/tloen/alpaca-lora">Alpaca-LoRA</a>, our approach further reduces the computational demands, and can be generalized to follow visual instructions for multi-modal reasoning.</p>
<p><em>Language Modality:</em></p>
<ul>
<li><a href="https://arxiv.org/pdf/2109.01652.pdf">FLAN</a>: introduces an instruction tuning method</li>
<li><a href="https://arxiv.org/pdf/2202.01279.pdf">PromptSource</a>: a web-based GUI for creating and managing natural language prompt</li>
<li><a href="https://aclanthology.org/2022.emnlp-main.340.pdf">SUP-NATINST</a>: an benchmark of instructions on 1,616 NLP tasks</li>
<li><a href="https://arxiv.org/pdf/2203.02155.pdf">InstructGPT</a>: RLHF, significant performance improvements</li>
<li><strong><a href="https://github.com/tatsu-lab/stanford_alpaca">Alpaca</a>: data-efficient(self-instruction), high costs(fine-tuning)</strong></li>
<li><a href="https://lmsys.org/blog/2023-03-30-vicuna/">Vicuna</a> and <a href="https://arxiv.org/pdf/2304.03277.pdf">GPT-4-LLM</a>: reveal that dialog and enhanced instruction-following capabilities can be ignited by fine-tuning on either user-shared ChatGPT conversations or instruction-following data generated by the GPT-4 API</li>
</ul>
<p><em>Multi-modality:</em></p>
<p>Robot</p>
<ul>
<li>2020&#x2F;03&#x2F;31 <a href="https://arxiv.org/pdf/1912.01734.pdf">ALFRED</a>:  a benchmark for robotics instruction following</li>
<li>2022&#x2F;03&#x2F;16 <a href="https://arxiv.org/pdf/2110.07342.pdf">FILM</a>: a modular method for robotics instruction following</li>
</ul>
<p>Video</p>
<ul>
<li>2023&#x2F;05&#x2F;10 <a href="https://arxiv.org/pdf/2305.06355.pdf">VideoChat: Chat-Centric Video Understanding</a>: (1) integrates video foundation models and large language models via a learnable neural interface; (2) propose a video-centric instruction dataset based on WebVid-10M; (3) spatiotemporal reasoning, event localization, and causal relationship inference</li>
<li>2023&#x2F;06&#x2F;08 <a href="https://arxiv.org/pdf/2306.05424.pdf">Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models</a>: (1)  merges a video-adapted visual encoder with a LLM; (2) introduce a new dataset of 100,000 video-instruction pairs; (3) develop a quantitative evaluation framework for video-based dialogue models; (4) understanding and generating detailed conversations about videos</li>
<li>2023&#x2F;06&#x2F;12 <a href="https://arxiv.org/pdf/2306.02858.pdf">Video-LLaMA An Instruction-tuned Audio-Visual Language Model for Video Understanding</a>: (1) propose a Video Q-former for temporal info; (2) propose an Audio Q-former based on ImageBind for audio-visual signals</li>
</ul>
<p>Image(Model)</p>
<ul>
<li>2023&#x2F;04&#x2F;27 <a href="https://arxiv.org/pdf/2302.14045v1.pdf">Kosmos-1</a>, <a href="https://arxiv.org/pdf/2304.10592.pdf">MiniGPT4</a>, <a href="https://arxiv.org/pdf/2304.08485.pdf">LLaVA</a>, <a href="https://arxiv.org/pdf/2304.14178.pdf">mPLUG-Owl</a> <img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-14.jpg"></li>
<li>2023&#x2F;05&#x2F;05 <a href="https://arxiv.org/pdf/2305.03726.pdf">Otter: A Multi-Modal Model with In-Context Instruction Tuning</a></li>
<li>2023&#x2F;05&#x2F;11 <a href="https://arxiv.org/pdf/2305.06500.pdf">InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning</a>: (1) gather a wide variety of 26 publicly available datasets, transform them into instruction tuning format; (2)  instruction-aware visual feature extraction method</li>
<li>2023&#x2F;05&#x2F;22 <a href="https://arxiv.org/pdf/2305.04160.pdf">X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages</a>: propose X2L interface to convert other modality(image, speech, video) into foreign language via 3 training stages</li>
<li>2023&#x2F;05&#x2F;24 <a href="https://arxiv.org/pdf/2305.15023.pdf">Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models</a>: (1) Mixture-of-Modality Adaptation(MMA); (2) a routing algorithm for MMA, which enables an automatic shift between single- and multi-modal instructions; (3) MMA+LLaMA&#x3D;LaVIN(efficient), 1.4 training hours with 3.8M trainable params</li>
<li>2023&#x2F;05&#x2F;25 <a href="https://arxiv.org/pdf/2305.16355.pdf">PandaGPT: One Model To Instruction-Follow Them All</a>: (1) visual and auditory instruction-following capabilities(detailed image description generation, writing stories inspired by videos, answering questions about audios); (2) combines the multimodal encoders from ImageBind and the large language models from Vicuna; (3) displays emergent, i.e. zero-shot, cross-modal behaviors for data other than image and text (e.g., video, audio, depth, thermal, and IMU)</li>
<li>2023&#x2F;05&#x2F;25 <a href="https://arxiv.org/pdf/2305.16103.pdf">ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst</a>: (1) show that only language-paired two-modality data is sufficient to connect all modalities; (2) propose a new multi-modal instruction tuning dataset MULTIS, which covers a wide range of 16 multimodal tasks of text, image, video, and audio modalities; (3) a two-stage training, firstly aligns each modality with language, secondly aligns model with user intent</li>
<li>2023&#x2F;05&#x2F;30 <a href="https://arxiv.org/pdf/2305.18752.pdf">GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction</a>: (1) enable open-source LLMs to use multimodal tools; (2) generates an instruction-following dataset by prompting an advanced teacher with various multi-modal contexts; (3) provide a benchmark to evaluate the ability of LLMs to use tools</li>
<li>2023&#x2F;06&#x2F;02 <a href="https://arxiv.org/pdf/2305.05662.pdf">InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language</a>: integrates chatbots with non-verbal instructions(e.g., point movements like gestures and cursors), which requires fine-grained control, editing, and generation of visual content</li>
<li>2023&#x2F;06&#x2F;13 <a href="https://arxiv.org/pdf/2305.04790.pdf">MultiModal-GPT: A Vision and Language Model for Dialogue with Humans</a>: (1) capable of generating detailed captions, counting specific objects, and addressing general inquiries posed by users; (2) OpenFlamingo+LoRA; (3) construct multi-modal instruction templates; (4) also employ language-only instruction-following data for dialogue performance improvement</li>
</ul>
<p>Image(Dataset)</p>
<ul>
<li>2023&#x2F;06&#x2F;08 <a href="https://arxiv.org/pdf/2306.05425.pdf">MIMIC-IT: Multi-Modal In-Context Instruction Tuning</a>: (1) 2.8 million multimodal instruction-response pairs; (2) 8 languages; (3) contains videos</li>
<li>2023&#x2F;06&#x2F;08 <a href="https://arxiv.org/pdf/2306.04387.pdf">M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning</a>: (1) comprises 40 carefully curated datasets, including 2.4 million instances and 400 manually written task instructions, which surpasses previous datasets regarding task coverage; (2) 80 languages; (3) Ying-VLM model</li>
<li>2023&#x2F;06&#x2F;10 <a href="https://arxiv.org/pdf/2212.10773.pdf">MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning</a>: (1) a multimodal instruction tuning benchmark dataset that consists of 62 multimodal tasks; (2) a new evaluation metric, Sensitivity, to evaluate how sensitive the model is to the variety of instructions</li>
<li>2023&#x2F;06&#x2F;11 <a href="https://arxiv.org/pdf/2306.06687.pdf">LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark</a>: (1) extend MLLMs to point clouds; (2) LAMM-Dataset and LAMM-Benchmark for 2D image and 3D point cloud understanding</li>
</ul>
<h3 id="Large-Vision-Language-Models"><a href="#Large-Vision-Language-Models" class="headerlink" title="Large Vision-Language Models"></a>Large Vision-Language Models</h3><p>Recently, some researchers adopt pre-trained unimodal models as initialization and only train the newly introduced parameters. They use mapping networks or cross-attention layers to connect two modalities.</p>
<p>As a new method, LLaMA-Adapter also belongs to this line of work.</p>
<ul>
<li><a href="https://arxiv.org/pdf/2106.13884.pdf">Frozen</a>: fine-tunes an image encoder to transform visual tokens into LLM‚Äôs soft prompts</li>
<li><a href="https://arxiv.org/pdf/2111.07991.pdf">LiT</a>: utilizes pretrained image encoder to speed up CLIP training</li>
<li><a href="https://arxiv.org/pdf/2111.09734.pdf">CLIPCap</a>: proposes a mapping network to connect the pre-trained image encoder with LLMs</li>
<li><a href="https://arxiv.org/pdf/2110.04544.pdf">CLIP-Adapter</a>, <a href="https://arxiv.org/pdf/2111.03930.pdf">Tip-Adapter</a> and <a href="https://arxiv.org/pdf/2112.02413.pdf">PointCLIP</a>: introduce customized adapters upon CLIP for 2D and 3D few-shot learning</li>
<li><a href="https://arxiv.org/pdf/2204.14198.pdf">Flamingo</a>: inserts several cross-attention layers to inject visual knowledge into LLMs</li>
<li><a href="https://arxiv.org/pdf/2301.12597.pdf">BLIP2</a>: connects pre-trained image encoders and LLMs with a Q-Former</li>
</ul>
]]></content>
      <categories>
        <category>Paper</category>
        <category>2023</category>
        <category>03</category>
      </categories>
      <tags>
        <tag>Multi-modal</tag>
        <tag>LLM</tag>
        <tag>Instruction-Following</tag>
        <tag>Adapter</tag>
        <tag>Prefix-Tuning</tag>
        <tag>PEFT</tag>
      </tags>
  </entry>
  <entry>
    <title>LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model</title>
    <url>/2023/06/19/llama-adapter-v2/</url>
    <content><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-0.png"></p>
<p>Paper: <a href="https://arxiv.org/pdf/2304.15010.pdf">https://arxiv.org/pdf/2304.15010.pdf</a></p>
<p>Code: <a href="https://github.com/OpenGVLab/LLaMA-Adapter">https://github.com/OpenGVLab/LLaMA-Adapter</a></p>
<blockquote>
<p>After trained on language instruction data, LLaMA-Adapter-V1 is fine-tuned on COCO Caption, which introducing new visual cues but damaging instruction-following abilities.</p>
</blockquote>
<aside>
‚§¥Ô∏è Therefore, LLaMA-Adapter-V2 further improves the multi-modal instruction-following abilities of LLaMA by introducing 14M parameters over LLaMA.

</aside>

<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-1.png" alt="Training Pipeline of LLaMA-Adapter V2."></p>
<hr>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><h3 id="Instruction-following-Models"><a href="#Instruction-following-Models" class="headerlink" title="Instruction-following Models"></a>Instruction-following Models</h3><blockquote>
<p>Details can be seen in my previous note of LLaMA-Adapter V1: the ‚ÄòRelated Work&#x2F;Instruction-Following Models‚Äô section.</p>
</blockquote>
<p>LLaMA-Adapter V2 can function effectively using just language instruction data and image-text pairs, without relying on multi-modal instruction data.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-2.png" alt="Table 1. Training Comparison of Different Methods. CC, VG and L400 represent Conceptual Caption, Visual Genome and LAION 400M, respectively. ‚àó denotes the filtered dataset."></p>
<h3 id="Parameter-efficient-Fine-tuning"><a href="#Parameter-efficient-Fine-tuning" class="headerlink" title="Parameter-efficient Fine-tuning"></a>Parameter-efficient Fine-tuning</h3><blockquote>
<p>Details can be seen in my previous note of LLaMA-Adapter V1: the ‚ÄòRelated Work&#x2F;Parameter-Efficient Fine-Tuning(PEFT)‚Äô section.</p>
</blockquote>
<p>By utilizing an <strong>early fusion strategy</strong> and <strong>bias tuning</strong>, LLaMA-Adapter V2 injects visual features into LLMs, with only <strong>0.04%</strong> parameters of LLaMA.</p>
<h3 id="Integration-of-Expert-Systems"><a href="#Integration-of-Expert-Systems" class="headerlink" title="Integration of Expert Systems"></a>Integration of Expert Systems</h3><p>LLMs act as a core controller for external experts systems to boost its overall performance.</p>
<ul>
<li><strong>Vision Tasks</strong>: visual models as experts. <a href="https://arxiv.org/abs/2303.17580">HuggingGPT</a>, <a href="https://arxiv.org/abs/2303.04671">Visual ChatGPT</a>, <a href="https://arxiv.org/abs/2304.09842">Chameleon</a>, <a href="https://arxiv.org/abs/2303.11381">MMReACT</a> and <a href="https://arxiv.org/abs/2303.08128">ViperGPT</a></li>
<li><strong>Robotics</strong>: real-world sensors as experts. <a href="https://arxiv.org/abs/2303.03378">PaLM-E</a>, <a href="https://arxiv.org/abs/2207.05608">Inner Monologue</a> and <a href="https://arxiv.org/abs/2303.12153">Text2Motion</a></li>
</ul>
<p>For LLaMA-Adapter V2, experts integration compensates for the shortcomings brought by small training data.</p>
<h2 id="LLaMA-Adapter-V2"><a href="#LLaMA-Adapter-V2" class="headerlink" title="LLaMA-Adapter V2"></a>LLaMA-Adapter V2</h2><p>LLaMA-Adapter V2 is based on finetuned LLaMA-Adapter V1.</p>
<h3 id="Bias-Tuning-of-Linear-Layers"><a href="#Bias-Tuning-of-Linear-Layers" class="headerlink" title="Bias Tuning of Linear Layers"></a>Bias Tuning of Linear Layers</h3><aside>
üëâ To enhance its language instruction-following ability

</aside>

<p>The added params accounts for <strong>0.04%(~5M) of LLaMA</strong> and its initialization is helpful for stable training at early stages.</p>
<p><strong>We first unfreeze all the normalization layers in LLaMA, and then</strong>:</p>
<ul>
<li>$x$: input</li>
<li>$W$: pre-trained weights(frozen) of a certain linear layer</li>
<li>$b&#x3D;Init(0)$: learnable bias</li>
<li>$s&#x3D;Init(1)$: learnable scale factor</li>
<li>$y&#x3D;W¬∑x\rightarrow y&#x3D;s¬∑(W¬∑x+b)$: <strong>modify each linear layer in the Transformer</strong></li>
</ul>
<p><strong>Our bias tuning method compared with related work:</strong></p>
<ul>
<li><strong>BitFit, SSF</strong>: experiments on 80M parameters scale<ul>
<li><strong>Ours</strong>: from 7B to 65B</li>
</ul>
</li>
<li><strong>LoRA</strong>: input-aware bias<ul>
<li><p><strong>Ours</strong>: input-agnostic, less fine-tuning cost</p>
  <aside>
  ‚ùì why less cost
  
  </aside></li>
</ul>
</li>
</ul>
<h3 id="Joint-Training-with-Disjoint-Parameters"><a href="#Joint-Training-with-Disjoint-Parameters" class="headerlink" title="Joint Training with Disjoint Parameters"></a>Joint Training with Disjoint Parameters</h3><aside>
üëâ For balanced visual instruction tuning

</aside>

<ul>
<li>Trained on <strong>500K image-text pairs</strong>: visual projection layers, early zero-initialized attention</li>
<li>Trained on <strong>50K language-only instruction data</strong>: late zero-initialized attention, unfrozen norm, learnable bias and scale factor (or optional <a href="https://arxiv.org/pdf/2106.09685.pdf">LoRA</a>)</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-3.png" alt="Joint Training Paradigm in LLaMA-Adapter V2."></p>
<h3 id="Early-Fusion-of-Visual-Knowledge"><a href="#Early-Fusion-of-Visual-Knowledge" class="headerlink" title="Early Fusion of Visual Knowledge"></a>Early Fusion of Visual Knowledge</h3><aside>
üëâ To balance textual and visual understanding

</aside>

<p>$N$: the total number of Transformer layers</p>
<ul>
<li><p><strong>Visual prompts</strong>: concatenated with the word tokens at <strong>the first $K$ Transformer layers</strong> with zero-initialized attention</p>
<ul>
<li>$K&lt;N-L$</li>
</ul>
</li>
<li><p><strong>Static adaptation prompts</strong>: inserted into the last $L$ layers (e.g., L&#x3D;31)</p>
  <aside>
  ‚ùì a little hard to understand ‚Äùinserted into‚Äù: add or concatenate?
  
  </aside></li>
</ul>
<p>Prevent direct interactions between the two via placing them in different layers.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-4.png" alt="Early Fusion of Visual Knowledge."></p>
<aside>
‚ùì **But in code implementation, textual tokens are not concatenated with visual_query in the first layer**

</aside>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># LLaMA-Adapter/llama_adapter_v2_multimodal/llama/llama_adapter.py</span><br><span class="line"># class LLaMA_adapter:</span><br><span class="line">def forward(self, visual_query, tokens, start_pos: int):</span><br><span class="line">    _bsz, seqlen = tokens.shape</span><br><span class="line">    h = self.llama.tok_embeddings(tokens)</span><br><span class="line">    freqs_cis = self.llama.freqs_cis.to(h.device)</span><br><span class="line">    freqs_cis = freqs_cis[start_pos : start_pos + seqlen]</span><br><span class="line">    mask = None</span><br><span class="line">    mask = torch.full((1, 1, seqlen, seqlen),</span><br><span class="line">                        float(&quot;-inf&quot;), device=h.device)</span><br><span class="line">    mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)</span><br><span class="line"></span><br><span class="line">    for layer in self.llama.layers[:-1 * self.query_layer]:</span><br><span class="line">        h = layer(h, start_pos, freqs_cis, mask)</span><br><span class="line"></span><br><span class="line">    adapter = self.adapter_query.weight.reshape(</span><br><span class="line">        self.query_layer, self.query_len, -1).unsqueeze(1)</span><br><span class="line">    adapter_index = 0</span><br><span class="line">    for layer in self.llama.layers[-1 * self.query_layer:]:</span><br><span class="line">        dynamic_adapter = adapter[adapter_index].repeat(_bsz, 1, 1)</span><br><span class="line">        dynamic_adapter = dynamic_adapter + visual_query</span><br><span class="line">        h = layer(h, start_pos, freqs_cis, mask, dynamic_adapter)</span><br><span class="line">        adapter_index = adapter_index + 1</span><br><span class="line"></span><br><span class="line">    h = self.llama.norm(h)</span><br><span class="line">    output = self.llama.output(h[:, -1, :])</span><br><span class="line">    return output.float()</span><br></pre></td></tr></table></figure>

<h3 id="Integration-with-Experts"><a href="#Integration-with-Experts" class="headerlink" title="Integration with Experts"></a>Integration with Experts</h3><aside>
üëâ To boost zero-shot multi-modal reasoning

</aside>

<p><strong>Well-trained img2text experts</strong>:</p>
<ul>
<li><strong>default implementation</strong> adopts LLaMA-Adapter v1 pre-trained on COCO Caption for short and accurate image descriptions generation</li>
<li>can be replaced with a search engine &#x2F; OCR</li>
</ul>
<p>We can easily switch among different experts based on the specific downstream task.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-5.png" alt="Generation Pipeline of LLaMA-Adapter V2."></p>
<aside>
‚ùì The order of 3 input parts(Textual Context, Question, Visual Context)

</aside>

<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Experimental-Setups"><a href="#Experimental-Setups" class="headerlink" title="Experimental Setups"></a>Experimental Setups</h3><p><strong>Training Data</strong>:</p>
<ul>
<li>52K single-turn instruction data from GPT-4-LLM</li>
<li>567K captioning data from COCO Caption</li>
<li>80K conversation data from ShareGPT (train a chatbot, multi-round)</li>
</ul>
<p><strong>Implementation Details</strong>:</p>
<ul>
<li>visual prompt length: 20</li>
</ul>
<h3 id="Stronger-Language-Instruction-Model"><a href="#Stronger-Language-Instruction-Model" class="headerlink" title="Stronger Language Instruction Model"></a>Stronger Language Instruction Model</h3><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-6.png" alt="Part of Table 2. LLaMA-Adapter V2 provides more comprehensive answers and detailed explanations."></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-7.png" alt="A Chatting Example using 7B LLaMA-Adapter V2. But its understanding of context is not very accurate."></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-8.png" alt="A Chatting Example using 65B LLaMA-Adapter V2."></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-9.png" alt="Response Quality Comparisons(assessed by GPT-4 on 80 questions)."></p>
<ul>
<li>Ours: based on LLaMA-65B, fine-tune 14M parameters</li>
<li>Vicuna: based on LLaMA-13B, fine-tune 13B parameters</li>
</ul>
<h3 id="Visual-Instruction-Model"><a href="#Visual-Instruction-Model" class="headerlink" title="Visual Instruction Model"></a>Visual Instruction Model</h3><aside>
üëá Image Captioning

</aside>

<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-10.png" alt="Comparisons on COCO Caption."></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-11.png" alt="Comparisons of Image Captioning Results(between LLaMA-Adapter and LLaMA-Adapter V2)."></p>
<p>The failure case is intentionally choosen as an out-of-distribution example (cartoon picture) for testing.</p>
<p>It shows that the image-text alignment ability is not strong enough.</p>
<p>This motivates us to employ <strong>additional expert systems</strong> to enhance the image understanding ability.</p>
<aside>
üëá Visual Understanding

</aside>

<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-12.png" alt="The Visual Understanding Examples of LLaMA-Adapter V2."></p>
<ul>
<li>identify and explain the specific object or feature in the image</li>
<li>sophisticated reasoning and decision-making</li>
<li>offer a plausible guess or explanation when image cannot provide sufficient direct info</li>
</ul>
<aside>
üëá Integration with Experts

</aside>

<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-13.png" alt="Visual Understanding with the help of Caption Experts(more precise and detailed responses)."></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-14.png" alt="Visual Understanding with the help of OCR Experts. The example and OCR context are from DocVQA."></p>
<h2 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h2><ul>
<li>integrate more expert visual systems</li>
<li>use multi-modal instruction dataset</li>
<li>other PEFT methods (e.g., LoRA)</li>
</ul>
]]></content>
      <categories>
        <category>Paper</category>
        <category>2023</category>
        <category>04</category>
      </categories>
      <tags>
        <tag>Multi-modal</tag>
        <tag>LLM</tag>
        <tag>Instruction-Following</tag>
        <tag>Adapter</tag>
        <tag>Prefix-Tuning</tag>
        <tag>PEFT</tag>
        <tag>Expert Integration</tag>
      </tags>
  </entry>
</search>
