<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>[Waiting...]2022 Machine Learning Specialization</title>
    <url>/2023/06/15/2022-machine-learning-specialization/</url>
    <content><![CDATA[<blockquote>
<p><em>Definition of Machine Learning(informal)</em>: Field of study that gives computers the ability to learn without being explicitly programmed. [1959, Arthur Samuel]</p>
</blockquote>
<ul>
<li>Main Course Content<ul>
<li><strong>Supervised Learning</strong></li>
<li>Unsupervised Learning</li>
</ul>
</li>
<li>Others<ul>
<li>Reinforcement Learning</li>
<li>Practical advice for applying learning algorithms</li>
</ul>
</li>
</ul>
<h1 id="Supervised-Machine-Learning-Regression-and-Classification"><a href="#Supervised-Machine-Learning-Regression-and-Classification" class="headerlink" title="Supervised Machine Learning: Regression and Classification"></a>Supervised Machine Learning: Regression and Classification</h1><h2 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h2><h2 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h2><h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><!-- ## Machine Learning Overview
## Linear Regression with One Variable
## Training Linear Regression
## Linear Regression with Multiple Variables
## Practical Tips for Linear Regression
## Classification
## Cost Function
## Gradient Descent
## Regularization to Reduce Overfitting -->
<h1 id="Advanced-Learning-Algorithm"><a href="#Advanced-Learning-Algorithm" class="headerlink" title="Advanced Learning Algorithm"></a>Advanced Learning Algorithm</h1><h2 id="Neural-Networks"><a href="#Neural-Networks" class="headerlink" title="Neural Networks"></a>Neural Networks</h2><h2 id="Decision-Trees"><a href="#Decision-Trees" class="headerlink" title="Decision Trees"></a>Decision Trees</h2><h2 id="Advice-for-ML"><a href="#Advice-for-ML" class="headerlink" title="Advice for ML"></a>Advice for ML</h2><!-- ## Neural Networks Intuition
## Neural Network Model
## TensorFlow Implementation
## Neural Network Implementation in Python
## Speculations on Artificial General Intelligence(AGI)
## Vectorization(optional)
## Neural Network Training
## Activation Functions
## Multiclass Classification
## Additional Neural Network Concepts
## Advice for Applying Machine Learning
## Bias and Variance
## Machine Learning Development Process
## Skewed datasets(optional)
## Decision Trees
## Decision Tree Learning
## Tree Ensembles -->
<h1 id="Unsupervised-Learning-Recommender-Systems-and-Reinforcement-Learning"><a href="#Unsupervised-Learning-Recommender-Systems-and-Reinforcement-Learning" class="headerlink" title="Unsupervised Learning: Recommender Systems and Reinforcement Learning"></a>Unsupervised Learning: Recommender Systems and Reinforcement Learning</h1><h2 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h2><h2 id="Anomaly-Detection"><a href="#Anomaly-Detection" class="headerlink" title="Anomaly Detection"></a>Anomaly Detection</h2><h2 id="Collaborative-Filtering"><a href="#Collaborative-Filtering" class="headerlink" title="Collaborative Filtering"></a>Collaborative Filtering</h2><h2 id="Content-based-Filtering"><a href="#Content-based-Filtering" class="headerlink" title="Content-based Filtering"></a>Content-based Filtering</h2><h2 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h2><!-- ## Clustering
## Anomaly Detection
## Recommender System
## Recommender Systems Implementation
## Content-based Filtering
## Reinforcement Learning
## State-action Value Function
## Continuous State Spaces -->]]></content>
      <categories>
        <category>Online Course</category>
        <category>Andrew Ng</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>(LongMem)Augmenting Language Models with Long-Term Memory</title>
    <url>/2023/07/01/LongMem/</url>
    <content><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/longmem-0.png"></p>
<p>Paper: <a href="https://arxiv.org/pdf/2306.07174.pdf">https://arxiv.org/pdf/2306.07174.pdf</a></p>
<p>Code: <a href="https://github.com/Victorwz/LongMem">https://github.com/Victorwz/LongMem</a></p>
<p>Bilibili: <a href="https://www.bilibili.com/video/BV17M4y177WG/?share_source=copy_web">Augmenting Language Models with Long-Term Memory （UCSB， Microsoft 2023）</a></p>
<aside>
👇 **Problems**

</aside>

<ul>
<li>LLMs can <strong>only afford fix-sized inputs</strong> due to the input length limit, preventing them from utilizing rich long-context information from past inputs.</li>
</ul>
<p><em><strong>Two existing ways addressing the length limit issue:</strong></em></p>
<ul>
<li>Directly increasing the input length of LLM will cause <strong>huge computation complexity</strong>.</li>
<li>Developing sparse attention reduces the computation cost, but still requires <strong>training from scratch</strong>.</li>
</ul>
<aside>
👇 **Contributions**

</aside>

<ul>
<li><strong>LongMem</strong> enables LLMs to memorize long history, which benefits various downstream tasks.</li>
</ul>
<blockquote>
<p>The idea behind this is very similar to LoRA’s, creating a small network next to the backbone.</p>
</blockquote>
<p><em><strong>Two Benefits of LongMem:</strong></em></p>
<ul>
<li>Decouples the process of encoding memory and the process of memory retrieval and fusion, via decoupling LLM and SideNet, which effectively <strong>resolves the issue of memory staleness</strong>.</li>
<li>The SideNet with frozen LLM is <strong>parameter-efficient</strong> can avoid catastrophic forgetting.</li>
</ul>
<blockquote>
<p><strong>Remaining Questions</strong>: What is the ablation study of $m$ and $m_s$? What are limitation of LongMem and its future work?</p>
</blockquote>
<hr>
<h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/longmem-1.png" alt="Overview of the memory caching and retrieval flow of LongMem."></p>
<p>Three key components of LongMem: <strong>frozen LLM, Cache Memory Bank(CMB), Residual SideNet(RS).</strong></p>
<p><strong>CMB</strong> is a cached head-wise vector queue that stores the memory.</p>
<p><strong>RS</strong> is efficiently trained to fuse the memory context information.</p>
<h3 id="Encoding-and-Storing-Memory"><a href="#Encoding-and-Storing-Memory" class="headerlink" title="Encoding and Storing Memory"></a>Encoding and Storing Memory</h3><p>Most LLMs process a fixed-sized input. Therefore, split a long sequence into fix-length segments.</p>
<blockquote>
<p>Each segment has several sequences.</p>
</blockquote>
<ul>
<li><strong>Previous segments</strong>: key-value pairs of self-attention at m-th layer($\mathcal{Z}_k,\mathcal{Z}_v\in\mathbb{R}^{H\times M\times d}$) are stored in <strong>CMB</strong><ul>
<li>$H$ is the attention heads num, $d$ is per-head dimension, $M$ indicates the capacity of <strong>CMB</strong></li>
</ul>
</li>
<li><strong>Current segment ${x_i}_{i&#x3D;1}^{|x|}:$</strong> hidden states(of each layer) are retained and transfered to <strong>RS</strong><ul>
<li>$\mathbf{H}<em>{LLM}^0\in\mathbb{R}^{|x|\times E}$ is the initial hidden states of ${x_i}</em>{i&#x3D;1}^{|x|}$, $E$ is the hidden dimension</li>
<li>$\mathbf{H}<em>{LLM}^{l’}&#x3D;f</em>{\theta_{LLM}^{l’}}(\mathbf{H}_{LLM}^{l’-1}),\forall l’\in[1,L’]$ is the successive hidden states, $L’$ is the total layer num of LLM</li>
</ul>
</li>
<li><strong>Current token</strong>: top-K relevant key-value pairs are retrieved and fused into language modeling</li>
</ul>
<p><strong>CMB Update Mechanism</strong>: after memory retrieval and fusion, the oldest sequences are removed, the current sequences are appended.</p>
<h3 id="Memory-Retrieval"><a href="#Memory-Retrieval" class="headerlink" title="Memory Retrieval"></a>Memory Retrieval</h3><p><strong>Text-chunk</strong>: an n-gram structure of <strong>chunk-size</strong> $csz$ number of contiguous tokens.</p>
<p>The hyperparameter $csz$ controls <strong>the granularity of retrieved contexts</strong>, which can be empirically adjusted based on downstream tasks.</p>
<blockquote>
<p>For instance, In-Context Learning requires more fine-grained label tokens from demostration examples cached in memory, where a smaller $csz$ is helpful.</p>
</blockquote>
<p><strong>Advantages of Token-to-Chunk Retrieval</strong>(compared with token-to-token):</p>
<ul>
<li>reduce the size of the retrieval index and <strong>accelerate</strong> the process</li>
<li>further improve the retrieval <strong>accuracy</strong> (not always, when $csz$ is too large)</li>
</ul>
<p><strong>Three steps of retrieval:</strong></p>
<ol>
<li>The <strong>CMB</strong> has $M&#x2F;csz$ chunks. We use the <strong>mean-pooled vector</strong> on the chunk-size dimension to get the key vector for retrieval.</li>
<li>Then we retrieve the top-($K&#x2F;csz$) chunks w.r.t(with respect to) the dot product between the <strong>attention query</strong> of current token $\mathbf{x}_i$ and the <strong>mean-pooled attention key</strong> of a candidate chunk.</li>
<li>Finally, by squeezing and flatterning, we get $K$ key-valus pairs ${\widetilde{\mathbf{K}}_j,\widetilde{\mathbf{V}}<em>j}</em>{j&#x3D;1}^K$ at token-level.</li>
</ol>
<h3 id="Memory-Fusion"><a href="#Memory-Fusion" class="headerlink" title="Memory Fusion"></a>Memory Fusion</h3><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/longmem-2.png" alt="Overview of LongMem architecture. “**MemAug**” represents **Memory-Augmented Layer** for memory retrieval and fusion."></p>
<aside>
👇 **Residual** **SideNet** Architecture and Initialization

</aside>

<p>SideNet consists of (L-1) normal Transformer decoder layers and one special <strong>MemAug</strong>.</p>
<p>$L$ is the total layer num of <strong>SideNet</strong>, $L’&#x3D;\alpha L, \alpha\in{2,3,4,…}$, $L&lt;L’$ for efficiency.</p>
<p><strong>Weight Initialization</strong>: $\mathcal{\Theta}<em>{Side}^{\frac{l’}{\alpha}}&#x3D;\mathcal{\Theta}</em>{LLM}^{l’}$</p>
<p><strong>Cross-Network Residual Connections</strong>:</p>
<p>$\mathbf{H}<em>{Side}^l&#x3D;f</em>{\mathcal{\Theta}<em>{Side}^l}(\mathbf{H}</em>{Side}^{l-1})+(\mathbf{H}<em>{LLM}^{\alpha l}-\mathbf{H}</em>{LLM}^{\alpha(l-1)}),\forall l\in[1,L]$</p>
<blockquote>
<p>The residual connections after the SA and FFN of a decoder layer will be performed as normal in $f_{\mathcal{\Theta}<em>{Side}^l}(\mathbf{H}</em>{Side}^{l-1})$ and parallel to the proposed cross-network residual connections. SA is self-attention. FFN is feed-forward network.</p>
</blockquote>
<aside>
👇 **MemAug:** Each token attends on both local contexts and retrieved memory contexts.

</aside>

<p>$\mathbf{Q}, \mathbf{K}, \mathbf{V}, \mathbf{A},\mathbf{M}\in\mathbb{R}^{|x|\times d}$, $g$ is a <strong>trainable</strong> <strong>head-wise</strong> gating factor, $m_s$ is the layer index of <strong>MemAug.</strong></p>
<blockquote>
<p>The initialization of gating factor g: Parameter(torch.zeros(self.num_heads))</p>
</blockquote>
<p>The hidden state output from previous layer ($\mathbf{H}^{(m_s-1)}_{Side}\in\mathbb{R}^{|x|\times d}$) is linearly projected into $\mathbf{Q}, \mathbf{K}, \mathbf{V}$ via three matrices $W^Q,W^K,W^V\in\mathbb{R}^{d\times d}$.</p>
<p>${\widetilde{\mathbf{K}}_i,\widetilde{\mathbf{V}}<em>i}</em>{i&#x3D;1}^{|x|}\in\mathbb{R}^{|x|\times K\times d}$ is the retrieved contexts. Each token has distinct contexts.</p>
<p>$\mathbf{A}&#x3D;softmax(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d}})\mathbf{V},\mathbf{M}&#x3D;Concat{softmax(\frac{\mathbf{Q}_i\widetilde{\mathbf{K}}_i^T}{\sqrt{d}})\widetilde{\mathbf{V}}<em>i}</em>{i&#x3D;1}^{|x|}$</p>
<p>$\mathbf{H}^{m_s}_{Side}&#x3D;sigmoid(g)·\mathbf{A}+(1-sigmoid(g))·\mathbf{M}:$ the output of <strong>MemAug</strong></p>
<h3 id="Training-Objective"><a href="#Training-Objective" class="headerlink" title="Training Objective"></a>Training Objective</h3><ul>
<li>$P(\mathbf{x}<em>i|\mathbf{x}<em>1,…,\mathbf{x}</em>{i-1})&#x3D;softmax(W\mathbf{H}^{L}</em>{Side}):$ the token probability is computed using the last SideNet hidden states<ul>
<li>$W:$ the frozen output embedding weight shared by LLM and SideNet</li>
</ul>
</li>
<li>$\max\sum_{x\in\mathcal{D}}\sum_{i&#x3D;1}^{|x|}\log P(\mathbf{x}_i|\mathbf{x}<em>1, …, \mathbf{x}</em>{i-1}):$ the training objective of <strong>LongMem</strong><ul>
<li>$x$ is a randomly sampled sentence from the pre-training text corpus $\mathcal{D}$</li>
</ul>
</li>
</ul>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Training-Setup"><a href="#Training-Setup" class="headerlink" title="Training Setup"></a>Training Setup</h3><aside>
👇 Batchfying the training corpora (document-level shuffling within each group)

</aside>

<blockquote>
<p>The conventional process truncates the corpora without padding and conduct a segment-level shuffling.</p>
</blockquote>
<p>Doc Group Num &#x3D; Batch Size</p>
<p>Segment num is same for each group.</p>
<blockquote>
<p>Maybe need padding. Seg num is unlimited.</p>
</blockquote>
<p>In a mini-batch, we select a segment from each group and get total batch-size num of segments.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/longmem-3.png" alt="**Batchfying the large text corpora** into batches to ensure that each consecutive segments within each document is distributed in consecutive batches."></p>
<aside>
👇 Training Corpus and Hyperparameters

</aside>

<p><strong>Corpus</strong>: sample a subset of the <a href="https://arxiv.org/pdf/2101.00027.pdf">Pile</a>.</p>
<p>The pre-training of reproduced GPT-2* iterates on 117B tokens in total, with 512 batch-size and 1024-token fixed segment-length.</p>
<blockquote>
<p>Original GPT-2 adopts absolute position embedding, which is found to perform poorly to enable LLM to learn long-distance dependencies.</p>
</blockquote>
<p><strong>MemAug</strong> is the 9-th layer of SideNet.</p>
<p>$L’&#x3D;24,L&#x3D;12,\alpha&#x3D;\frac{L’}{L}&#x3D;2,H&#x3D;16,d&#x3D;64,$  $csz&#x3D;4,K&#x3D;64,m&#x3D;18$</p>
<blockquote>
<p>The attention keys and values from m&#x3D;18-th layer of backbone LLM is cached into CMB.</p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/longmem-4.png" alt="Memory-Augmented Adaptation and Architectural Hyperparameters."></p>
<p>The pre-training and adaptation are trained on <strong>16 32GB-Tesla-V100 GPUs</strong>.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/longmem-5.png" alt="The superiority of our method over fully dense self-attention (GPT-2*) in terms of **inference speed and GPU-memory utilization**."></p>
<aside>
👇 Memory Retrieval Module

</aside>

<p>The fixed memory-size of <strong>CMB</strong> in one GPU is 65536 key-value pairs of tokens.</p>
<blockquote>
<p>We enable each GPU to construct and update their own memory retrieval module for efficiency.</p>
<p>The capacity of CMB (M) is not unlimited.</p>
</blockquote>
<p>The retrieval takes about 15ms per 1k tokens, which is 55% timecost of LLM forwarding pass.</p>
<p>Use <a href="https://arxiv.org/pdf/1702.08734.pdf">faiss</a> to store the mean-pooled attention keys of $M&#x2F;csz$ chunks and perform efficient retrieval.</p>
<aside>
👇 Baselines

</aside>

<p>Adopt reproduced GPT-2* and <a href="https://arxiv.org/pdf/2203.08913.pdf">MemTRM</a> as two baseline under same pre-training setting.</p>
<p>We insert the knn-augmented layer proposed by <a href="https://arxiv.org/pdf/2203.08913.pdf">MemTRM</a> as the same 18-th layer in the LLM decoder.</p>
<h3 id="Long-Context-Language-Modeling"><a href="#Long-Context-Language-Modeling" class="headerlink" title="Long-Context Language Modeling"></a>Long-Context Language Modeling</h3><aside>
👇 Zero-Shot Evaluation Setting(3 long-context modeling datasets)

</aside>

<p>The majority of included books or papers in these datasets have the length of at least 16k tokens. </p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/longmem-6.png" alt="Dataset Statistics of five splits of PG-22 based on length range and ArXiv."></p>
<p><strong>PG-22(Project Gutenberg 2020-2022 Language Modeling):</strong> We crawled and cleaned the books published between 2020 and 2022 under <a href="https://www.gutenberg.org/">Project Gutenberg Library</a>. </p>
<blockquote>
<p>Our training subset PG-19 only contains books published before 1919.</p>
</blockquote>
<p><strong>ArXiv:</strong> We select a val split of ArXiv paper subset in the <a href="https://arxiv.org/pdf/2101.00027.pdf">Pile</a> corpus. The val split does not exist in our training corpus.</p>
<p><strong><a href="https://arxiv.org/pdf/2204.10878.pdf">ChapterBreak</a>:</strong> We select the Archive of Our Own (AO3) subset. Use its splits of 4k, 6k, and 8k prefix.</p>
<p><em><strong>Details of <a href="https://arxiv.org/pdf/2204.10878.pdf">ChapterBreak</a>:</strong></em></p>
<ul>
<li>For LLMs that cannot process over 4k tokens, we abandon the front prefix to fulfill the maximum input length of LLMs.</li>
<li>For <a href="https://arxiv.org/pdf/2203.08913.pdf">MemTRM</a> and LongMem model, we firstly load the given 4k&#x2F;6k&#x2F;8k prefix contexts into the cached memory(while the input length to local context is still 1k tokens) and then do the scoring.</li>
<li>We use the perplexity as the scorer for each candidate suffix segment in zero-shot evaluation manner.</li>
<li>Then the suffix segment with lower perplexity is selected as the label.</li>
<li>The suffix identification accuracy is used as the evaluation metric.</li>
</ul>
<aside>
👇 Results

</aside>

<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/longmem-7.png" alt="Evaluation results on long-context language modeling datasets. We report token-level perplexity (PPL) (lower the better) on all datasets."></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/longmem-8.png" alt="Zero-shot Suffix Identification Accuracy on AO3 subset of ChapterBreak. Baselines marked with † are directly cited from [ChapterBreak](https://arxiv.org/pdf/2204.10878.pdf)."></p>
<h3 id="Memory-Augmented-In-Context-Learning"><a href="#Memory-Augmented-In-Context-Learning" class="headerlink" title="Memory-Augmented In-Context Learning"></a>Memory-Augmented In-Context Learning</h3><blockquote>
<p>Conventional ICL is heavily restricted by input context length.</p>
</blockquote>
<p>LongMem: <strong>Infinite-length ICL when loading large number of demonstration examples into cached memory.</strong></p>
<aside>
👇 Evaluation Setting

</aside>

<ul>
<li>NLU datasets, 4&#x2F;20-shot: <a href="https://aclanthology.org/D13-1170.pdf">SST-2</a>, <a href="https://www.cs.cornell.edu/home/cardie/papers/lre05withappendix.pdf">MPQA</a>, <a href="http://richard.cyganiak.de/2008/papers/dbpedia-iswc2007.pdf">MR</a>, <a href="https://aclanthology.org/P04-1035.pdf">Subj</a>, <a href="https://aclanthology.org/D13-1170.pdf">SST-5</a></li>
<li>QA dataset, open-ended generation, 3-shot(about 1k tokens): <a href="https://arxiv.org/pdf/1606.05250.pdf">SQuAD</a></li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/longmem-9.png" alt="The hand-crafted **ICL prompt templates**. We concatenate the demonstration examples with newlines to delimit them."></p>
<p>The prediction label is directly generated using greedy decoding.</p>
<p>Chunk size $csz&#x3D;2$</p>
<aside>
👇 Results

</aside>

<p>We report the mean and standard deviation of 6 runs with different random seeds to overcome the randomness in selecting k-shot demonstration examples.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/longmem-10.png" alt="Accuracy [%] on 5 NLU tasks. We sample 2000 extra demonstration examples and load them into cached memory."></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/longmem-11.png" alt="Exact match (EM) and F1 scores on SQuAD. LongMem loads 200 extra demonstration examples into cached memory."></p>
<h3 id="Ablation-Studies"><a href="#Ablation-Studies" class="headerlink" title="Ablation Studies"></a>Ablation Studies</h3><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/longmem-12.png" alt="(a) Effects of chunk size on ICL; (b) Effects of memory size, ∆Perplexity on 4 splits of PG-22 (notice msz=16k)."></p>
<p>In general, the memory size should be compatible with the average length of documents or contexts, i.e., a set of books with average 16k tokens should deploy the memory size of 16k tokens in cached memory.</p>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p><strong>(1) Large Language Models</strong></p>
<p><strong>(2) x-formers</strong></p>
<p>x-former enables transformers to attend on longer context. However, existing x-formers is <strong>not efficient</strong> and their <strong>upper-bound seq_len</strong> is only 16k tokens. </p>
<p><a href="https://arxiv.org/pdf/1901.02860.pdf">Transformer-XL</a> proposes to cache attention keys and values of past segment and reuse them in recurrent manner.</p>
<p><a href="https://arxiv.org/pdf/2006.04768.pdf">LinFormer</a>, <a href="https://arxiv.org/pdf/2004.05150.pdf">LongFormer</a>, <a href="https://arxiv.org/pdf/2003.05997.pdf">Routing Transformer</a> propose various sparse attention mechanisms for decreasing $O(n^2)$ complexity to $O(n log n)$ or even $O(n)$.</p>
<p><a href="https://arxiv.org/pdf/2007.14062.pdf">BigBird</a> achieves a 4k sequence length via attending on a subset of context tokens.</p>
<p><strong>(3) Side-Tuning (<a href="https://arxiv.org/pdf/1912.13503.pdf">paper1</a>, <a href="https://arxiv.org/pdf/2206.06522.pdf">paper2</a>)</strong></p>
<p>Side-Tuning is a <strong>task-specific</strong> tuning method for pre-trained models via training a lightweight side-network that is fused with the fixed pre-trained network via summation.</p>
<blockquote>
<p>In constrast, LongMem has well zero-shot performance.</p>
</blockquote>
<p>Our method distinguishes the side-tuning method in terms of <strong>learning objective</strong> and <strong>cross-network fusion ways</strong>.</p>
<p>LongMem proposes to augment LLMs with decoupled memory for memorizing long past inputs, which does not involve any task-specific tuning.</p>
<p>The cross-network residual connections proposed by LongMem is novel and distincts from the vanilla summation of Side-Tuning.</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>2023</category>
        <category>06</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Side-Tuning</tag>
        <tag>X-Formers</tag>
        <tag>PEFT</tag>
      </tags>
  </entry>
  <entry>
    <title>(Lynx)What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?</title>
    <url>/2023/07/06/Lynx/</url>
    <content><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-0.png"></p>
<p>Paper: <a href="https://arxiv.org/pdf/2307.02469.pdf">https://arxiv.org/pdf/2307.02469.pdf</a></p>
<p>Project Page: <a href="https://lynx-llm.github.io/">https://lynx-llm.github.io/</a></p>
<aside>
👇 Problems

</aside>

<p>The performance of multimodal LLMs heavily relies on network structures, training data, instruction diversity, and evaluation benchmarks, which have not been extensively discussed.</p>
<aside>
👇 Contributions

</aside>

<ol>
<li>Present a systematic and comprehensive study, quantitatively and qualitatively.<ul>
<li><strong>Network Structures</strong><ul>
<li>LLaMA vs Vicuna</li>
<li>Prefix-tuning vs Cross-attention</li>
</ul>
</li>
<li><strong>Training Data</strong>(combinations and sampling strategies)<ul>
<li>Quality vs Quantity</li>
<li>COYO700M, DataComp1B, BlipCapFilt</li>
</ul>
</li>
<li><strong>Instruction Diversity</strong><ul>
<li>500 instructions for over 50 tasks</li>
</ul>
</li>
<li><strong>Evaluation</strong>. Collect the first comprehensive evaluation set including both image and video tasks through crowd-sourcing.</li>
</ul>
</li>
<li>Present Lynx model.</li>
</ol>
<h1 id="1-Lynx"><a href="#1-Lynx" class="headerlink" title="1 - Lynx"></a>1 - Lynx</h1><p><img src="https://lynx-llm.github.io/static/images/lynx.png" alt="Our model is based on prefix-tuning architecture. In contrast to the cross-attention-based models like Flamingo."></p>
<p>Our model is based on prefix-tuning architecture. In contrast to the cross-attention-based models like Flamingo.</p>
<h2 id="1-1-Formulations"><a href="#1-1-Formulations" class="headerlink" title="1.1 - Formulations"></a>1.1 - Formulations</h2><p><strong>Prefix-Tuning</strong>: Visual tokens $\mathbf{w}<em>v&#x3D;{w_i}</em>{i&#x3D;1}^V$ are directly concatenated with instruction tokens $\mathbf{w}<em>l&#x3D;{w_j}</em>{j&#x3D;V+1}^{V+L}$.</p>
<p>Sentence prediction equation: $p(w_{V+L+1:V+L+T}|w_{1:V+L})\sim\prod\limits_{t&#x3D;V+L+1}^{V+L+T}P(w_t|w_{&lt;t})$, ended by <EOS>.</p>
<h2 id="1-2-Details-of-Model-Architecture"><a href="#1-2-Details-of-Model-Architecture" class="headerlink" title="1.2 - Details of Model Architecture"></a>1.2 - Details of Model Architecture</h2><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-1.png" alt="Lynx: train trainable adapters based on frozen Vicuna. Token dim in hidden state is 4096, while 2048 in adapter."></p>
<h3 id="1-2-1-Adapter"><a href="#1-2-1-Adapter" class="headerlink" title="1.2.1 - Adapter"></a>1.2.1 - Adapter</h3><p>The trainable adapters are inserted into the LLMs after every $M$ blocks.($M&#x3D;1$)</p>
<h3 id="1-2-2-Visual-Encoder"><a href="#1-2-2-Visual-Encoder" class="headerlink" title="1.2.2 - Visual Encoder"></a>1.2.2 - Visual Encoder</h3><p>Apply <a href="https://arxiv.org/pdf/2211.07636.pdf">EVA-1B</a> as <strong>Visual Encoder</strong> $\phi_v(x)$ to map an image $x$ with resolution $H\times W$ to a sequence of $\frac{H}{14}\times \frac{W}{14}$ visual tokens.</p>
<p><a href="https://arxiv.org/pdf/2103.03206.pdf">Resampler</a> $\Phi$ mechanism reduces the dimensions of vision inputs.</p>
<p>By injecting the long vision token sequence into a short and <strong>learnable query sequence</strong> $\mathbf{w}_v^q$, we adapt the $\Phi$ to improve the efficiency of training and inference: $\mathbf{w}_v&#x3D;\Phi(\phi_v(x),\mathbf{w}_v^q)$.</p>
<p>$\mathbf{w}_v$ is the <strong>condensed sequence</strong> of 32 tokens.</p>
<h2 id="1-3-Pretraining"><a href="#1-3-Pretraining" class="headerlink" title="1.3 - Pretraining"></a>1.3 - Pretraining</h2><p>Utilize $&gt;120M$ image-text pairs to build inter-modality connections.</p>
<p>Next-word prediction training with the cross entropy loss.(same in finetuing)</p>
<blockquote>
<p>Compared to the contrastive pretraining, pretraining with next-word prediction requires data with fluent texts that can represent the “natural” causal dependency between the predicted word and the past context very well.</p>
</blockquote>
<p>$0\sim100k$ steps for $224\times224$ images, $100k\sim110k$ steps for $420\times420$ images.</p>
<p>After 110k steps, we freeze the visual encoder and <strong>thus the expense of increasing image resolution is affordable</strong>.</p>
<h2 id="1-4-Instruction-Fintuning"><a href="#1-4-Instruction-Fintuning" class="headerlink" title="1.4 - Instruction Fintuning"></a>1.4 - Instruction Fintuning</h2><p>We collect an instruction finetuning multi-modal dataset based on the public ones.</p>
<p>Different <strong>weight combinations</strong> of instruction data have a crucial influence on the final performance.</p>
<aside>
👉 (1) 50+ text-only, image-text, video-text tasks belonging to **5 Categories**:

</aside>

<ol>
<li>Text-only Instruction-Following</li>
<li>Image&#x2F;Video Visual Question Answering</li>
<li>Image&#x2F;Video Captioning</li>
<li>Classification</li>
<li>Image-conditioned Dialog for Complex Reasoning and Instruction Following</li>
</ol>
<p><strong>Provide corresponding instructions for each task:</strong></p>
<p>Manually label ≥3 instructions and prompt GPT4 to automatically generate more.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Here are some instructions that define a visual-language task. Continue to write 15 instructions with the same meaning: 1) PROMPT1; 2) PROMPT2; 3) PROMPT3;</span><br></pre></td></tr></table></figure>

<aside>
👉 (2) available public instruction data: FlanT5, Alpaca, Mini-GPT4, LLaVA, Baize

</aside>

<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-2.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-3.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-4.png" alt="**Training Data.** $\sim14B$ tokens for the pretraining, $\sim3B$ for the instruction-finetuning. Ratio indicates weight strategy."></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-5.png" alt="**Training hyperparameters**. Some parameters not use learning rate decay schedule. Use the DeepSpeed to accelerate training, and set the BFloat16 as the default model precision."></p>
<h1 id="2-Experiment"><a href="#2-Experiment" class="headerlink" title="2 - Experiment"></a>2 - Experiment</h1><p><strong>Description-first strategy</strong>: Before sending the request from the user, we feed a fixed prompt “Describe the image in detail” first in the “0th” round of the conversation. After that, the user’s instructions will be sequentially processed.</p>
<p>During the deployment, this strategy improves performance of most models(but not for MiniGPT4).</p>
<p>For MiniGPT4, we generated the response with its default settings.</p>
<p>For mPLUG-owl, we follow the default parameters presented at <a href="http://vlarena.opengvlab.com/">http://vlarena.opengvlab.com/</a>.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-6.png" alt="Hyper-parameters for Generation during the deployment. We set hyper-parameters to encourage short response generation for Open-VQA."></p>
<h2 id="2-1-Evaluation-Protocols"><a href="#2-1-Evaluation-Protocols" class="headerlink" title="2.1 - Evaluation Protocols"></a>2.1 - Evaluation Protocols</h2><aside>
👉 (1) Open-VQA(ours); (2) [OwlEval](https://arxiv.org/pdf/2304.14178.pdf) from mPLUG-Owl; (3) [MME](https://arxiv.org/pdf/2306.13394.pdf)

</aside>

<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-7.png" alt="Manually collected **Open-VQA** supports **open-ended answers**, which contains diverse questions on objects, OCR, counting, reasoning, action recognition, chronological ordering, etc."></p>
<p><strong>Open-VQA</strong> consists of 450 samples, based on VQA 2.0, OCRVQA, Place365, MSVD, MSRVTT, SthV2.</p>
<p>Though Place365 is a classification task and SthV2 is a video captioning task, we write proper prompts to make them both VQA tasks.</p>
<p><strong>Use GPT4 as judger via following prompt:</strong></p>
<blockquote>
<p>GPT4 achieves a consistency of more than 95% compared with humans. (We evaluate the consistency on 100 samples from a randomly selected subset with our model.)</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Given the question “QUESTION”, does the answer “PREDICTION” imply the answer “GROUND_TRUTH”? Answer with Yes or No.</span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-8.png" alt="To make the comparison fair, we pad each image in the **OwlEval** with 8 pixels. "></p>
<p>Recruit <strong>humans</strong> to evaluate the quality(first <strong>correctness</strong>, then <strong>richness</strong>) of language generation.</p>
<p><strong>Scores</strong> range from 1 to 5 with <strong>2 rules</strong>:</p>
<ol>
<li>At most 2 models that gain equal scores</li>
<li>for each annotator, total tie num ≤ 10 for the whole set</li>
</ol>
<h2 id="2-2-Quantitative-Experiments"><a href="#2-2-Quantitative-Experiments" class="headerlink" title="2.2 - Quantitative Experiments"></a>2.2 - Quantitative Experiments</h2><aside>
👉 The key to **balance language generation and correctness** is a high-quality VL dataset that (1) includes high quality and fluent texts (2) aligns the texts and images well

</aside>

<ul>
<li>If a model has lower accuracy on Open-VQA, it tends to make factual errors inconsistent with the given image during text generation. (under-training on VL tasks)</li>
<li>Models with higher performance on Open-VQA usually tend to lose language generation ability, e.g., generate short sentences. (over-training on VL tasks)</li>
</ul>
<p><img src="https://lynx-llm.github.io/static/images/result_1.png" alt="Compare existing multi-modal LLMs on the Open-VQA image benchmark. InstructBLIP and Lynx achieve high performance."></p>
<p>Compare existing multi-modal LLMs on the Open-VQA image benchmark. InstructBLIP and Lynx achieve high performance.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-9.png" alt="Different from InstructBLIP, Lynx is more user-friendly."></p>
<p><img src="https://lynx-llm.github.io/static/images/result_all.png" alt="(b) MME: All scores are normalized to the range from 0 to 100. (d) Ablation study on our Open-VQA videos."></p>
<p>(b) MME: All scores are normalized to the range from 0 to 100. (d) Ablation study on our Open-VQA videos.</p>
<p>Lynx is good at MME perception tasks including Color, Celebrity, Scene, Landmark, Position, Count, and Existence.</p>
<h2 id="2-3-Ablation-Study"><a href="#2-3-Ablation-Study" class="headerlink" title="2.3 - Ablation Study"></a>2.3 - Ablation Study</h2><aside>
👉 What matters to train a high-performance GPT4-style model?

</aside>

<blockquote>
<p>A GPT4-style LLM is defined as a decoder-only transformer that takes both visual and instructional tokens as inputs and generates responses in text auto-regressively.</p>
</blockquote>
<ul>
<li><p><strong>LLaMA</strong>(language generation) <strong>vs. Vicuna</strong>(correctness, instruction-following ability)</p>
</li>
<li><p><strong>Impact of Diversified Instructions</strong>(during training)</p>
</li>
<li><p><strong>Impact of Training Data</strong>(noise: COYO700M, DataComp1B)</p>
<blockquote>
<p>attribute the worse results to the difference between generative pretraining and contrastive pretraining.</p>
</blockquote>
</li>
<li><p><strong>Prefix-Tuning vs. Cross-Attn</strong> (follow Open-Flamingo)</p>
<blockquote>
<p>We only use multi-modal instruction data for pre-training. For the finetuning stage, we experiment with two variants, with or without trainable LLM, i.e., with or without the use of text instruction data.</p>
</blockquote>
</li>
<li><p><strong>Impact of Larger Image Resolution</strong></p>
</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-10.png" alt="Ablation study on our Open-VQA images."></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-11.png" alt="Human evaluation of different ablation models. (a) w/ LLaMA vs w/ Vicuda; (b) w/o diversified instructions vs w/ diversified prompts; (c) w/ large-scale noisy data vs w/o large-scale noisy data; (d) prefix-finetuning vs cross-attention."></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-12.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Lynx-13.png" alt="w/ diversified prompts versus w/o diversified prompts."></p>
<h1 id="3-Related-Work"><a href="#3-Related-Work" class="headerlink" title="3 - Related Work"></a>3 - Related Work</h1><p><strong>(1) Centralized Multi-modal Interactive System</strong></p>
<ul>
<li>【existing work】Visual ChatGPT, MM-REACT, HuggingGPT, InternGPT, SayCan, InnerMonologue</li>
<li>【adavantage】address problems that are well-defined</li>
<li>【limit】lack zero-shot ability to handle open-ended instructions</li>
</ul>
<p><strong>(2) End-to-end Multi-modal LLMs</strong></p>
<blockquote>
<p>adding some additional trainable parameters</p>
</blockquote>
<ul>
<li>【type 1】Cross-Attention(Flamingo)</li>
<li>【type 2】directly concatenate visual and textual tokens<ul>
<li>BLIP2: Q-Former</li>
<li>PaLM-E: no fixed layers</li>
<li>Mini-GPT4: projection layer</li>
<li>LLaVA: tunes LLM during the instruction finetuning stage</li>
<li>mPLUG-Owl: first stage tunes vision encoder, second stage tunes LLM</li>
<li>Kosmos-1: train a LLM from scratch</li>
</ul>
</li>
</ul>
<h1 id="4-Conclusions"><a href="#4-Conclusions" class="headerlink" title="4 - Conclusions"></a>4 - Conclusions</h1><h2 id="4-1-Findings-and-Takeaways"><a href="#4-1-Findings-and-Takeaways" class="headerlink" title="4.1 - Findings and Takeaways"></a>4.1 - Findings and Takeaways</h2><ul>
<li>Prefix-tuning may be the currently best way to multi-modal adaptation for LLMs.</li>
<li>Multi-modal LLMs are not as instruction-following as LLMs.</li>
<li>The quality of training data is critical to model performance.</li>
<li>Diverse tasks and prompts are crucial for zero-shot abilities.</li>
<li>Balancing the correctness and language generation ability is important.</li>
</ul>
<h2 id="4-2-Limitations"><a href="#4-2-Limitations" class="headerlink" title="4.2 - Limitations"></a>4.2 - Limitations</h2><ul>
<li><strong>Training Data</strong>.<ul>
<li>Struggle to <strong>balance different abilities</strong>(correctness&amp;language generation, long&amp;short answer)</li>
<li>no available image-text datasets that contain <strong>long texts</strong> which are ideal for pretraining</li>
<li>do not find the <strong>optimal data combination</strong> strategy restricted by computational resource</li>
</ul>
</li>
<li><strong>Safety</strong>. We do not conduct safety checks and restrictions(e.g., ethical, political, and racism issues).</li>
</ul>
<h2 id="4-3-Future-Work"><a href="#4-3-Future-Work" class="headerlink" title="4.3 - Future Work"></a>4.3 - Future Work</h2><ul>
<li>Scale up model size</li>
<li>a high-quality VL dataset for training<ul>
<li>Larger and more diverse instructional tasks</li>
<li>Find optimal data combination</li>
</ul>
</li>
<li>More comprehensive evaluation</li>
<li>Multi-Lingual</li>
<li>Safety</li>
</ul>
]]></content>
      <categories>
        <category>Paper</category>
        <category>2023</category>
        <category>07</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Prefix-Tuning</tag>
        <tag>Adapter</tag>
        <tag>Dataset</tag>
        <tag>Instruction-Following</tag>
        <tag>Multi-modal</tag>
        <tag>Evaluation</tag>
      </tags>
  </entry>
  <entry>
    <title>MIMIC-IT: Multi-Modal In-Context Instruction Tuning</title>
    <url>/2023/07/04/MIMIC-IT/</url>
    <content><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-0.png"></p>
<p>MIMIC-IT Paper: <a href="https://arxiv.org/pdf/2306.05425.pdf">https://arxiv.org/pdf/2306.05425.pdf</a></p>
<p>Otter Paper: <a href="https://arxiv.org/pdf/2305.03726.pdf">https://arxiv.org/pdf/2305.03726.pdf</a></p>
<p>Code: <a href="https://github.com/Luodian/Otter">https://github.com/Luodian/Otter</a></p>
<p>Project Page: <a href="https://otter-ntu.github.io/">https://otter-ntu.github.io/</a></p>
<aside>
👇 Problems

</aside>

<p>Current VL instruction-response pairs in terms of quantity, diversity and creativity are limited, posing challenges to the generalization of interactive VLMs.</p>
<aside>
👇 Contributions

</aside>

<ul>
<li><strong>MIMIC-IT</strong>: dataset with 2.8M instruction-response pairs</li>
<li><strong>Syphus</strong>: automatic instruction-response generation pipeline<blockquote>
<p>Its filtering does not consider whether the visual info and textual info are paired or not.</p>
</blockquote>
</li>
<li><strong>Otter</strong>: improve OpenFlamingo in perception, reasoning, and planning via finetuning it on MIMIC-IT</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT%20Overview.jpg" alt="MIMIC-IT overview. Each instruction is accompanied by multi-modal conversational context."></p>
<h1 id="1-Related-Work-Multi-modal-Instruction-Tuning-Dataset"><a href="#1-Related-Work-Multi-modal-Instruction-Tuning-Dataset" class="headerlink" title="1 - Related Work(Multi-modal Instruction Tuning Dataset)"></a>1 - Related Work(Multi-modal Instruction Tuning Dataset)</h1><blockquote>
<p>One potential reason for the zero-shot performance gain by instruction tuning is that it internalizes the <a href="https://arxiv.org/pdf/2209.15189.pdf">context</a>, which is preferred in user interactions especially when user input skips commonsense context.</p>
</blockquote>
<p><a href="https://arxiv.org/pdf/2212.10773.pdf">Multi-Instruct</a>: Initially introduces the notion of instruction tuning in multi-modal models</p>
<p><a href="https://arxiv.org/pdf/2304.10592.pdf">Mini-GPT4</a>: creates its instruction-based dataset by merging Conceptual Caption, SBU, and LAION with handwritten instruction templates.</p>
<aside>
👇 [LLaVA-Instruct-150K](https://arxiv.org/pdf/2304.08485.pdf) elevates the quality of VL instruction-following datasets.

</aside>

<p>Utilizing self-instruct and handwritten seed instructions, <strong>LLaVA-Instruct-150K</strong> obtains instructions and responses from GPT-4 based on COCO image captions and object bounding boxes.</p>
<p><em><strong>Its three limitations:</strong></em></p>
<ol>
<li><strong>Limited visual diversity</strong>: only COCO images.</li>
<li><strong>Single image as visual data</strong>: cannot process multiple images or videos.</li>
<li><strong>Language-only in-context information</strong>: no multi-modal in-context info</li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-1.png" alt="LLaVA-Instruct-150K: Language-only In-context."></p>
<aside>
👇 Three characteristics of MIMIC-IT

</aside>

<blockquote>
<p>While these previous works focuse on general scene images, MIMIC-IT categorizes our data sources into indoor scenes, outdoor scenes, conversations, and egocentric videos.</p>
</blockquote>
<ol>
<li><strong>Diverse visual scenes:</strong> incorporating images and videos from general scenes, egocentric view scenes, and indoor RGB-D images.</li>
<li><strong>Multiple images (or a video) as visual data</strong>: supporting instruction-response pairs accompanied by any number of images or videos.</li>
<li><strong>Multi-modal in-context information</strong>: including multiple instruction-response pairs and multiple images or videos</li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-2.png" alt="MIMIC-IT: Multi-modal In-context. Left part is in-context, right part is the query."></p>
<h1 id="2-Multi-modal-In-Context-Instruction-Tuning-Dataset"><a href="#2-Multi-modal-In-Context-Instruction-Tuning-Dataset" class="headerlink" title="2 - Multi-modal In-Context Instruction Tuning Dataset"></a>2 - Multi-modal In-Context Instruction Tuning Dataset</h1><h2 id="2-1-MIMIC-IT-Data-Format"><a href="#2-1-MIMIC-IT-Data-Format" class="headerlink" title="2.1 - MIMIC-IT Data Format"></a>2.1 - MIMIC-IT Data Format</h2><p>$I_q$ denotes the $q$-th instruction, $R_q$ is the response, $X_q$ refers to images or videos.</p>
<blockquote>
<p>Videos can be viewed as ordered sequences of images.</p>
</blockquote>
<p>A <strong>query example</strong> is a tuple $(I_q, R_q, X_q)$, where ${x_{j&#x3D;1}^N}\in X_q$, $N$ is image num.</p>
<aside>
👆 $p_{\theta}(R_q|(I_q,X_q))$ is the **standard instruction-tuning process** with trainable params $\theta$.

</aside>

<p>$C_{\psi}:(I_q,X_q)\mapsto{(I_k,X_k)}_{k&#x3D;1}^M$ are $M$ <strong>in-context examples</strong> of current query example.(The function is task-dependent.)</p>
<p><strong>MIMIC-IT Data Format</strong>: $d_q&#x3D;(I_q,R_q,X_q,C_{\psi}(I_q,X_q)),d_q\sim D_{MIMIC-IT}$.</p>
<aside>
👆 $p_{\theta}(R_q|(I_q,X_q,C_{\psi}(I_q,X_q)))$ incorporates in-context examples.

</aside>

<h2 id="2-2-Sythus-Automatic-Instruction-Response-Generation-Pipeline"><a href="#2-2-Sythus-Automatic-Instruction-Response-Generation-Pipeline" class="headerlink" title="2.2 - Sythus: Automatic Instruction-Response Generation Pipeline"></a>2.2 - Sythus: Automatic Instruction-Response Generation Pipeline</h2><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-3.png" alt="Sythus overview. Step 4 expands pairs into 8 languages via GPT—— Chinese (zh), Japanese (ja), Spanish (es), German (de), French (fr), Korean (ko), and Arabic (ar)."></p>
<p><strong>System messages</strong> define the desired tone and style of the generated instruction-response(IR) pairs.</p>
<p><strong>Visual annotations</strong> provide essential image information such as bounding boxes, captions, timestamps.</p>
<p><strong>In-context examples</strong> assist ChatGPT in learning within the context.</p>
<p><strong>Cold-start stage</strong> is before the large-scale query. This stage identifies the optimal system message and in-context example for each specific task via prompting ChatGPT.</p>
<p><strong>Safety and Ethical Filtering</strong>: The GPT content policy eliminates output that is suspicious for unfair opportunities, stereotyping, overrepresentation&#x2F;underrepresentation, explicit content, disinformation, or unreliable information.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-4.png" alt="System message for TV show Captions (TVC) query."></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-5.png" alt="In-context exemplars for TV show Captions (TVC) query."></p>
<h2 id="2-3-Visual-Data-Exploration"><a href="#2-3-Visual-Data-Exploration" class="headerlink" title="2.3 - Visual Data Exploration"></a>2.3 - Visual Data Exploration</h2><p><strong>Why we leverage existing datasets?</strong></p>
<p>(1) high-quality visual annotations (2) align with real-world distribution</p>
<blockquote>
<p>In each sub-task, we elaborate on the process of organizing various data into an in-context instruction tuning format.</p>
</blockquote>
<h3 id="2-3-1-General-Scene-Understanding"><a href="#2-3-1-General-Scene-Understanding" class="headerlink" title="2.3.1 - General Scene Understanding"></a>2.3.1 - General Scene Understanding</h3><aside>
👇 **LLaVA-Interleaved (LA-I)**.

</aside>

<p>Retrieve ten in-context examples for each instruction-response pair in <a href="https://arxiv.org/pdf/2304.08485.pdf">LLaVA-Instruct-150K</a> based on instruction text2text similarity(<strong>TTS</strong>) or image2image similarity(<strong>IIS</strong>).</p>
<p>Trained on the LA task, the model exhibits exceptional scene comprehension, reasoning abilities, and multi-round conversation capabilities.</p>
<aside>
👇 **Spot The Difference (SD)**. Identify scene differences between the paired images with varying complexity levels.

</aside>

<ul>
<li><strong>General Scene Difference</strong>: (1) create image pairs from the <a href="https://arxiv.org/pdf/1405.0312.pdf">COCO2017</a> via <strong>IIS</strong> (2) use image captions and object detection annotations</li>
<li><strong>Subtle Difference</strong>: (1) image pairs from <a href="https://arxiv.org/pdf/1808.10584.pdf">Spot-the-Diff</a>(extracted from surveillance footage) (2) employ natural language difference descriptions as annotations</li>
</ul>
<aside>
👇 **Visual Story Telling (VIST)**. Generate coherent and engaging narratives based on visual input.

</aside>

<p><a href="https://arxiv.org/pdf/1604.03968.pdf">Visual Storytelling</a> includes event-based image sequences and corresponding inquiry questions. </p>
<p>Given that image annotations often contain narratives and timelines not directly observable, we instruct ChatGPT to act as a viewer answering questions about the images. </p>
<p>The prompts also incorporate thought-provoking inquiries to promote creativity. </p>
<aside>
👇 **Dense Captions (DC)**.

</aside>

<p>Expanding the scope of video understanding, DC features dense captions from <a href="https://arxiv.org/pdf/1705.00754.pdf">DenseCaption&#x2F;Activity caption</a> corresponding to clips within longer videos. The instructions pose a diverse set of questions, addressing the general visual content of the video, human actions, and behaviors, the chronological sequence of events, and causal relationships. This approach encourages VLMs to delve deeper into the intricacies of video content. </p>
<aside>
👇 **TV Show Captions (TVC)**.

</aside>

<p>The primary purpose of incorporating TV show clips with high-level captions into the training process of VLMs is to enhance their social reasoning abilities and deepen their understanding of complex character dynamics. By organizing drama clips from <a href="https://arxiv.org/pdf/2001.09099.pdf">TVR</a> to analyze character relationships and motivations, we aim to challenge VLMs to move beyond mere perception and demonstrate their reasoning capabilities within the context of TV show narratives. This focused approach is crucial for fostering advanced VLMs capable of effectively handling diverse real-world situations and user queries.</p>
<h3 id="2-3-2-Egocentric-View-Understanding"><a href="#2-3-2-Egocentric-View-Understanding" class="headerlink" title="2.3.2 - Egocentric View Understanding"></a>2.3.2 - Egocentric View Understanding</h3><aside>
👇 **Indoor Event Planning (IEP)**. Planning capabilities for diverse 2D room photos.

</aside>

<p><strong>2D Photos Sampling</strong>: We gather indoor scene RGB-D images from <a href="https://arxiv.org/pdf/1702.04405.pdf">ScanNetv2</a> and sample them into multiple 2D visual inputs, representing a room’s layout from a first-person perspective.</p>
<p><strong>Instructions Generation</strong> via prompting ChatGPT: direct humans to perform various activities in indoor spaces.</p>
<ul>
<li>Initially, we have ChatGPT create a personality for the room owner.</li>
<li>Subsequently, the planning should be intimately related to the room’s layout and the generated room owner, underlining the importance of context awareness in VLMs.</li>
</ul>
<aside>
👇 [Ego4D (E4D, videos)](https://arxiv.org/pdf/2110.07058.pdf). For first-person augmented reality (AR) headset assistant applications.

</aside>

<p>By prompting ChatGPT to generate instructions based on visual descriptions, our goal is to simulate practical interactions between users and AR assistants.</p>
<p>An instance of assistant-related questions and  context-aware responses:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Instruction: What should I do now?</span><br><span class="line">Response: Based on my observation, you can now proceed to do....</span><br></pre></td></tr></table></figure>

<h2 id="2-4-Dataset-Statistics"><a href="#2-4-Dataset-Statistics" class="headerlink" title="2.4 - Dataset Statistics"></a>2.4 - Dataset Statistics</h2><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-6.png" alt="Comparison between MIMIC-IT and other multi-modal instruction datasets. MIMICIT features: (1) The largest. (2) Including video data. (3) In-context scenarios. (4) Multilingual. "></p>
<p><strong>2.8M instruction-response pairs</strong>(2.2M unique instructions): each pair includes at least one multi-modal in-context example and one language-only in-context example.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-7.png" alt="The data statistics of MIMIC-IT. (c) retains 25% of Ego4D instructions for a more balanced distribution."></p>
<p>(a) and (b) examine the characteristics and diversity using <a href="https://github.com/explosion/spacy-models/releases/tag/en_core_web_md-3.5.0">spaCy</a> to get top 20 most frequent root verbs alongside their top 4 direct noun objects.</p>
<p>(c) demonstrates diversity in terms of instructions&#x2F;responses length, image num per instruction, and in-context examples num per instruction.</p>
<h1 id="3-Empricial-Evaluation"><a href="#3-Empricial-Evaluation" class="headerlink" title="3 - Empricial Evaluation"></a>3 - Empricial Evaluation</h1><h2 id="3-1-Otter-A-Multi-Modal-In-Context-Instruction-Tuned-Model"><a href="#3-1-Otter-A-Multi-Modal-In-Context-Instruction-Tuned-Model" class="headerlink" title="3.1 - Otter: A Multi-Modal In-Context Instruction Tuned Model"></a>3.1 - Otter: A Multi-Modal In-Context Instruction Tuned Model</h2><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Otter.png"></p>
<p><strong>Language Encoder</strong>: LLaMA-7B(frozen)</p>
<p><strong>Vision Encoder</strong>: CLIP ViT-L&#x2F;14(frozen)</p>
<p><strong>Perceiver Resampler Module</strong>: ~1.3B trainable parameters</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># [] indicates special word, &lt;&gt; indicates data slot</span><br><span class="line"># role label: User/GPT</span><br><span class="line"># format the training data as follows(chatbot-like format):</span><br><span class="line">&lt;context&gt; [image] User:&lt;instruction&gt; GPT:[answers] &lt;answer&gt;.[endofchunk]</span><br></pre></td></tr></table></figure>

<blockquote>
<p>To support user-assistant conversations, we adopt “GPT” as the role label because it does not have any specific semantic meaning in vocabulary.</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Hyperparameters</span><br><span class="line">optimizer: AdamW</span><br><span class="line">starting learning rate(lr): 10^&#123;-5&#125;</span><br><span class="line">batch size: 4</span><br><span class="line">epoch: 6</span><br><span class="line">lr scheduler: cosine annealing scheduler</span><br><span class="line">gradient clipping threshold: 1.0</span><br><span class="line">loss: cross-entropy</span><br></pre></td></tr></table></figure>

<aside>
👇 HuggingFace

</aside>

<ul>
<li><a href="https://huggingface.co/luodian/OTTER-Image-LLaMA7B-LA-InContext">OTTER-Image-LLaMA7B-LA-InContext</a>: as described in paper</li>
<li><a href="https://huggingface.co/luodian/OTTER-Image-MPT7B">OTTER-Image-MPT7B</a>: Tune OpenFlamingv2 to enable generation abilities for both long and short answers.</li>
<li><a href="https://huggingface.co/luodian/OTTER-Video-LLaMA7B-DenseCaption">OTTER-Video-LLaMA7B-DenseCaption</a></li>
<li><a href="https://huggingface.co/luodian/OTTER-Video-LLaMA7B-FunQA">OTTER-Video-LLaMA7B-FunQA</a></li>
</ul>
<blockquote>
<p><strong>Otter-Image</strong> supports multiple images input as in-context examples, which is the first multi-modal instruction tuned model that supports to organize inputs this way.</p>
</blockquote>
<blockquote>
<p><strong>Otter-Video</strong> supports videos inputs (frames are arranged as original Flamingo’s implementation) and multiple images inputs (they serve as in-context examples for each other).</p>
</blockquote>
<aside>
👇 Engineering Work

</aside>

<p>We have <a href="https://huggingface.co/luodian/OTTER-9B-INIT">integrated Otter into Hugging Face Transformers</a> and trained it using the <a href="https://huggingface.co/docs/accelerate/index">Hugging Face Accelerator</a>.</p>
<p>We use <strong>bf16 mixed precision</strong> and train Otter on <strong>4×RTX-3090 GPUs(24GB)</strong>.</p>
<blockquote>
<p>reduced from 1× A100 GPU</p>
</blockquote>
<p>We provide the support of Fully Sharded Data Parallel (FSDP) and DeepSpeed.</p>
<p>We provide a <strong>script for converting</strong> the original OpenFlamingo-9B checkpoint into the Hugging Face Model format.(<a href="https://huggingface.co/luodian/openflamingo-9b-hf">luodian&#x2F;openflamingo-9b-hf</a>)</p>
<h2 id="3-2-Usage-Examples-and-Demonstrations"><a href="#3-2-Usage-Examples-and-Demonstrations" class="headerlink" title="3.2 - Usage Examples and Demonstrations"></a>3.2 - Usage Examples and Demonstrations</h2><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Otter-example.png" alt="**Otter’s response examples in different scenarios**. Otter is able to serve for situation understanding and reasoning, learning with in-context examples, and egocentric visual assistant."></p>
<p><strong>Otter’s response examples in different scenarios</strong>. Otter is able to serve for situation understanding and reasoning, learning with in-context examples, and egocentric visual assistant.</p>
<p><strong>Scene Understanding and Reasoning</strong>.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;image&gt;Human:&#123;instruction&#125; Assistant:&lt;answer&gt;&#123;model-generated response&#125;&lt;endofchunk&gt;</span><br></pre></td></tr></table></figure>

<p><strong>Learning with In-context Examples</strong>.</p>
<p>The organized input data format(on the LA-T2T task) is as follows:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Multiple in-context example with similar instructions</span><br><span class="line">&lt;image&gt;Human:&#123;instruction&#125; Assistant:&lt;answer&gt;&#123;response&#125;&lt;|endofchunk|&gt;</span><br><span class="line"># ....</span><br><span class="line">&lt;image&gt;Human:&#123;instruction&#125; Assistant:&lt;answer&gt;&#123;response&#125;&lt;|endofchunk|&gt;</span><br><span class="line"># Query example</span><br><span class="line">&lt;image&gt;Human:&#123;instruction&#125; Assistant:&lt;answer&gt;</span><br></pre></td></tr></table></figure>

<p><strong>Egocentric Visual Assistant(Otter-E)</strong>.</p>
<p>Otter-E is specifically designed for AR headset applications.</p>
<p>In real-life scenarios, you are not encouraged to consult visual assistants for such hazardous actions.</p>
<h2 id="3-3-ChatGPT-Evaluation"><a href="#3-3-ChatGPT-Evaluation" class="headerlink" title="3.3 - ChatGPT Evaluation"></a>3.3 - ChatGPT Evaluation</h2><blockquote>
<p>Current evaluation metrics for VL models, like VQAv2 acc, exhibit shortcomings in terms of robustness.</p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-8.png" alt="**[MMAGIBench](https://github.com/open-mmlab/mmagibench) evaluation results** judged by ChatGPT."></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/MIMIC-IT-9.png" alt="(a) ChatGPT evaluation. (b) Human evaluation comparison. (c) Few-shot in-context learning evaluation on [COCO caption (CIDEr)](https://arxiv.org/pdf/1405.0312.pdf)."></p>
<h2 id="3-4-Human-Evaluation"><a href="#3-4-Human-Evaluation" class="headerlink" title="3.4 - Human Evaluation"></a>3.4 - Human Evaluation</h2><p><a href="https://github.com/OpenGVLab/Multi-Modality-Arena">Multi-Modality Arena</a> uses an Elo rating(higher is better) system to evaluate <strong>the usefulness and alignment</strong> of VLM responses.</p>
<p>The system calculates the relative skill levels of players.</p>
<p>This system works well for evaluating conversational AI models, because multiple models can have pairwise “battles” responding to the same inputs in a user-blind evaluation.</p>
<h2 id="3-5-Few-shot-In-Context-Learning-Metric-Evaluation"><a href="#3-5-Few-shot-In-Context-Learning-Metric-Evaluation" class="headerlink" title="3.5 - Few-shot In-Context Learning Metric Evaluation"></a>3.5 - Few-shot In-Context Learning Metric Evaluation</h2><p>As expected, the finetuning also brings marginal performance gain on zero-shot evaluation.</p>
<h1 id="4-Conclusion"><a href="#4-Conclusion" class="headerlink" title="4 - Conclusion"></a>4 - Conclusion</h1><aside>
👇 **Limitations**

</aside>

<p>For MIMIC-IT:</p>
<p>ChatGPT is prone to language hallucinations, therefore it might generate incorrect responses.</p>
<p>For Otter:</p>
<ul>
<li><strong>Object hallucinations</strong>(objects mentioned in text inputs do not appeare in images or videos)</li>
<li><strong>Language hallucinations</strong> from OpenFlamingo is inherited by Otter.</li>
</ul>
<aside>
👇 **Future Works**

</aside>

<p>For MIMIC-IT:</p>
<ul>
<li>More trustworthy LMs or generation techniques for self-instruct data generation.</li>
<li>More embodied AI datasets such as <a href="https://arxiv.org/pdf/2210.06407.pdf">LanguageTable</a> and <a href="https://arxiv.org/pdf/2204.01691.pdf">SayCan</a>.</li>
</ul>
<p>For Otter:</p>
<ul>
<li>introducing negative examples in the training data to reduce hallucination issue</li>
<li>more efficient training schemas(e.g., LoRA)</li>
<li>more modalities.</li>
</ul>
]]></content>
      <categories>
        <category>Paper</category>
        <category>2023</category>
        <category>06</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Dataset</tag>
        <tag>Instruction-Following</tag>
        <tag>Multi-modal</tag>
        <tag>In-Context Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>A Survey on Multimodal Large Language Models</title>
    <url>/2023/06/30/MLLM/</url>
    <content><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-author.png"></p>
<p>Paper: <a href="https://arxiv.org/pdf/2306.13549.pdf">https://arxiv.org/pdf/2306.13394.pdf</a></p>
<p>Code: <a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models">https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models</a></p>
<aside>
💡 ***Why we need MLLM?***

</aside>

<p><strong>MLLM</strong>: the LLM-based model with the ability to receive and reason with multimodal information</p>
<p><strong>Three Advantages</strong>: (1) more in line with human perception (2) more user-friendly interface (3) more well-rounded task-solver</p>
<p><strong>Three Example Capabilities</strong>: (1) write website codes based on images (2) understand deep meaning of memes (3) OCR-free math reasoning</p>
<p><strong>Four Key Techniques</strong>: M-IT, M-ICL, M-CoT, LAVR</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-1.png"></p>
<hr>
<h2 id="M-IT-Multimodal-Instruction-Tuning"><a href="#M-IT-Multimodal-Instruction-Tuning" class="headerlink" title="M-IT: Multimodal Instruction Tuning"></a>M-IT: Multimodal Instruction Tuning</h2><h3 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h3><p><strong>Instruction</strong>: the description of tasks</p>
<p><strong>Instruction Tuning</strong>: a technique of finetuning LLMs on instruction-formatted datasets</p>
<p><strong>Instruction Tuning 3 Advantages:</strong></p>
<ol>
<li><p>better understand and respond to various human requests</p>
</li>
<li><p>zero-shot generalization to new tasks</p>
</li>
<li><p>non-experts can use natural language to interact with LLMs</p>
<blockquote>
<p><em>The most popular programming language in the future will be English.</em></p>
</blockquote>
</li>
</ol>
<p><strong>Instruction Tuning &amp; typical learning paradigms.</strong> A: large-scale task-specific training data; B: zero-shot performance is still quite average</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-2.png"></p>
<p><strong>Extend from Unimodality to Multimodality</strong>. Predict the next token of the response. The instruction template is flexible and subject to manual designs. It can also be generalized to multi-round instructions.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-3.png"></p>
<h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-4.png"></p>
<aside>
👉 **Benchmark Adaptation**

</aside>

<p><strong>Instruction templates for VQA datasets</strong>. &lt;image&gt; and {Question} are the image and the question in the original VQA datasets, respectively. Utilize existing benchmark datasets to construct instruction-formatted datasets.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-5.png"></p>
<p><strong>Directly using concise answers may limit the output length of  MLLM</strong>.</p>
<ul>
<li>Modify Instructions: ChatBridge, InstructBLIP</li>
<li>Extend the length of existing answers: $𝑀^3 𝐼𝑇$</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-6.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-7.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-8.png"></p>
<aside>
👉 **Self-Instruction**

</aside>

<p><strong>Self-Instruction</strong>: To meet real-world human needs, such as multiple rounds of conversations.</p>
<ul>
<li><strong>LLaVA-Instruct-150k</strong>:  an M-IT dataset</li>
<li><strong>MiniGPT-4, ChatBridge, GPT4Tools, DetGPT</strong>: develop different M-IT datasets catering for different needs</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-9.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-10.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-11.png"></p>
<aside>
👉 **Hybrid Composition**

</aside>

<p>M-IT data + language-only user-assistant conversation data.</p>
<p><strong>Adavantage</strong>: conversational proficiencies, instruction-following abilities</p>
<p><strong>Representative work</strong>:</p>
<ul>
<li><strong>LaVIN</strong>: directly constructs a minibatch by randomly sampling from both data</li>
<li><strong>MultiInstruct</strong>: two transfer learning strategies<ul>
<li>Mixed instruction tuning: combine both types of data and randomly shuffle. (The empirical results show that mixed instruction tuning is at least not worse than solely tuning on multimodal data.)</li>
<li>Sequential instruction tuning: text data followed by multimodal data</li>
</ul>
</li>
</ul>
<h3 id="Modality-Bridging"><a href="#Modality-Bridging" class="headerlink" title="Modality Bridging"></a>Modality Bridging</h3><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-12.png"></p>
<aside>
👉 **Learnable Interface**

</aside>

<p><strong>Adavantages</strong>: little cost, avoid catastrophic forgetting</p>
<p><strong>Three manners</strong>:</p>
<ol>
<li><strong>Query-Based</strong>: (learnable query tokens) Flamingo, BLIP-2</li>
<li><strong>Projection-Based</strong>: LLaVA, MedVInTTE</li>
<li><strong>Parameter-Efficient Tuning</strong>: LLaMA-Adapter, LaVIN</li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-13.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-14.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-15.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-16.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-17.png"></p>
<aside>
👉 **Expert Model**

</aside>

<p><strong>Advantage</strong>: Via expert models, convert multimodal inputs into languages without training.</p>
<p><strong>Disadvantage</strong>: less flexible, info loss</p>
<p><a href="https://arxiv.org/pdf/2305.06355.pdf">**VideoChat-Text</a>:** transforming videos into textual descriptions distorts spatial-temporal relationships.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-18.png"></p>
<h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h3><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-19.png"></p>
<aside>
👉 **Closed-set**

</aside>

<p><strong>Closed-set questions</strong>: its possible answer options are predefined and limited</p>
<p><strong>Two evaluation types</strong>:</p>
<ol>
<li><strong>leverage existing datasets</strong>: limited to a small range of tasks (e.g., acc for VQA, CIDEr score for text2caption). Two evaluation settings:<ol>
<li><strong>Zero-shot</strong>: split into held-in&#x2F;held-out parts</li>
<li><strong>Finetuning</strong>: domain-specific downstream tasks</li>
</ol>
</li>
<li><strong>develop new benchmarks specially designed for MLLMs</strong>: more comprehensive<ol>
<li><a href="https://arxiv.org/pdf/2306.13394.pdf"><strong>MME</strong></a>: 14 perception and cognition tasks, manually design instruction-answer pairs to avoid data leakage</li>
<li><strong><a href="https://arxiv.org/pdf/2306.06687.pdf">LAMM-Benchmark</a></strong>: various 2D&#x2F;3D vision tasks</li>
<li><strong><a href="https://arxiv.org/pdf/2306.05424.pdf">Video-ChatGPT</a></strong>: (1) video-based generative performance (2) zero-shot QA</li>
</ol>
</li>
</ol>
<aside>
👉 **Open-set**

</aside>

<p><strong>Open-set questions</strong>: its possible answer options are open and flexible.</p>
<p>MLLMs act as chatbots and the chat content can be arbitrary.</p>
<ol>
<li><strong>Manual scoring</strong>: require humans to assess specific dimensions</li>
<li><strong>GPT scoring</strong>:  rate with GPT, reduce human labour (LLaVA)<ul>
<li>Multimodal interface of GPT-4 is not publicly available. Therefore, setting GPT-4 as the performance upper bound is not perfect.</li>
</ul>
</li>
<li><strong>Case study</strong>: (mPLUG-Owl, Video-LLaMA)</li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-20.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-21.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-22.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-23.png"></p>
<aside>
👉 **Others**

</aside>

<p><strong><a href="https://arxiv.org/pdf/2212.10773.pdf">MultiInstruct</a></strong>: sensitivity metric, model’s robustness to varied instructions</p>
<p><strong><a href="https://arxiv.org/pdf/2305.10355.pdf">POPE</a></strong>: delve into the object hallucination problem</p>
<p><strong><a href="https://arxiv.org/pdf/2305.16934.pdf">AttackVLM</a></strong>: (safety) the robustness of MLLMs to adversarial attacks</p>
<h2 id="M-ICL-Multimodal-In-Context-Learning"><a href="#M-ICL-Multimodal-In-Context-Learning" class="headerlink" title="M-ICL: Multimodal In-Context Learning"></a>M-ICL: Multimodal In-Context Learning</h2><p><strong>ICL</strong>: few-shot, training-free</p>
<p><strong>M-ICL</strong>: Based on M-IT, at inference time, M-ICL adds a demonstration set (i.e., a set of in-context samples), to the original sample.</p>
<p><strong>Template example of an M-ICL query</strong>:</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-24.png"></p>
<aside>
👉 **Two Scenarios**

</aside>

<p><strong>Scenario 1: solving various visual reasoning tasks</strong></p>
<p>(<a href="https://arxiv.org/pdf/2303.11381.pdf">**MM-React</a>, <a href="https://arxiv.org/pdf/2305.03726.pdf">Otter</a>, <a href="https://arxiv.org/pdf/2204.14198.pdf">Flamingo</a>, <a href="https://arxiv.org/pdf/2109.05014.pdf">PICa</a>, <a href="https://arxiv.org/pdf/2106.13884.pdf">Frozen</a>**)</p>
<ul>
<li>Via demonstrations, LLMs get a sense of what the task is doing and what the output template is.</li>
<li>Thus LLMs learn from a few task-specific examples and generalize to a new but similar question.</li>
</ul>
<p><strong>Scenario 2: teaching LLMs to use external tools</strong></p>
<p>(<strong><a href="https://arxiv.org/pdf/2304.09842.pdf">Chameleon</a>, <a href="https://arxiv.org/pdf/2211.11559.pdf">Visual programming</a>, <a href="https://arxiv.org/pdf/2303.17580.pdf">HuggingGPT</a></strong>)</p>
<ul>
<li>Demonstrations are often text-only and more fine-grained.</li>
<li>They typically comprise a chain of steps that could be sequentially executed to fulfill the task. (related to CoT)</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-25.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-26.png"></p>
<h2 id="M-CoT-Multimodal-Chain-of-Thought"><a href="#M-CoT-Multimodal-Chain-of-Thought" class="headerlink" title="M-CoT: Multimodal Chain of Thought"></a>M-CoT: Multimodal Chain of Thought</h2><p><strong>CoT</strong>: a series of intermediate reasoning steps</p>
<p><strong>CoT Scenario</strong>: complex reasoning</p>
<p><strong>CoT Main Idea</strong>: output both reasoning process and final answer</p>
<p><strong>M-CoT</strong> (<strong><a href="https://arxiv.org/pdf/2201.11903.pdf">CoT-PT</a>, <a href="https://arxiv.org/pdf/2302.00923.pdf">MM-CoT</a>, <a href="https://arxiv.org/pdf/2305.02317.pdf">VCoT</a>, <a href="https://arxiv.org/pdf/2305.13903.pdf">VideoCoT</a></strong>)</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-27.png"></p>
<p><strong>Modality Bridging</strong></p>
<ul>
<li><p><strong>Expert Model</strong>: transforming visual input into textual descriptions (<a href="https://arxiv.org/pdf/2209.09513.pdf">ScienceQA</a>: concatenate image captions and original textual imput to LLMs)</p>
</li>
<li><p><strong>Learnable Interface</strong>: feature fusion (<a href="https://arxiv.org/pdf/2302.00923.pdf">MM-CoT</a>: adopts a two-stage framework)</p>
<p>  <img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-28.png"></p>
</li>
</ul>
<p><strong>Learning Paradigms</strong></p>
<ul>
<li><strong>Finetuning</strong>: often for specific datasets(e.g., <a href="https://arxiv.org/pdf/2209.09513.pdf">ScienceQA</a>)</li>
<li><strong>Few-shot</strong>: requires hand-crafting some in-context examples</li>
<li><strong>Zero-shot</strong>: require no specific example</li>
</ul>
<p><strong>Chain Config——</strong>(When to stop the chains?)</p>
<ul>
<li><strong>Adaptive</strong>: requires LLMs to decide when to halt the reasoning chains</li>
<li><strong>Pre-defined</strong>: stops the chains with a pre-defined length</li>
</ul>
<p><strong>Generation Pattens——</strong>(How the chain is constructed?)</p>
<p>The generated steps should be consistent and correct.</p>
<ul>
<li><p><strong>Predicting</strong>: extending the reasoning chains given conditions such as instructions and previous reasoning history</p>
</li>
<li><p><strong>Infilling</strong>: deducing steps between surrounding context (previous and following steps) to fill the logical gaps (VideoCoT)</p>
<p>  <img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-29.png"></p>
</li>
</ul>
<h2 id="LAVR-LLM-Aided-Visual-Reasoning"><a href="#LAVR-LLM-Aided-Visual-Reasoning" class="headerlink" title="LAVR: LLM-Aided Visual Reasoning"></a>LAVR: LLM-Aided Visual Reasoning</h2><p><strong>LAVR</strong>: use external tools or visual foundation models.</p>
<p><strong>Three advantages</strong> compared with conventional visual reasoning models: <strong>Strong generalization, Emergent abilities, Better interactivity and control</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-30.png"></p>
<aside>
👉 **Training Paradigms**

</aside>

<p><a href="https://arxiv.org/pdf/2305.18752.pdf">GPT4Tools</a>: generate a new tool-related instruction dataset</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-31.png"></p>
<aside>
👉 **Functions**

</aside>

<p><em>The primary roles that LLMs play in LAVR.</em></p>
<ul>
<li><strong>Controller</strong>: (single-round) breaks down a complex task [via CoT], assign subtasks to right tools&#x2F;modules</li>
<li><strong>Decision Maker</strong>: (multi-round) summarize history and current info, decide whether the info is sufficient to finish the task, friendly present answer</li>
<li><strong>Semantics Refiner</strong>: integrate info into fluent scentences, generate texts according to different specific needs</li>
</ul>
<h2 id="Challenges-and-Future-Directions"><a href="#Challenges-and-Future-Directions" class="headerlink" title="Challenges and Future Directions"></a>Challenges and Future Directions</h2><ol>
<li><p><strong>Perception Capabilities</strong>: compromise between info capacity and computation burden</p>
</li>
<li><p><strong>Consistent Reasoning Chain</strong>: right reasoning chain delivers right answer</p>
</li>
<li><p><strong>Instruction-following Abilities</strong>: (e.g., yes&#x2F;no type)</p>
</li>
<li><p><strong>Object Hallucination</strong>: more fine-grained modality alignment</p>
<p> <img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/mllm-32.png"></p>
<p> (The object does not appear in the image)</p>
</li>
<li><p><strong>Parameter-Efficient Training</strong></p>
</li>
</ol>
]]></content>
      <categories>
        <category>Paper</category>
        <category>2023</category>
        <category>06</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Instruction-Following</tag>
        <tag>Multi-modal</tag>
        <tag>In-Context Learning</tag>
        <tag>Chain of Thought</tag>
        <tag>Tool Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Shikra: Unleashing Multimodal LLM’s Referential Dialogue Magic</title>
    <url>/2023/07/09/Shikra/</url>
    <content><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-0.png"></p>
<p>Paper: <a href="https://arxiv.org/pdf/2306.15195.pdf">https://arxiv.org/pdf/2306.15195.pdf</a></p>
<p>Code: <a href="https://github.com/shikras/shikra">https://github.com/shikras/shikra</a></p>
<aside>
👇 Problems

</aside>

<p>In human conversations, individuals can indicate relevant regions within a scene while addressing others.</p>
<p>This natural referential ability in dialogue remains absent in current MLLMs.</p>
<aside>
👇 Contributions

</aside>

<ul>
<li><p>Study referential ability in existing MLLMs.</p>
</li>
<li><p>Define new task——Referential dialogue(RD)</p>
<p>  Potential applications of this ability</p>
<ul>
<li>aiding AI assistants in Mixed Reality headsets</li>
<li>facilitating precise communication in online shopping scenery</li>
</ul>
</li>
<li><p>Propose Shikra</p>
<ul>
<li>Shikra can comprehend and output <strong>spatial coordinates</strong> in natural language.</li>
<li>With a <strong>simple architecture</strong>, Shikra shows no need for extra vocabularies, position encoder, pre-&#x2F;post-detection modules, or external plugin models.</li>
<li>Shikra enables numerous <strong>exciting applications</strong>, like providing mentioned objects’ coordinates in chains of thoughts and comparing user-pointed regions similarities.</li>
<li>It achieves <strong>promising performance</strong> on conventional VL tasks w&#x2F;o finetuning.</li>
</ul>
</li>
</ul>
<h1 id="1-Referential-Dialogue"><a href="#1-Referential-Dialogue" class="headerlink" title="1 - Referential Dialogue"></a>1 - Referential Dialogue</h1><ul>
<li><strong>Current MLLM Can</strong>: VQA, Image Caption, and multimodal dialogue</li>
<li><strong>Current MLLM Cannot</strong>: REC, REG(referring expression generation), and PointQA</li>
</ul>
<p><strong>Shikra can do them all.</strong></p>
<p>The model demonstrates proficiency in tasks not in the training set, such as identifying similarities between two indicated objects, or counting somethings, and providing their positions.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-1.png" alt="Demo of Referential Dialogue (RD). Users can point to specific areas and ask questions. In turn, Shikra will indicate the specific regions when replying, if necessary."></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-2.png" alt="Referential Dialogues between real users and Shikra-7B."></p>
<h1 id="2-Chessboard-Test-for-Current-MLLM"><a href="#2-Chessboard-Test-for-Current-MLLM" class="headerlink" title="2 - Chessboard Test for Current MLLM"></a>2 - Chessboard Test for Current MLLM</h1><blockquote>
<p>The current MLLMs cannot directly output coordinates. We need to explore appropriate coordinate representations and finer-grained training data.</p>
</blockquote>
<p><strong>Chessboard Test</strong>:</p>
<p>Simplifies the object grounding into a part choice task:</p>
<ol>
<li><p>Divide a image into a 2 × 2 chessboard.</p>
</li>
<li><p>Next, ask:</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># &lt;image&gt;: image tokens, &lt;expr&gt; class name</span><br><span class="line">“&lt;image&gt; Which part is &lt;expr&gt; in if the picture is divided equally into four 2 by 2 parts? Choose from: (A) Top-left (B) Top-right (C) Bottom-left (D) Bottom-right.”</span><br></pre></td></tr></table></figure></li>
</ol>
<p><strong>Test Data Construction</strong>:</p>
<ul>
<li>Based on <a href="https://arxiv.org/pdf/1908.03195.pdf">LVIS</a>, which is a perception detection with over 1000 entry-level object categories.</li>
<li>We choose objects that are completely within a certain part (i.e., ambiguous positions are not considered).</li>
<li>Select 600 images per part, resulting in 2,400 images across 945 categories.</li>
</ul>
<p><strong>Tested Model</strong>: <a href="https://arxiv.org/pdf/2304.08485.pdf">LLaVA-13B</a></p>
<p><strong>Test Results</strong>: 25.96% acc, comparable to random selection.</p>
<h1 id="3-Breeding-Shikra"><a href="#3-Breeding-Shikra" class="headerlink" title="3 - Breeding Shikra"></a>3 - Breeding Shikra</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">User Question: </span><br><span class="line">“How many other clothes in the &lt;image&gt; are of the same color as the jacket [0.268, 0.372]?”. </span><br><span class="line">Shikra reply: </span><br><span class="line">“The jacket [0.268, 0.372] is green. We can find a T-shirt [0.653, 0.532] and cropped pants [0.569, 0.101] a with same green color. So the answer is two.”</span><br></pre></td></tr></table></figure>

<h2 id="3-1-Architecture"><a href="#3-1-Architecture" class="headerlink" title="3.1 - Architecture"></a>3.1 - Architecture</h2><p><strong>Frozen Visual Encoder</strong>: pre-trained ViT-L&#x2F;14 CLIP, its output embedding denoted as $\mathbf{V}\in\mathbb{R}^{16\times16\times1024}$</p>
<p><strong>Trainable LLM</strong>: Vicuna-7&#x2F;13B</p>
<p><strong>One Trainable fully connected layer</strong>: map $\mathbf{V}$ to $\mathbf{V’}\in\mathbb{R}^{256\times D}$, $D$ is 4096 for Vicuna-7B and 5120 for 13B</p>
<p>Visual embedding can be inserted into anywhere of input sequence.</p>
<p>We do not introduce any vocabulary or special encoder for encoding position information.</p>
<p>We have not introduced additional pre-&#x2F;post-detectors for points or bounding boxes.</p>
<h2 id="3-2-Numerical-representation-of-position"><a href="#3-2-Numerical-representation-of-position" class="headerlink" title="3.2 - Numerical representation of position"></a>3.2 - Numerical representation of position</h2><p>x, y is normalized(3 decimal places) according to image size.</p>
<p><strong>Bounding box</strong>: $[x_{min}, y_{min},x_{max}, y_{max}]$</p>
<p><strong>Region center point</strong>: $[x_{center}, y_{center}]$</p>
<p>The coordinates can appear anywhere in the input and output sequence of the model.</p>
<p>Like regular text, tokenizing without discrimination.</p>
<h2 id="3-3-Instruction-data-construction"><a href="#3-3-Instruction-data-construction" class="headerlink" title="3.3 - Instruction data construction"></a>3.3 - Instruction data construction</h2><h3 id="3-3-1-Reorganization-of-public-data"><a href="#3-3-1-Reorganization-of-public-data" class="headerlink" title="3.3.1 - Reorganization of public data"></a>3.3.1 - Reorganization of public data</h3><p><strong>Spotting Captioning Definition</strong>: describe the image and spots the mentioned objects or regions using points or boxes. We use Flickr30K Entities for this task.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-3.png" alt="All training data used by Shikra. The asterisk indicates that this data is only used in the second stage."></p>
<p>We have <strong>excluded images present in the test and validation data</strong> from the training data to prevent potential data leakage, despite their distinction in terms of image-text pairs.</p>
<h3 id="3-3-2-Generated-Data"><a href="#3-3-2-Generated-Data" class="headerlink" title="3.3.2 - Generated Data"></a>3.3.2 - Generated Data</h3><blockquote>
<p>Data in 3.3.1 lack CoT data with <strong>positional annotations</strong>, natural communication data with <strong>positional annotations</strong>, etc.</p>
</blockquote>
<p><strong>Flickr30K Entities:</strong> has five descriptions for each image. These mentioned objects appearing in the image will be labeled using bounding box. </p>
<p><strong>Shikra-RD</strong>: High-quality RD data built from Flickr30K Entities using GPT-4.</p>
<p><strong>Shikra-RD</strong> contains 5,922 QA pairs with <strong>positional annotations</strong> and will continues expanding in the future.</p>
<p><strong>GPT-4 Usage</strong>: We explained the format of the bounding boxes to GPT-4 and asked it to understand the image through these five sentences and boxes. Next, we require GPT-4 to design Q&amp;A pairs. </p>
<h3 id="3-3-3-Task-prompts"><a href="#3-3-3-Task-prompts" class="headerlink" title="3.3.3 - Task prompts"></a>3.3.3 - Task prompts</h3><ol>
<li>Write <strong>a sample template</strong> of a specific task</li>
<li>Have GPT-4 rewrite it in rich language, expanding it into <strong>hundreds of variations</strong>.</li>
</ol>
<p><strong>During training</strong>, we can randomly choose from them.</p>
<p><strong>During inference</strong>, users can describe their needs in their own format.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-4.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-5.png" alt="Examples of task templates used by Shikra on different types of training data. &lt;image&gt; represents visual tokens, &lt;objs&gt; denotes coordinates of region center points, &lt;expr&gt; is the expression in REC."></p>
<h2 id="3-4-Tuning-details"><a href="#3-4-Tuning-details" class="headerlink" title="3.4 - Tuning details"></a>3.4 - Tuning details</h2><ul>
<li>First stage: train Shikra on the reorganized VL dataset for 100,000 steps (around 1.5 epoch)</li>
<li>Second stage: raise the sampling ratio to 50% on LLaVA-Instruct-150K and Shikra-RD</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">optimizer: AdamW</span><br><span class="line">learning rate scheduler: cosine annealing scheduler</span><br><span class="line">initial learning rate: 2e-5</span><br><span class="line">global batch size: 64</span><br><span class="line">device: 8 NVIDIA A100 GPUs</span><br><span class="line">time cost: ~100h for first stage, ~20h for second stage</span><br></pre></td></tr></table></figure>

<h1 id="4-Experiment-and-Analysis"><a href="#4-Experiment-and-Analysis" class="headerlink" title="4 - Experiment and Analysis"></a>4 - Experiment and Analysis</h1><h2 id="4-1-Grounding-CoT-or-verbal-CoT"><a href="#4-1-Grounding-CoT-or-verbal-CoT" class="headerlink" title="4.1 - Grounding CoT or verbal CoT?"></a>4.1 - Grounding CoT or verbal CoT?</h2><aside>
💡 Investigate whether **Grounding CoT**(GCoT, CoT with position annotations) can reduce visual hallucinations and improve model performance.

</aside>

<p>We train three toy models of Shikra-7B (without using additional datasets) on the CLEVR dataset. </p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-6.png" alt="Comparing different forms of CoTs. $Q, A, C$, and $C^{Point}$ denote the Question, final Answer, Chain of thoughts, and Chain of thoughts with center points $[x_{center},y_{center}]$."></p>
<h2 id="4-2-Location-tokens-or-just-numbers"><a href="#4-2-Location-tokens-or-just-numbers" class="headerlink" title="4.2 - Location tokens or just numbers?"></a>4.2 - Location tokens or just numbers?</h2><p>We train two toy Shikra using two different representations with REC data.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-7.png" alt="Comparing different position representations. Vocab., e.g., &lt;bin_0&gt;, · · · , &lt;bin_1000&gt;. Numerical directly uses numbers."></p>
<p><strong>Numerical representation Advantages</strong>:</p>
<ul>
<li>better performance</li>
<li>does not retrain vocabularies for localization tasks</li>
<li>freely control the precision</li>
</ul>
<p><strong>Numerical representation Disadvantages</strong>:</p>
<ul>
<li>longer token length brings more computational costs</li>
</ul>
<h2 id="4-3-Quantitative-results-on-conventional-tasks"><a href="#4-3-Quantitative-results-on-conventional-tasks" class="headerlink" title="4.3 - Quantitative results on conventional tasks"></a>4.3 - Quantitative results on conventional tasks</h2><aside>
👇 **REC task**: ground the object described with an expression.

</aside>

<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-8.png" alt="Results on standard REC task. OFA-L* refers to the OFA-Large checkpoint without finetuning. GRIT refexp is the ablation split."></p>
<aside>
👇 **PointQA task**

</aside>

<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-9.png" alt="Comparing pointQA capabilities on the Visual-7W. Accuracy (%) is used for evaluation."></p>
<p><strong>Visual-7W</strong>: given a question and four box options, model should choose one as the answer.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-10.png" alt="Comparing pointQA capabilities on the LookTwice-QA. We use Shikra-13B and Accuracy (%) for evaluation."></p>
<p><strong>LookTwice-QA</strong>: answer question based on the input point&#x2F;box.</p>
<p>[<strong>Pronoun&#x2F;Superclass&#x2F;Class]</strong> indicate different levels of referential clarity in the question, e.g., “How many of these <strong>[∅&#x2F;fruits&#x2F;apples]</strong> <obj>?”</p>
<p><strong><obj></strong> denotes the coordinates of input point&#x2F;box</p>
<aside>
👇 **conventional VL tasks**

</aside>

<blockquote>
<p>It’s worth noting that these task configurations are just some subsets of Referential Dialogue.</p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-11.png" alt="Comparing generalist models on VQA and Image Captioning. Use Shikra-13B. FM: Flamingo. Acc for VQA, CIDEr for Caption."></p>
<p>We also provide $VQAv2^{val}$ (83.3) and OK-VQA (53.8) results on LVLM-eHub toolbox for easy comparison.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-12.png" alt="Object hallucination benchmark using POPE evaluation pipeline."></p>
<p><strong>Accuracy</strong> denotes the accuracy of predictions.</p>
<p><strong>Precision</strong> signifies the true positive samples among the predicted positives.</p>
<p><strong>Recall</strong> indicates the correct identification of all true positive samples.</p>
<p><strong>“Yes”</strong> represents the probability of the model outputting a positive answer.</p>
<h1 id="5-Related-Work"><a href="#5-Related-Work" class="headerlink" title="5 - Related Work"></a>5 - Related Work</h1><h2 id="5-1-Vision-Language-Positioning-Tasks"><a href="#5-1-Vision-Language-Positioning-Tasks" class="headerlink" title="5.1 - Vision-Language Positioning Tasks"></a>5.1 - Vision-Language Positioning Tasks</h2><p>Many vision-language tasks require localization representation.</p>
<h3 id="5-1-1-Tasks-with-output-boxes"><a href="#5-1-1-Tasks-with-output-boxes" class="headerlink" title="5.1.1 - Tasks with output boxes"></a>5.1.1 - Tasks with output boxes</h3><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-13.png" alt="REC."></p>
<p><strong>Referring Expression Comprehension (REC)</strong> aims to localize <strong>a</strong> target object in an image described by a referring expression.</p>
<p><strong>Described Object Detection</strong> extends REC to more realistic scenarios where the object may <strong>not exist</strong> or there may be <strong>multiple</strong> objects.</p>
<p><strong>VQA Grounding</strong> aims to answer visual questions and associate the answers with specific visual regions or objects.</p>
<h3 id="5-1-2-Tasks-with-input-boxes"><a href="#5-1-2-Tasks-with-input-boxes" class="headerlink" title="5.1.2 - Tasks with input boxes"></a>5.1.2 - Tasks with input boxes</h3><p><strong>Grounding Caption (GC)</strong>: Given an image and a location box, <strong>GC</strong> aims to generate a description for this location by considering the surrounding environment.</p>
<p><strong>Referring Expression Generation (REG)</strong>: Compared to <strong>GC</strong>, <strong>REG</strong> requires the generated description to indicate that it describes this region specifically, not others, making it necessary for the description to be discriminative.</p>
<p><strong>PointQA</strong> requires a model answer for a visual question where the questioner queries a specific position in the picture.</p>
<h3 id="5-1-3-Shikra-both-input-and-output"><a href="#5-1-3-Shikra-both-input-and-output" class="headerlink" title="5.1.3 - Shikra: both input and output"></a>5.1.3 - Shikra: both input and output</h3><p>Our model is not only compatible with the above tasks, but also can handles the input and output of position representation flexibly and simultaneously.</p>
<h2 id="5-2-Position-Representation"><a href="#5-2-Position-Representation" class="headerlink" title="5.2 - Position Representation"></a>5.2 - Position Representation</h2><p>RoI: regions of interest</p>
<h3 id="5-2-1-Inputting-RoI"><a href="#5-2-1-Inputting-RoI" class="headerlink" title="5.2.1 - Inputting RoI"></a>5.2.1 - Inputting RoI</h3><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-14.png" alt="**Method 1:** directly concatenate cropped image patches with the original image as model input."></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-15.png" alt="**Method 2**: use 0/1 mask or Gaussian map input with the original image to emphasize the area of user interest."></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/Shikra-16.png" alt="**Method 3**: first encode points and boxes to positional encodings then add them to intermediate features or learned queries."></p>
<h3 id="5-2-2-Outputting-RoI"><a href="#5-2-2-Outputting-RoI" class="headerlink" title="5.2.2 - Outputting RoI"></a>5.2.2 - Outputting RoI</h3><p><strong>Anchor-based methods</strong> utilize predefined sliding windows and proposal candidate regions for classification., e.g., Fast R-CNN.</p>
<p>Some methods directly regress four values for bounding box coordinates, e.g., FCOS.</p>
<p>Some methods adopt one-to-one label assignment to evolve object detection into an end-to-end manner, e.g., DETR and POTP.</p>
<p><strong>Pix2seq</strong> formalizes the detection task as a sequence generation task:</p>
<ul>
<li>It desires the spatial position of the image in 1,000 bins and uses a 1,000-token vocabulary to represent it.</li>
<li>For detection, Pix2seq performs classification on the coordinate vocabulary in an auto-regressive manner.</li>
</ul>
<p>Following Pix2seq, several methods, e.g., OFA, Unified-IO, UniTab, GIT, and VisionLLM introduce similar coordinate vocabulary alongside the language vocabulary for object detection and REC tasks.</p>
<h3 id="5-2-3-Shikra"><a href="#5-2-3-Shikra" class="headerlink" title="5.2.3 - Shikra"></a>5.2.3 - Shikra</h3><p>Shikra formulates position input&#x2F;output as the most natural and flexible form of language and compare it with the extra coordinate vocabulary in Section 4.2.</p>
<h1 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6 - Conclusion"></a>6 - Conclusion</h1><p><strong>Limitations</strong></p>
<ul>
<li>Shikra is unsuitable for dense object detection and segmentation tasks.</li>
<li>Shikra may produce harmful and counterfactual responses.</li>
</ul>
<p><strong>Future Work</strong></p>
<ul>
<li>Exploring improved coordinate representations for these tasks.</li>
<li>Improve GCoT</li>
<li>Multilingual</li>
</ul>
]]></content>
      <categories>
        <category>Paper</category>
        <category>2023</category>
        <category>06</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Instruction-Following</tag>
        <tag>Multi-modal</tag>
        <tag>Fine-grained MLLM</tag>
        <tag>Position Representation</tag>
      </tags>
  </entry>
  <entry>
    <title>(LENS)Towards Language Models That Can See: Computer Vision Through the LENS🔍 of Natural Language</title>
    <url>/2023/07/03/LENS/</url>
    <content><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-0.png"></p>
<p>Paper: <a href="https://arxiv.org/pdf/2306.16410.pdf">https://arxiv.org/pdf/2306.16410.pdf</a></p>
<p>Code: <a href="https://github.com/ContextualAI/lens">https://github.com/ContextualAI/lens</a></p>
<p>BlogPost: <a href="https://contextual.ai/introducing-lens/">https://contextual.ai/introducing-lens/</a></p>
<p>Demo: <a href="https://lens.contextual.ai/">https://lens.contextual.ai/</a></p>
<p><strong>Problems</strong>: Existing alignment methods for visual and language modalities need training, which requires heavy computation burden and large corpora.</p>
<p><strong>Contributions</strong>: Training-free LENS enables LLM to have visual capabilities.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-1.png" alt="Comparison of approaches for aligning visual and language modalities. Old-Style trains visual encoder. Flamingo inserts new layer into LLM. BLIP-2 introduces new lightweight module between visual encoder and LLM."></p>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ul>
<li><a href="https://arxiv.org/pdf/2212.10846.pdf">Img2Prompt</a> uses LLMs for solving VQA tasks.(In contrast, LENS extends the capacity of LLM to also solve object recognition tasks.)</li>
<li><a href="https://arxiv.org/pdf/2303.08128.pdf">ViperGPT</a> also leverages LLMs to solve VQA tasks, but heavily relies on BLIP2 which needs extra training rounds of multimodal pre-training.</li>
</ul>
<p><strong>[Img2Prompt, ViperGPT] “top-down”</strong> approach(question-aware): attention mechanisms are driven by nonvisual or task-specific contexts.</p>
<p><strong>[LENS] <a href="https://arxiv.org/pdf/1707.07998.pdf">“bottom-up”</a></strong> approach: does not involve any question-guided information extraction</p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-2.png" alt="The training-free LENS framework. Via a set of “vision modules”, a frozen LLM can perform object recognition or visual reasoning tasks."></p>
<h3 id="Two-Visual-Vocabularies"><a href="#Two-Visual-Vocabularies" class="headerlink" title="Two Visual Vocabularies"></a><strong>Two</strong> <strong>Visual Vocabularies</strong></h3><blockquote>
<p>They act as a bridge to convert an image into textual info.</p>
</blockquote>
<p><strong>Tag(Object) Vocabulary</strong>: We collect tags from various sources, including image classification(<a href="https://arxiv.org/pdf/1409.0575.pdf">ImageNet</a>, <a href="https://data.caltech.edu/records/mzrjq-6wc02">Caltech 101</a>, <a href="https://arxiv.org/pdf/1311.3618.pdf">DTD</a>, <a href="https://ieeexplore.ieee.org/document/6248092">Cats and Dogs</a>, <a href="https://www.robots.ox.ac.uk/~vgg/publications/2008/Nilsback08/nilsback08.pdf">Flower</a>, <a href="https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/">Food 101</a>, <a href="https://vision.princeton.edu/projects/2010/SUN/paper.pdf">SUN</a>, <a href="http://vision.stanford.edu/pdf/3drr13.pdf">3D</a>), object detection and semantic segmentation(<a href="https://arxiv.org/pdf/1908.03195.pdf">LAVIS</a>, <a href="https://arxiv.org/pdf/1405.0312.pdf">MS COCO</a>, <a href="https://arxiv.org/pdf/1811.00982.pdf">Open Images V4</a>), <a href="https://arxiv.org/pdf/1602.07332.pdf">Visual Genome</a>.</p>
<p><strong>Attribute Vocabulary</strong>: Following <a href="https://arxiv.org/pdf/2210.07183.pdf">Menon &amp; Vondrick</a>, we employ GPT-3 to generate descriptions of the visual characteristics that differentiate each object category.</p>
<h3 id="Four-LENS🔍-Components"><a href="#Four-LENS🔍-Components" class="headerlink" title="Four LENS🔍 Components"></a>Four LENS<strong>🔍</strong> Components</h3><p><strong>Tag Module</strong> identifies and assigns suitable tags to the image via CLIP. We adopt a common prompt: “A photo of {tagname}”.</p>
<p><strong>Attributes Module</strong> identifies and assigns relevant attributes to the objects present in the image via CLIP. We incorporates the task-specific prompts outlined in <a href="https://arxiv.org/pdf/2210.07183.pdf">Menon &amp; Vondrick</a>.</p>
<p><strong>Intensive Captioner</strong> utilizes BLIP and applies <a href="https://arxiv.org/pdf/1805.04833.pdf">stochastic top-k sampling</a> to generate N captions per image.</p>
<p><strong>Reasoning Module</strong> generates answers based on the textual descriptions from vision modules, along with the task-specific instructions.</p>
<h3 id="Prompt-Design"><a href="#Prompt-Design" class="headerlink" title="Prompt Design"></a>Prompt Design</h3><blockquote>
<p>Concatenate generic prompts with task-specific prompts.</p>
<p>Can see in demo.</p>
</blockquote>
<p>OCR is optional(for hateful-memes task).</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># prompt template</span><br><span class="line">Tags: &#123;Top-k tags&#125;</span><br><span class="line">Attributes: &#123;Top-K attributes&#125;</span><br><span class="line">Captions: &#123;Top-N Captions&#125;</span><br><span class="line">OCR: this is an image with written &quot;&#123;meme text&#125;&quot; on it</span><br><span class="line">Question: &#123;task-specific prompt&#125; \n Short Answer:</span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-3.png" alt="Object recognition prompt used in LENS."></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-4.png" alt="VQA prompt used in LENS."></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-5.png" alt="Hateful-memes prompt used in LENS."></p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><blockquote>
<p>The experiments is only about Flan-T5 and do not include other LLMs.</p>
</blockquote>
<h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><p><strong>Object Recognition</strong>: 8 benchmarks in <a href="https://arxiv.org/pdf/2103.00020.pdf">CLIP</a>.</p>
<p><strong>VL Reasoning</strong>: the test-dev split of <a href="https://arxiv.org/pdf/1612.00837.pdf">VQA 2.0</a>, the test set of <a href="https://arxiv.org/pdf/1906.00067.pdf">OK-VQA</a>.</p>
<p><strong>Others</strong>: the dev and test-seen sets of the <a href="https://arxiv.org/pdf/2005.04790.pdf">Hateful Memes</a>, the test set of <a href="https://arxiv.org/pdf/2103.00020.pdf">Rendered SST2</a>.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-6.png" alt="Datasets examined for evaluation of LENS. The closed-ended manner uses a fixed class(/tag/object) vocabulary. ."></p>
<h3 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h3><ul>
<li><strong>Tags and attributes modules</strong>: <a href="https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K">OpenCLIP-H&#x2F;142</a>, <a href="https://huggingface.co/openai/clip-vit-large-patch14">CLIP-L&#x2F;143</a></li>
<li><strong>Captioner</strong>: <a href="https://huggingface.co/Salesforce/blip-image-captioning-large">BLIP-large4</a> finetuned on COCO to generate 50 captions per image.</li>
<li><strong>Frozen LLMs</strong>: <a href="https://arxiv.org/pdf/2301.13688.pdf">Flan-T5</a> models.</li>
</ul>
<p>We employ beam search with number of beams equal to 5.</p>
<blockquote>
<p>Beam search is an improved algorithm for greedy search.</p>
</blockquote>
<p>Additionally, we apply a length penalty equal to -1, encouraging the generation of concise answers as in <a href="https://arxiv.org/pdf/2301.12597.pdf">BLIP-2</a>.</p>
<p>These experiments were conducted on 8 NVIDIA A100 (40GB) GPUs.</p>
<aside>
👇 Task-specific optimizations

</aside>

<ul>
<li><strong>Object recognition</strong>: we utilize the tag module and the attribute module, but skip the intensive captioning modules.</li>
<li><strong>VQA</strong>: we solely use the intensive captioning module.</li>
<li><strong>Hateful Memes and Rendered-SST2</strong>: we incorporate the tag, attributes, and captioning modules.</li>
</ul>
<p>We generate only one caption using beam search with a width of 5.</p>
<h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><aside>
👇 Object recognition(0/1/3-shot)

</aside>

<p>We compare LENS with SOTA models following <a href="https://arxiv.org/pdf/2210.07183.pdf">Menon &amp; Vondrick</a>.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-7.png" alt="**Zero-shot results** for LENS in object recognition tasks."></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-8.png" alt="Average few-shot performance of LENS on vision tasks except ImageNet (due to its large size)."></p>
<ul>
<li>More shots help to increase performance.</li>
<li>The both results show direct relationship between ViT size and performance(but not for LLM size).</li>
</ul>
<aside>
👇 Vision and Language reasoning(0-shot)

</aside>

<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-9.png" alt="The highly competitive nature of LENS."></p>
<h3 id="Ablations-on-LENS-components"><a href="#Ablations-on-LENS-components" class="headerlink" title="Ablations on LENS components"></a>Ablations on LENS components</h3><aside>
👇 Object recognition

</aside>

<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-10.png" alt="Ablations results using OpenCLIP-H/14 as vision encoder and $Flan-T5_{XL}$ as the LLM."></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-11.png" alt="The detailed results."></p>
<p>The object information helps more than the attributes, but together they are complimentary and lead to overall better performance.</p>
<aside>
👇 Vision and Language reasoning

</aside>

<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-12.png" alt="Ablation results of LENS with Flan-T5XXL on VQA 2.0 minival split."></p>
<p>Increasing the caption num improves the performance gradually but starts saturating eventually.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-13.png" alt="On the dev set of Hateful Memes. Adding more visual information improves the performance consistently."></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-14.png" alt="**Selected examples of LENS with all prompts**, illustrating its reasoning capabilities by answering questions about complex scenes and scenarios."></p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/LENS-15.png" alt="**Failure examples of LENS.**"></p>
<p>(a) <strong>Incorrect Visual Info</strong> of objects or attributes from CLIP and BLIP</p>
<p>(b) <strong>Inconsistency</strong> between the responses</p>
<p>(c) <strong>Presuppositions</strong> and in-built biases</p>
<p>(d) <strong>Forgetting</strong> and limitations of context windows of the LLMs.</p>
<p><em><strong>Limitations of LENS</strong></em></p>
<ul>
<li>the vision capability of LENS heavily relies on CLIP and BLIP</li>
<li>LENS evaluation still requires substantial computational resources.</li>
</ul>
<p><em><strong>Future work</strong></em></p>
<ul>
<li>more effective ways integrating visual modules</li>
<li>more efficient methods to reduce the computational burden</li>
<li>expand to more modalities(e.g., audio, video)</li>
</ul>
]]></content>
      <categories>
        <category>Paper</category>
        <category>2023</category>
        <category>06</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Multi-modal</tag>
        <tag>Expert Integration</tag>
      </tags>
  </entry>
  <entry>
    <title>How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources</title>
    <url>/2023/06/15/T%C3%BClu/</url>
    <content><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/open-instruct-1.jpg" alt="authors">Paper: <a href="https://arxiv.org/pdf/2306.04751.pdf">https://arxiv.org/pdf/2306.04751.pdf</a></p>
<p>Code: <a href="https://github.com/allenai/open-instruct">https://github.com/allenai/open-instruct</a></p>
<blockquote>
<p>Evaluation for instruction-tuned models remains inconsistent and difficult. Therefore, this work covers extensive evaluations on a large range of models and datasets.</p>
</blockquote>
<p><em><strong>When reading this note, you can think about the following questions:</strong></em></p>
<ol>
<li>What instruction datasets, pretrained models and <strong>evaluation metrics</strong> are used?</li>
<li>What are the evaluation results?</li>
</ol>
<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><h3 id="Instruction-Tuning"><a href="#Instruction-Tuning" class="headerlink" title="Instruction Tuning"></a>Instruction Tuning</h3><p><strong>Definition</strong>: finetuning pretrained LMs to better understand and respond to various human requests that are expressed in natural language.</p>
<p><strong>Advantages</strong>: (1) zero-shot generalization to new tasks; (2) non-experts can use natural language to interact with LLMs.</p>
<blockquote>
<p>The most popular programming language in the future will be English.</p>
</blockquote>
<p><strong>Training Paradigms</strong>: (1) supervised learning(demonstrations); (2) reinforcement learning (feedback data)</p>
<p><strong>Key Components</strong>: (1) pretrained LMs; (2) instruction datasets(diversity, task num)</p>
<h3 id="Evaluation-Method"><a href="#Evaluation-Method" class="headerlink" title="Evaluation Method"></a>Evaluation Method</h3><p><strong>Benchmark-based evaluation</strong></p>
<ul>
<li><a href="https://arxiv.org/abs/2211.09110">HELM</a>, <a href="https://doi.org/10.5281/zenodo.5371628">LM Evaluation Harness</a>: suitable for various NLP models</li>
<li><a href="https://arxiv.org/pdf/2210.11416.pdf">Flan-T5 work</a>: focus on factuality and reasoning abilities</li>
<li><a href="https://arxiv.org/abs/2303.08774">GPT-4</a>, <a href="https://arxiv.org/abs/2305.10403">PaLM v2</a>: proprietary models with comprehensive evaluations</li>
</ul>
<p><strong>Open-ended instruction-following ability evaluation</strong></p>
<ul>
<li><a href="https://arxiv.org/abs/2305.14387">Alpaca Farm</a>: leverage other models as annotators for judging model generations</li>
<li><a href="https://lmsys.org/blog/2023-05-03-arena/">Chatbot Arena</a>: leverage humans</li>
</ul>
<p>This work involves traditional benchmarks, model-based evaluation, and human-based evaluation.</p>
<h2 id="Training-Models-with-Various-Datasets"><a href="#Training-Models-with-Various-Datasets" class="headerlink" title="Training Models with Various Datasets"></a>Training Models with Various Datasets</h2><h3 id="Datasets-and-Format-Unity"><a href="#Datasets-and-Format-Unity" class="headerlink" title="Datasets and Format Unity"></a>Datasets and Format Unity</h3><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/open-instruct-2.png"> <strong>Datasets</strong>: Only CoT and Code-Alpaca are built for specific skills. <a href="https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/tree/main/HTML_cleaned_raw_dataset">ShareGPT</a> is a collection of user interactions with various chat systems publicly shared.</p>
<p><strong>Human data mixture</strong>: FLAN V2, CoT, Dolly, Open Assistant 1</p>
<p><strong>Human+GPT data mixture</strong>: Human data mixture + GPT4-Alpaca, Code-Alpaca, ShareGPT</p>
<p><strong>Format Unity</strong>: It aims at representing arbitrary rounds as one sentence.</p>
<ul>
<li>$N$: instance num in a dataset</li>
<li>$i$: round num in each example</li>
<li>${(x_1^j, y_1^j,x_2^j, y_2^j,…,x_i^j, y_i^j)}_{j&#x3D;1}^N$: an instruction dataset</li>
</ul>
<h3 id="Models-Training"><a href="#Models-Training" class="headerlink" title="Models Training"></a>Models Training</h3><ul>
<li>$X:{(x_1^j, x_2^j,…,x_i^j)}_{j&#x3D;1}^N$</li>
<li>$Y:{(y_1^j, y_2^j,…,y_i^j)}_{j&#x3D;1}^N$</li>
<li>$t_n$: the $n$-th input token(belonging to X or Y)</li>
<li>loss function $L&#x3D;-\sum\limits_n \log p_{\theta}(t_n|t_{&lt;n})\times\left{\begin{array}{}1 &amp; if\space t_n\in Y \ 0 &amp; otherwise\end{array}\right.$</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># hyperparams</span><br><span class="line">max_seq_len: 1024 for 30B and 65B, 2048 for others</span><br><span class="line">epoch: 2</span><br><span class="line">learning rate: 1e-5 for 30B and 65B, 2e-5 for others. (linear decay and linear warmup only used for 3% of total steps)</span><br></pre></td></tr></table></figure>

<p><strong>Tülu</strong>: a suite of 7B to 65B LLaMA models fully-instruction-tuned on Human+GPT data mixture.</p>
<h2 id="Evaluation-Setup"><a href="#Evaluation-Setup" class="headerlink" title="Evaluation Setup"></a>Evaluation Setup</h2><p>Load models using <strong><a href="https://arxiv.org/pdf/2208.07339.pdf">8-bit mode</a></strong> provided in the Huggingface Transformers library.</p>
<p>When doing generation, we use greedy decoding and a max length of 512 tokens, unless otherwise specified.</p>
<h3 id="Facets-of-Evaluation"><a href="#Facets-of-Evaluation" class="headerlink" title="Facets of Evaluation"></a>Facets of Evaluation</h3><p><strong>(1) Specific model capabilities</strong>: </p>
<ul>
<li><strong>Factual knowledge</strong>: <a href="https://arxiv.org/abs/2009.03300">MMLU</a>. [<a href="https://github.com/hendrycks/test">Its official evaluation scripts and prompts</a>]. Modify it to allow for batch processing. We evaluate using 0 and 5 few-shot examples, following the original setup of MMLU.</li>
<li><strong>Reasoning</strong>. We evaluate with and without chain-of-thought (CoT vs Direct). Subsample GSM and BBH to a reasonable size to improve the efficiency of doing CoT reasoning.<ul>
<li><a href="https://arxiv.org/abs/2110.14168">GSM</a>(test split) for mathematical reasoning capabilities(8-shot). Because all answers in GSM are numbers, we extract the last number in the model response as the final answer. Sampled 200 from the 1319 examples.</li>
<li><a href="https://arxiv.org/abs/2210.09261">BBH</a> for general reasoning capabilities(3-shot). For the CoT setup, we extract the first word after the phrase ‘So the answer is’, or the entire response if there is no such substring present.</li>
</ul>
</li>
<li><strong>Multilinguality</strong>: <a href="https://arxiv.org/abs/2003.05002">TyDiQA</a> for multilingual QA. Follow <a href="https://arxiv.org/pdf/2305.10403.pdf">PaLM 2</a>‘s setup. We use the gold-passage setup where one passage containing the reference answer is given.</li>
<li><strong>Coding</strong>: <a href="https://arxiv.org/abs/2107.03374">Codex-Eval(HumanEval)</a> for abilities of generating functionally correct programs from docstrings. Following the original paper, we compute unbiased estimates of pass@k to measure the functional correctness of models’ outputs. We report both pass@1 and pass@10. The pass@1 results were obtained by sampling with a temperature of 0.1 and the pass@10 results with a temperature of 0.8.</li>
</ul>
<p><strong>(2) Open-ended instruction following</strong>: model-based evaluation and human evaluation</p>
<h3 id="Model-Based-Evaluation-using-GPT-4"><a href="#Model-Based-Evaluation-using-GPT-4" class="headerlink" title="Model-Based Evaluation using GPT-4"></a>Model-Based Evaluation using GPT-4</h3><p><strong>Dataset</strong>: Use a test set of 805 instructions.</p>
<p><strong>Code</strong>: Adopt <a href="https://arxiv.org/abs/2305.14387">AlpacaFarm</a>‘s code, but slightly alter prompts to fit our message format.</p>
<p><strong>A GPT-4 annotator(‘greedy_gpt4’)</strong>: Compare the testing model with Davinci-003.</p>
<p><strong>Win-rate</strong>: The percentage of model generations that GPT-4 reports as being preferred over the generations from Davinci-003.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Hyperparameters</span><br><span class="line">temperature: 0</span><br><span class="line">batch: 5 (reduce it if the 5 examples exceed the 8192 token context window limit)</span><br><span class="line">max_output_token_len: extended from 300 to 2048 (in order to avoid cut-off generations)</span><br></pre></td></tr></table></figure>

<h3 id="Human-Evaluation"><a href="#Human-Evaluation" class="headerlink" title="Human Evaluation"></a>Human Evaluation</h3><p>The model information is anonymized and their outputs are put in random order.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/open-instruct-8.jpg"></p>
<p><strong>Use 332 instructions</strong>: 252 from <a href="https://arxiv.org/abs/2212.10560">Self-Instruct</a> and 80 from <a href="https://lmsys.org/blog/2023-03-30-vicuna/">Vicuna</a> evaluation set.</p>
<p><strong>Compare 3 pairs of models</strong>: (1) TÜLU 65B vs ChatGPT (2) TÜLU 65B vs TÜLU 7B (3) TÜLU 65B vs a 65B LLaMA model trained on the Human data mixture.</p>
<p><strong>Interface for human judgements</strong>:</p>
<ul>
<li><strong>Indivisual acceptability</strong>: “yes”&#x2F;“no”, a 2-way decision. For “yes”, if and only if the response answered the request in the query, had no significant errors, and did not have repetitive information.</li>
<li><strong>Pairwise preference</strong>: “A is clearly better”&#x2F;“A is slightly better”&#x2F;“Tie”&#x2F;“B is slightly better”&#x2F;“B is clearly better”, a 5-way decision, select which one is more helpful.</li>
</ul>
<p><strong>Recruited 18 expert annotators</strong>, which are researchers at AI2 or students at UW for the annotation. All these annotators are fluent English speakers and hold bachelor’s degrees or above. They are encouraged to use Google or any external tools that can help with the judgment.</p>
<p><strong>Inter-Annotator Agreement</strong>:</p>
<ul>
<li>We measure the agreement of our annotators on a subset of <strong>119 examples</strong> (63 instances randomly sampled from the ChatGPT3 vs TÜLU 65B comparison, and 59 instances randomly sampled from the TÜLU 65B vs TÜLU 7B comparison).</li>
<li><strong>Indivisual acceptability</strong>: an agreement of 0.84.</li>
<li><strong>Pairwise preference</strong>: an agreement of 0.72. Following <a href="https://arxiv.org/pdf/2305.11206.pdf">Lima</a>, we report a tie-discounted accuracy, which assigns one point if both annotators agreed, half a point if either annotator (but not both) labeled a tie, and zero point otherwise. We also merged “clearly better” and “slightly better” together, so our final options will be simply comparing which of A and B is better, or a tie.</li>
</ul>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><blockquote>
<p>The best model in any given evaluation reaches on average 83% of ChatGPT performance, and 68% of GPT-4 performance.</p>
</blockquote>
<h3 id="Analysis-of-Instruction-Tuning-Datasets-and-Base-Models"><a href="#Analysis-of-Instruction-Tuning-Datasets-and-Base-Models" class="headerlink" title="Analysis of Instruction Tuning Datasets and Base Models"></a>Analysis of Instruction Tuning Datasets and Base Models</h3><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/open-instruct-3.jpg"> <img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/open-instruct-4.jpg"> “1.4T” means $1.4\times 10^{12}$ tokens are used to train the model. “180B” means $180\times 10^{9}$</p>
<ul>
<li>There is not a single best instruction tuning dataset across all tasks</li>
<li>Combining datasets results in the best overall performance on the benchmark tasks</li>
<li>Base model quality is extremely important for downstream performance</li>
</ul>
<h3 id="Pushing-the-Limits-of-Open-Models"><a href="#Pushing-the-Limits-of-Open-Models" class="headerlink" title="Pushing the Limits of Open Models"></a>Pushing the Limits of Open Models</h3><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/open-instruct-5.jpg"></p>
<ul>
<li>Instrcution tuning brings large benefits on top of LLaMA models at all sizes</li>
<li>Smaller models benefit most from instruction tuning</li>
<li>TÜLU still lags behind SOTA proprietary models</li>
</ul>
<h3 id="Model-Based-x2F-Human-Evaluation-Results-for-Open-ended-Generation"><a href="#Model-Based-x2F-Human-Evaluation-Results-for-Open-ended-Generation" class="headerlink" title="Model-Based&#x2F;Human Evaluation Results for Open-ended Generation"></a>Model-Based&#x2F;Human Evaluation Results for Open-ended Generation</h3><p><strong>Model-Based Evaluation</strong><br><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/open-instruct-6.jpg"></p>
<ul>
<li>Models trained on mixtures based on traditional NLP datasets perform poorly</li>
<li>Datasets that encourage long, diverse generations perform best</li>
<li>ShareGPT performs best</li>
</ul>
<blockquote>
<p>The judge model(has bias) may not always reveal the testing model deficiencies.</p>
</blockquote>
<p><strong>Human Evaluation</strong><br><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/open-instruct-7.jpg"></p>
<ul>
<li>Human evaluation results largely correlate with the AlpacaFarm and benchmark-based evaluation</li>
<li>Making use of distilled datasets provides a large performance boost</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p><em><strong>Future Work</strong></em></p>
<ul>
<li>explore instruction-tuning methods that use reinforcement learning</li>
<li>explore more recent strong base models and other instruction datasets</li>
<li>design more versatile model(generality)<ul>
<li>better dataset mixing</li>
<li>instruction-tuning modular models (e.g., <a href="https://arxiv.org/abs/1701.06538">mixture-of-experts</a>)</li>
</ul>
</li>
<li>improve the reliability and scalability of human evaluation for instruction-following models</li>
</ul>
<p><em><strong>Limitations</strong></em></p>
<ul>
<li>Small proportions of data may contain personally identifying details, but this work does not remove them, which may produce toxic or harmful generations.</li>
<li>Not include evaluations of multi-turn dialogue and summarization abilities</li>
</ul>
<p><em><strong>Broader Impact</strong></em></p>
<p>Training and releasing large instruction-tuned models need sufficient testing to limit potential harm.</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>2023</category>
        <category>06</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Instruction-Following</tag>
        <tag>Evaluation</tag>
      </tags>
  </entry>
  <entry>
    <title>[Waiting...]Hexo Usage</title>
    <url>/2023/06/08/hexo-usage/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask for help on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>. Win11 is the default OS in this post.</p>
<h2 id="Hexo-Install-6-3-0"><a href="#Hexo-Install-6-3-0" class="headerlink" title="Hexo Install(6.3.0)"></a>Hexo Install(6.3.0)</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install -g hexo-cli <span class="comment"># cmd with administrator permissions</span></span><br><span class="line">hexo -v <span class="comment"># check whether the installation is successful</span></span><br><span class="line"><span class="built_in">mkdir</span> &lt;root_dir&gt; <span class="comment"># create an empty dir</span></span><br><span class="line"><span class="built_in">cd</span> &lt;root_dir&gt;</span><br><span class="line">hexo init</span><br><span class="line">hexo s <span class="comment"># run server</span></span><br></pre></td></tr></table></figure>
<h2 id="Next-Theme-8-17-0"><a href="#Next-Theme-8-17-0" class="headerlink" title="Next Theme(8.17.0)"></a>Next Theme(8.17.0)</h2><p>Use <a href="https://github.com/next-theme/hexo-theme-next">hexo-theme-next</a> as an example. More info: <a href="https://theme-next.js.org/docs">Theme Next Doc</a>, <a href="http://t.csdn.cn/Tu6fy">CSDN blog 1</a>, <a href="http://t.csdn.cn/EmYFJ">CSDN blog 2</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/next-theme/hexo-theme-next themes/next</span><br><span class="line"><span class="comment"># open root_dir/_config.yml, replace &quot;theme: landscape&quot; with &quot;theme: next&quot;</span></span><br></pre></td></tr></table></figure>

<h2 id="Deploy-to-Github"><a href="#Deploy-to-Github" class="headerlink" title="Deploy to Github"></a>Deploy to Github</h2><p>add “.gitignore” file to blog root dir:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">.DS_Store</span><br><span class="line">Thumbs.db</span><br><span class="line">db.json</span><br><span class="line">*.log</span><br><span class="line">node_modules/</span><br><span class="line">public/</span><br><span class="line">.deploy*/</span><br><span class="line">_multiconfig.yml</span><br></pre></td></tr></table></figure>
<p>open cmd, then cd blog root dir (win10)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install hexo-deployer-git --save <span class="comment"># install a plugin</span></span><br><span class="line">git config --global user.email <span class="string">&quot;you@example.com&quot;</span></span><br><span class="line">git config --global user.name <span class="string">&quot;Your Name&quot;</span></span><br></pre></td></tr></table></figure>
<p>open root_dir&#x2F;_config.yml, modify “deploy”</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: git@github.com:ifshinelx/ifshinelx.github.io.git</span><br><span class="line">  branch: main</span><br></pre></td></tr></table></figure>
<p>deploy (After the cmd execution, it takes several minutes for the github page to refresh)<br>For the first deployment, you need to click <a href="http://ifshinelx.github.io/ifshinelx.github.io">http://ifshinelx.github.io/ifshinelx.github.io</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo clean <span class="comment"># clean cache</span></span><br><span class="line">hexo g <span class="comment"># generate static files</span></span><br><span class="line">hexo d <span class="comment"># deploy to remote sites</span></span><br></pre></td></tr></table></figure>

<span id="more"></span>

<h2 id="Personalization"><a href="#Personalization" class="headerlink" title="Personalization"></a>Personalization</h2><h3 id="Hexo-Basic-Info-config-yml"><a href="#Hexo-Basic-Info-config-yml" class="headerlink" title="Hexo Basic Info(_config.yml)"></a>Hexo Basic Info(_config.yml)</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">title: XinLiu&#x27;s Homepage, Welcome!</span><br><span class="line">subtitle: &#x27;&#x27;</span><br><span class="line">description: &#x27;&#x27;</span><br><span class="line">keywords:</span><br><span class="line">author: Xin Liu</span><br><span class="line">language: en</span><br><span class="line">timezone: &#x27;Asia/Shanghai&#x27;</span><br><span class="line"></span><br><span class="line">url: https://ifshinelx.github.io</span><br><span class="line">math:</span><br><span class="line">  every_page: false</span><br><span class="line">  mathjax:</span><br><span class="line">    enable: true</span><br></pre></td></tr></table></figure>

<h3 id="NexT-Theme-Settings-Basic"><a href="#NexT-Theme-Settings-Basic" class="headerlink" title="NexT Theme Settings Basic"></a><a href="https://theme-next.js.org/docs/theme-settings/">NexT Theme Settings Basic</a></h3><p>root_dir&#x2F;themes&#x2F;next&#x2F;_config.yml<br>add “little star.jpg” to root_dir&#x2F;themes&#x2F;next&#x2F;source&#x2F;images&#x2F;</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cache:</span><br><span class="line">  enable: true</span><br><span class="line"></span><br><span class="line"># Remove unnecessary files after hexo generate.</span><br><span class="line">minify: true</span><br><span class="line"></span><br><span class="line">scheme: Gemini</span><br><span class="line">favicon:</span><br><span class="line">  small: /images/little star.jpg</span><br><span class="line">  medium: /images/little star.jpg</span><br><span class="line">  # small: /images/favicon-16x16-next.png</span><br><span class="line">  # medium: /images/favicon-32x32-next.png</span><br><span class="line">creative_commons:</span><br><span class="line">  size: small</span><br><span class="line">  sidebar: true</span><br><span class="line">  post: true</span><br><span class="line">  language: deed.en</span><br><span class="line">menu:</span><br><span class="line">  home: / || fa fa-home</span><br><span class="line">  archives: /archives/ || fa fa-archive</span><br><span class="line">menu_settings:</span><br><span class="line">  icons: true</span><br><span class="line">  badges: true</span><br></pre></td></tr></table></figure>
<p>More Info: <a href="https://theme-next.js.org/docs/third-party-services/math-equations.html">Math Equations</a>, <a href="https://theme-next.js.org/docs/tag-plugins/label.html">Label</a>, <a href="https://theme-next.js.org/docs/tag-plugins/note.html">Note</a>, <a href="https://theme-next.js.org/docs/tag-plugins/tabs.html">Tabs</a></p>
<h3 id="Sidebar"><a href="#Sidebar" class="headerlink" title="Sidebar"></a><a href="https://theme-next.js.org/docs/theme-settings/sidebar.html">Sidebar</a></h3><p>NexT config file</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sidebar:</span><br><span class="line">  position: right</span><br><span class="line">avatar:</span><br><span class="line">  url: /images/little star.jpg #/images/avatar.gif</span><br><span class="line">social:</span><br><span class="line">  GitHub: https://ifshinelx.github.io || fab fa-github</span><br><span class="line">  E-Mail: mailto:ifshine_lx@163.com || fa fa-envelope</span><br><span class="line">recent_posts_title: Recent Posts</span><br><span class="line">recent_posts_layout: block</span><br><span class="line">recent_posts: true</span><br></pre></td></tr></table></figure>
<p>In root_dir&#x2F;themes&#x2F;next&#x2F;layout&#x2F;_partials&#x2F;sidebar&#x2F;site-overview.njk, add the code block:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;%- if theme.social %&#125;</span><br><span class="line">...</span><br><span class="line">&#123;%- endif %&#125;</span><br><span class="line"></span><br><span class="line">&#123;# recent posts #&#125;</span><br><span class="line">&#123;% if theme.recent_posts %&#125;</span><br><span class="line">  &lt;div class=&quot;links-of-blogroll motion-element &#123;&#123; &quot;links-of-blogroll-&quot; + theme.recent_posts_layout  &#125;&#125;&quot;&gt;</span><br><span class="line">    &lt;div class=&quot;links-of-blogroll-title&quot;&gt;</span><br><span class="line">      &lt;!-- modify icon to fire by szw --&gt;</span><br><span class="line">      &lt;i class=&quot;fa fa-history fa-&#123;&#123; theme.recent_posts_icon | lower &#125;&#125;&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;</span><br><span class="line">      &#123;&#123; theme.recent_posts_title &#125;&#125;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">    &lt;ul class=&quot;links-of-blogroll-list&quot; style=&quot;padding: 0px 12px;&quot;&gt;</span><br><span class="line">      &#123;% set posts = site.posts.sort(&#x27;-date&#x27;) %&#125;</span><br><span class="line">      &#123;% set recent_posts = posts.slice(0, 5).toArray() %&#125;</span><br><span class="line">      &#123;% for post in recent_posts %&#125;</span><br><span class="line">        &#123;% if post.title != &quot;Home&quot; %&#125;</span><br><span class="line">          &lt;li class=&quot;recent_posts_li&quot;&gt;</span><br><span class="line">            &lt;a href=&quot;&#123;&#123; url_for(post.path) &#125;&#125;&quot; title=&quot;&#123;&#123; post.title &#125;&#125;&quot; target=&quot;_blank&quot;&gt;&#123;&#123;date(post.date, &#x27;MM-DD&#x27;) &#125;&#125; &#123;&#123; post.title &#125;&#125; &lt;/a&gt;</span><br><span class="line">          &lt;/li&gt;</span><br><span class="line">        &#123;% endif %&#125;</span><br><span class="line">      &#123;% endfor %&#125;</span><br><span class="line">    &lt;/ul&gt;</span><br><span class="line">  &lt;/div&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br><span class="line"></span><br><span class="line">&#123;%- if theme.creative_commons.license and theme.creative_commons.sidebar %&#125;</span><br><span class="line">...</span><br><span class="line">&#123;%- endif %&#125;</span><br></pre></td></tr></table></figure>
<p>In root_dir&#x2F;themes&#x2F;next&#x2F;source&#x2F;css&#x2F;main.styl, add the code block in the file end.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">li.recent_posts_li &#123;</span><br><span class="line">    text-align: left;</span><br><span class="line">    display: block;</span><br><span class="line">    word-break: keep-all;</span><br><span class="line">    white-space: nowrap;</span><br><span class="line">    overflow: hidden;</span><br><span class="line">    text-overflow: ellipsis;</span><br><span class="line"></span><br><span class="line">    &amp;:hover &#123;</span><br><span class="line">      a&#123;</span><br><span class="line">        color: #fc6423;</span><br><span class="line">        border-bottom-color: #fc6423;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h3 id="Footer"><a href="#Footer" class="headerlink" title="Footer"></a><a href="https://theme-next.js.org/docs/theme-settings/footer.html">Footer</a></h3><p>NexT config file</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">footer:</span><br><span class="line">  since: 2023</span><br><span class="line">  powered: false</span><br><span class="line">busuanzi_count:</span><br><span class="line">  enable: true</span><br></pre></td></tr></table></figure>
<p>In root_dir&#x2F;themes&#x2F;next&#x2F;layout&#x2F;_partials&#x2F;footer.njk, delete the code block:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;div class=&quot;wordcount&quot;&gt;</span><br><span class="line">...</span><br><span class="line">&lt;/div&gt;</span><br></pre></td></tr></table></figure>
<p>In root_dir&#x2F;themes&#x2F;next&#x2F;layout&#x2F;_partials&#x2F;footer.njk, add the code block:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;%- if theme.busuanzi_count.enable %&#125;</span><br><span class="line">&lt;div class=&quot;busuanzi-count&quot;&gt;</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  &#123;%- if config.symbols_count_time.total_symbols %&#125;</span><br><span class="line">  &lt;span class=&quot;post-meta-item&quot;&gt;</span><br><span class="line">    &lt;span class=&quot;post-meta-item-icon&quot;&gt;</span><br><span class="line">      &lt;i class=&quot;fa fa-chart-line&quot;&gt;&lt;/i&gt;</span><br><span class="line">    &lt;/span&gt;</span><br><span class="line">    &#123;%- if theme.symbols_count_time.item_text_total %&#125;</span><br><span class="line">      &lt;span&gt;&#123;&#123; __(&#x27;symbols_count_time.count_total&#x27;) + __(&#x27;symbol.colon&#x27;) &#125;&#125;&lt;/span&gt;</span><br><span class="line">    &#123;%- endif %&#125;</span><br><span class="line">    &lt;span title=&quot;&#123;&#123; __(&#x27;symbols_count_time.count_total&#x27;) &#125;&#125;&quot;&gt;&#123;&#123; symbolsCountTotal(site) &#125;&#125;&lt;/span&gt;</span><br><span class="line">  &lt;/span&gt;</span><br><span class="line">  &#123;%- endif %&#125;</span><br><span class="line"></span><br><span class="line">&lt;/div&gt;</span><br><span class="line">&#123;%- endif %&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Posts"><a href="#Posts" class="headerlink" title="Posts"></a><a href="https://theme-next.js.org/docs/theme-settings/posts.html">Posts</a></h3><p>a <strong>Read More</strong> button in a post: </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;!-- more --&gt;</span><br></pre></td></tr></table></figure>
<p>a plug-in</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm i hexo-word-counter --save</span><br><span class="line">hexo clean</span><br></pre></td></tr></table></figure>
<p>NexT config file</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tag_icon: true</span><br></pre></td></tr></table></figure>

<h3 id="Custom-Pages-Tags-Categories-Home"><a href="#Custom-Pages-Tags-Categories-Home" class="headerlink" title="Custom Pages(Tags, Categories, Home)"></a><a href="https://theme-next.js.org/docs/theme-settings/custom-pages.html">Custom Pages(Tags, Categories, Home)</a></h3><p><a href="https://hexo.io/docs/front-matter#Categories-amp-Tags">Hexo’s Docs of Categories &amp; Tags</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> root_dir</span><br><span class="line">hexo new page tags</span><br><span class="line">hexo new page categories</span><br></pre></td></tr></table></figure>
<p>NexT config file</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">menu:</span><br><span class="line">  tags: /tags/ || fa fa-tags</span><br><span class="line">  categories: /categories/ || fa fa-th</span><br></pre></td></tr></table></figure>

<p>root_dir&#x2F;source&#x2F;tags&#x2F;index.md</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">date: 2023-06-10 21:55:15</span><br><span class="line">type: &quot;tags&quot;</span><br><span class="line">comments: false</span><br><span class="line">---</span><br></pre></td></tr></table></figure>
<p>root_dir&#x2F;source&#x2F;categories&#x2F;index.md</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">date: 2023-06-10 21:55:25</span><br><span class="line">type: &quot;categories&quot;</span><br><span class="line">comments: false</span><br><span class="line">---</span><br></pre></td></tr></table></figure>
<p>tag color setting</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// create themes\next\layout\tag-color.njk</span><br><span class="line">&lt;script type=&quot;text/javascript&quot;&gt;</span><br><span class="line">     var alltags = document.getElementsByClassName(&#x27;tag-cloud-tags&#x27;);</span><br><span class="line">     var tags = alltags[0].getElementsByTagName(&#x27;a&#x27;);</span><br><span class="line">     for (var i = tags.length - 1; i &gt;= 0; i--) &#123;</span><br><span class="line">       var golden_ratio = 0.618033988749895;</span><br><span class="line">       var s = 0.5;</span><br><span class="line">       var v = 0.999;</span><br><span class="line">       var h = golden_ratio + Math.random()*0.8 - 0.5;</span><br><span class="line">       var h_i = parseInt(h * 6);</span><br><span class="line">       var f = h * 6 - h_i;</span><br><span class="line">       var p = v * (1 - s);</span><br><span class="line">       var q = v * (1 - f * s);</span><br><span class="line">       var t = v * (1 - (1 - f) * s);</span><br><span class="line">       var r, g, b;</span><br><span class="line">       switch (h_i) &#123;</span><br><span class="line">          case 0:</span><br><span class="line">              r = v;</span><br><span class="line">              g = t;</span><br><span class="line">              b = p;</span><br><span class="line">              break;</span><br><span class="line">          case 1:</span><br><span class="line">              r = q;</span><br><span class="line">              g = v;</span><br><span class="line">              b = p;</span><br><span class="line">              break;</span><br><span class="line">          case 2:</span><br><span class="line">              r = p;</span><br><span class="line">              g = v;</span><br><span class="line">              b = t;</span><br><span class="line">              break;</span><br><span class="line">          case 3 :</span><br><span class="line">              r = p;</span><br><span class="line">              g = q;</span><br><span class="line">              b = v;</span><br><span class="line">              break;</span><br><span class="line">          case 4:</span><br><span class="line">              r = t;</span><br><span class="line">              g = p;</span><br><span class="line">              b = v;</span><br><span class="line">              break;</span><br><span class="line">          case 5:</span><br><span class="line">              r = v;</span><br><span class="line">              g = p;</span><br><span class="line">              b = q;</span><br><span class="line">              break;</span><br><span class="line">          default:</span><br><span class="line">              r = 1;</span><br><span class="line">              g = 1;</span><br><span class="line">              b = 1;</span><br><span class="line">        &#125;</span><br><span class="line">       tags[i].style.background = &quot;rgba(&quot;+parseInt(r*255)+&quot;,&quot;+parseInt(g*255)+&quot;,&quot;+parseInt(b*255)+&quot;,&quot;+0.5+&quot;)&quot;;</span><br><span class="line">     &#125;</span><br><span class="line">&lt;/script&gt;</span><br><span class="line"></span><br><span class="line">&lt;style&gt;</span><br><span class="line">  .tag-cloud-tags&#123;</span><br><span class="line">    text-align: center;</span><br><span class="line">    counter-reset: tags;</span><br><span class="line">  &#125;</span><br><span class="line">  .tag-cloud-tags a&#123;</span><br><span class="line">    display: inline-block;</span><br><span class="line">    border: 0px;</span><br><span class="line">    border-radius: 10px;</span><br><span class="line">    padding: 0px 10px;</span><br><span class="line">    margin: 8px;</span><br><span class="line">    color: rgba(34, 34, 34, 0.8);</span><br><span class="line">    </span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  .tag-cloud-tags a:hover&#123;</span><br><span class="line">     box-shadow: 0px 5px 15px 0px rgba(0,0,0,.4);</span><br><span class="line">     transform: scale(1.1);</span><br><span class="line">     transition-duration: 0.15s;</span><br><span class="line">  &#125;</span><br><span class="line">&lt;/style&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// modify themes\next\layout\_partials\page\tags.njk</span><br><span class="line">&lt;div class=&quot;tag-cloud&quot;&gt;</span><br><span class="line">  ...</span><br><span class="line">&lt;/div&gt;</span><br><span class="line">&#123;% include &#x27;tag-color.njk&#x27; %&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// modify themes\next\layout\_macro\post.njk</span><br><span class="line">      &lt;header&gt;</span><br><span class="line">        ...</span><br><span class="line">        </span><br><span class="line">        &#123;%- if post.tags and post.tags.length %&#125;</span><br><span class="line">          &#123;%- set tag_indicate = &#x27;&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt;&#x27; if theme.tag_icon else &#x27;#&#x27; %&#125;</span><br><span class="line">          &lt;div class=&quot;post-tags&quot; style=&quot;margin-top: 5px;&quot;&gt;</span><br><span class="line">            &#123;%- for tag in post.tags.toArray() %&#125;</span><br><span class="line">              &lt;a href=&quot;&#123;&#123; url_for(tag.path) &#125;&#125;&quot; rel=&quot;tag&quot; style=&quot;border: 0px; border-radius: 10px; padding: 0px 10px;&quot;&gt;&#123;&#123; tag_indicate &#125;&#125; &#123;&#123; tag.name &#125;&#125;&lt;/a&gt;</span><br><span class="line">            &#123;%- endfor %&#125;</span><br><span class="line">          &lt;/div&gt;</span><br><span class="line">          &lt;script type=&quot;text/javascript&quot;&gt;</span><br><span class="line">              var tagsall=document.getElementsByClassName(&quot;post-tags&quot;)</span><br><span class="line">              for (var i = tagsall.length - 1; i &gt;= 0; i--)&#123;</span><br><span class="line">                  var tags=tagsall[i].getElementsByTagName(&quot;a&quot;);</span><br><span class="line">                  for (var j = tags.length - 1; j &gt;= 0; j--) &#123;</span><br><span class="line">                      var r=Math.floor(Math.random()*75+200);</span><br><span class="line">                      var g=Math.floor(Math.random()*75+200);</span><br><span class="line">                      var b=Math.floor(Math.random()*75+200);</span><br><span class="line">                      tags[j].style.background = &quot;rgb(&quot;+r+&quot;,&quot;+g+&quot;,&quot;+b+&quot;)&quot;;</span><br><span class="line">                  &#125;</span><br><span class="line">              &#125;                        </span><br><span class="line">            &lt;/script&gt;</span><br><span class="line">        &#123;%- endif %&#125;</span><br><span class="line">      &lt;/header&gt;</span><br></pre></td></tr></table></figure>

<p><strong>Home Page</strong>: root_dir&#x2F;themes&#x2F;next&#x2F;layout&#x2F;index.njk:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;% block content %&#125;</span><br><span class="line"></span><br><span class="line">  &#123;%- set postlen = site.posts.toArray().length %&#125;</span><br><span class="line">  &#123;%- set post = site.posts.sort(&#x27;-date&#x27;).toArray()[postlen-1] %&#125;</span><br><span class="line">  &#123;&#123; partial(&#x27;_macro/home.njk&#x27;, &#123;post: post, is_index: true&#125;) &#125;&#125;</span><br><span class="line"></span><br><span class="line">&#123;% endblock %&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Search-Services-Searchdb-not-Algolia"><a href="#Search-Services-Searchdb-not-Algolia" class="headerlink" title="Search Services(Searchdb, not Algolia)"></a><a href="https://theme-next.js.org/docs/third-party-services/search-services.html">Search Services(Searchdb, not Algolia)</a></h3><p>Details of Algolia Search are in <a href="https://theme-next.js.org/docs/third-party-services/search-services.html#Algolia-Search">here</a>.<br><a href="https://github.com/next-theme/hexo-generator-searchdb">Searchdb</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm i hexo-generator-searchdb --save</span><br></pre></td></tr></table></figure>
<p>Hexo config:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># search hexo-generator-searchdb</span><br><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br><span class="line">  content: true</span><br><span class="line">  format: html</span><br><span class="line">  limit: 100</span><br></pre></td></tr></table></figure>
<p>Next config:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">local_search:</span><br><span class="line">  enable: true</span><br></pre></td></tr></table></figure>

<h3 id="Comment-Systems-Gitalk"><a href="#Comment-Systems-Gitalk" class="headerlink" title="Comment Systems(Gitalk)"></a><a href="https://theme-next.js.org/docs/third-party-services/comments.html">Comment Systems(Gitalk)</a></h3><p>Click here to sign up for a <a href="https://github.com/settings/applications/new">new OAuth Application</a>. Other content can be filled in at will, but be sure to fill in the correct callback URL (usually the domain name corresponding to the comment page). Then you will get a Client ID and a Client secret.</p>
<p>Create a public repository you want to store Gitalk comments in your GitHub.</p>
<p>Set the value <code>enable</code> to <code>true</code>, add Client ID (<code>client_id</code>) and Client secret (<code>client_secret</code>) in step 1, add your Github username (<code>github_id</code> and <code>admin_user</code>) and the created repository name (<code>repo</code>) in step 2, and edit other configurations in <code>gitalk</code> section in the <em><strong>NexT config file</strong></em> as following:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Gitalk</span><br><span class="line"># For more information: https://gitalk.github.io</span><br><span class="line">gitalk:</span><br><span class="line">  enable: false</span><br><span class="line">  github_id: # GitHub repo owner</span><br><span class="line">  repo: # Repository name to store issues</span><br><span class="line">  client_id: # GitHub Application Client ID</span><br><span class="line">  client_secret: # GitHub Application Client Secret</span><br><span class="line">  admin_user: # GitHub repo owner and collaborators, only these guys can initialize gitHub issues</span><br><span class="line">  distraction_free_mode: true # Facebook-like distraction free mode</span><br><span class="line">  # When the official proxy is not available, you can change it to your own proxy address</span><br><span class="line">  proxy: https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token # This is official proxy address</span><br><span class="line">  # Gitalk&#x27;s display language depends on user&#x27;s browser or system environment</span><br><span class="line">  # If you want everyone visiting your site to see a uniform language, you can set a force language value</span><br><span class="line">  # Available values: en | es-ES | fr | ru | zh-CN | zh-TW</span><br><span class="line">  language:</span><br></pre></td></tr></table></figure>

<h3 id="Waiting…-Statistics-and-Analytics-Umami"><a href="#Waiting…-Statistics-and-Analytics-Umami" class="headerlink" title="[Waiting…] Statistics and Analytics(Umami)"></a>[Waiting…] <a href="https://theme-next.js.org/docs/third-party-services/statistics-and-analytics.html">Statistics and Analytics(Umami)</a></h3><p><a href="http://t.csdn.cn/OCEnG">MySQL</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">l</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Software</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>Home</title>
    <url>/2023/06/08/home/</url>
    <content><![CDATA[<p>I am currently a master student at <a href="https://www.ecnu.edu.cn/">ECNU</a>. My recent research interest mainly focuses on multi-modal learning (especially combined with LLMs). You can <a href="mailto:ifshine_lx@163.com">email</a> me for further communication.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/home.jpg"></p>
]]></content>
  </entry>
  <entry>
    <title>LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention</title>
    <url>/2023/06/09/llama-adapter-v1/</url>
    <content><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-1.jpg" alt="authors">Paper: <a href="https://arxiv.org/pdf/2303.16199.pdf">https://arxiv.org/pdf/2303.16199.pdf</a></p>
<p>Code: <a href="https://github.com/OpenGVLab/LLaMA-Adapter">https://github.com/OpenGVLab/LLaMA-Adapter</a></p>
<p>More Info: <a href="https://github.com/Lightning-AI/lit-parrot">Lighting AI | Lit-Parrot: lightweight update of llama</a></p>
<p><em><strong>When reading this note, you can think about the following questions:</strong></em></p>
<ol>
<li>What is learnable adaption prompts?</li>
<li>What is zero-init attention?</li>
<li>How to extend LLaMA-Adapter to multi-modal input?</li>
</ol>
<hr>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><em>4 characteristics of LLaMA-Adapter:</em></p>
<ol>
<li>1.2M Parameters</li>
<li>1 Hour: 8 A100 GPUs, three times faster than Alpaca</li>
<li>Plug with Expertise: insert their respective adapters and endow LLaMA with different expert knowledge</li>
<li>Multi-modal: simply add images tokens into adaption prompts</li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1--2.jpg" alt="LLaMA-Adapter&#39;s 4 characteristics and details"> <strong>The idea of LLaMA-Adapter is model-agnostic and can be applied to other LLMs.</strong></p>
<h3 id="Learnable-Adaption-Prompts"><a href="#Learnable-Adaption-Prompts" class="headerlink" title="Learnable Adaption Prompts"></a>Learnable Adaption Prompts</h3><ul>
<li>$K$: prompt length for each layer</li>
<li>$C$: feature dim of transformer</li>
<li>$N$: total transformer layer num of LLaMA</li>
<li><strong>${ P_l }_{l&#x3D;1}^{L}$ ($P_l\in\mathbb{R}^{K\times C}$) : learnable adaption prompts</strong> for topmost $L (L\le N)$ transformer layers with higher-level semantic representations</li>
<li>$T_l\in\mathbb{R}^{M\times C}$: $M$-length word tokens in $l$-th inserted layer</li>
<li>$[P_l;\space T_l]\in\mathbb{R}^{(K+M)\times C}$: concatenate $P_l$ and $T_l$, the learned <strong>instruction knowledge</strong> in $P_l$ guides $T_l$ to generate contextual responses</li>
</ul>
<h3 id="Zero-init-Attention"><a href="#Zero-init-Attention" class="headerlink" title="Zero-init Attention"></a>Zero-init Attention</h3><p>If the adaption prompts are randomly initialized, they will bring noise to the word tokens and damage original knowledge in LLaMA at the early training stage, which harms stablity and effectiveness.</p>
<p><em>$t_l\in\mathbb{R}^{1\times C}$: generate the (M+1)-th word $t_l$ on top of $[P_l;\space T_l]$</em> at the $l$-th inserted layer</p>
<ol>
<li>linear projection: queries $Q_l&#x3D;Linear_q(t_l)$, keys $K_l&#x3D;Linear_k([P_l;T_l;t_l])$, values $V_l&#x3D;Linear_v([P_l;T_l;t_l])$</li>
<li>attention scores: $S_l&#x3D;\frac{Q_lK_l^T}{\sqrt{C}}\in\mathbb{R}^{1\times(K+M+1)}$, records the feature similarities between $t_l$ and all $K+M+1$ tokens</li>
<li>reformulation: $S_l&#x3D;[S_l^K;S_l^{M+1}]^T, S_l^K\in\mathbb{R}^{K\times 1}, S_l^{M+1}\in\mathbb{R}^{(M+1)\times 1}$<ul>
<li>$S_l^K$ represents how much information the $P_l$ contribute to $t_l$, which probably causes noise in the early training stage</li>
</ul>
</li>
<li>softmax operation: $S_l^g&#x3D;[Softmax(S_l^K)·g_l;Softmax(S_l^{M+1})]^T$<ul>
<li>$g_l$ is a learnable gating factor initialized by zero, adaptively controls the importance of $S_l^K$</li>
<li>in practice, each head of attention has an independent $g_l$</li>
</ul>
</li>
<li>output of the attention layer: $t_l^o&#x3D;Linear_o(S_l^gV_l)\in\mathbb{R}^{1\times C}$</li>
</ol>
<h3 id="Multi-modal-Reasoning"><a href="#Multi-modal-Reasoning" class="headerlink" title="Multi-modal Reasoning"></a>Multi-modal Reasoning</h3><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-3.jpg"> Task textual input for ScienceQA: question + textual context + options. (We utilize “Generate caption for this image” as the textual instruction input for LLaMA-Adapter)</p>
<ol>
<li>multi-scale global visual features: ${I_m}_{m&#x3D;1}^{M},I_m\in\mathbb{R}^{1\times C_m}$, $M$ is the scale num<ul>
<li>from pre-trained CLIP</li>
</ul>
</li>
<li>overall image token: $I_p&#x3D;Projection\Big(Concat\left({I_m}_{m&#x3D;1}^M\right)\Big)\in\mathbb{R}^{1\times C}$<ul>
<li>concatenate along the channel dim</li>
<li>utilize cascaded MLPs as the learnable projection network with 0.6M parameters</li>
</ul>
</li>
<li>repeat $I_p$ for $K$ times</li>
<li>multi-modal prompt: $P_l^v&#x3D;P_l+Repeat(I_p)\in\mathbb{R}^{K\times C}$</li>
</ol>
<p><strong>Future work: using pre-trained modal-specific encoders, we can integrate instructional signals of different modalities into the adaption prompts</strong></p>
<h3 id="Zero-initialized-Attention-for-other-Large-Models"><a href="#Zero-initialized-Attention-for-other-Large-Models" class="headerlink" title="Zero-initialized Attention for other Large Models"></a>Zero-initialized Attention for other Large Models</h3><p>In addition to instruction-following models, our zero-initialized attention can be generalized to other vision and language models for parameter-efficient fine-tuning.</p>
<p><strong>Vision Model</strong>. We insert the adaption prompts as prefix into the topmost L transformer layers in the pre-trained <a href="https://arxiv.org/pdf/2010.11929.pdf">ViT</a>, and modify the attention operations to be zero-initialized at all inserted layers.</p>
<p><strong>Language Model</strong>. We evaluate our fine-tuning efficacy on <a href="https://arxiv.org/pdf/1907.11692.pdf">RoBERTa</a>, and implement the zero-initialized attention on top of <a href="https://arxiv.org/pdf/2110.07602.pdf">P-tuning v2</a>, a prompt tuning method for efficiently adapting large language models. Likewise, we only enable the prompt tokens in P-tuning v2 and our zero gating factors to be learnable during fine-tuning.</p>
<p><strong>Vision-Language Model</strong>. In detail, we adopt CLIP with a ViT-B&#x2F;16 as the visual encoder and a 12-layer transformer as the textual encoder. We freeze the entire CLIP and insert the adaption prompts with zero-initialized attention into CLIP’s visual encoder.</p>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Instruction-following-Evaluation"><a href="#Instruction-following-Evaluation" class="headerlink" title="Instruction-following Evaluation"></a>Instruction-following Evaluation</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">epochs: 5</span><br><span class="line">warmup epochs: 2</span><br><span class="line">batch size: 64</span><br><span class="line">learning rate: 0.009</span><br><span class="line">weight decay: 0.02</span><br><span class="line"># LLaMA-7B</span><br><span class="line">N: 32</span><br><span class="line">K: 10</span><br><span class="line">L: 30</span><br></pre></td></tr></table></figure>

<p><strong>Generation Stage Decoding Method</strong>: top-p sampling with a temperature 0.1 and a top-p &#x3D; 0.75</p>
<p><strong>Dataset</strong>: Alpaca-52K self-instruct data. LLaMA-Adapter paper wrongly denotes as Alphaca-52K</p>
<p><strong>Evaluation Metric</strong>: (1) quantitative evaluation, ask GPT-4 to assess the response quality on 80 questions(Since we observed that GPT-4 has a preference to give higher scores to the first response in comparison, we also switch the position of two responses, resulting in a total of 160 evaluation items.); (2) simply show some response examples</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-15.jpg"></p>
<p>Comparison of Instruction-Following Capability, LLaMA-Adapter is comparable to Alpaca with fully fine-tuned 7B parameters<br><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-4.jpg" alt="Comparison of Instruction-Following Capability"></p>
<p>Comparison with Instruct LLaMA (LLaMA-I, LLaMA-65B fine-tuned on large-scale instructional data), LLaMA-Adapter can be further enhanced with larger LLaMA, larger data, larger learnable parameters<br><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-5.jpg" alt="Comparison with Instruct LLaMA (LLaMA-I)"></p>
<h3 id="Multi-modal-Evaluation"><a href="#Multi-modal-Evaluation" class="headerlink" title="Multi-modal Evaluation"></a>Multi-modal Evaluation</h3><p><strong>Generation Stage Decoding Method</strong>: greedy search</p>
<p>Other hyperparameters are the same as instruction-following LLaMA-Adapter.</p>
<p><strong>Dataset</strong>: ScienceQA, COCO Caption</p>
<p><strong>Result on ScienceQA</strong>: MM-CoT relies on the complex two-stage inference. <strong>Future Work: leverage CoT to boost LLaMA-Adapter.</strong><br><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-7.jpg" alt="ScienceQA"></p>
<p><strong>Result on COCO Caption</strong>: Both BLIP and BLIP-2 adopt a costly pre-training stage on additional datasets for superior performance. In contrast, our LLaMA-Adapter only requires COCO Catption’s training set of 0.6M data and attains better accuracy than ClipCap. <img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-16.jpg"></p>
<h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>(ScienceQA for example)</p>
<p>Result: table 6 shows robustness to over-fitting on the small dataset. Even if LLaMA-Adapter has over-fitted the fine-tuning data(val loss), the val acc is still increasing.<br><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-8.jpg" alt="Ablation Study"></p>
<h3 id="Zero-initialized-Attention-for-other-Large-Models-1"><a href="#Zero-initialized-Attention-for-other-Large-Models-1" class="headerlink" title="Zero-initialized Attention for other Large Models"></a>Zero-initialized Attention for other Large Models</h3><p><strong>Vision Model——ViT</strong>: Image classification.(Table 7, Table 9)</p>
<p><strong>Language Model——RoBERTa</strong>: (1) Extractive question answering(Table 8), Exact Match (EM) and F1 scores on the dev set are reported. (2) NER and SRL.(Table 10)</p>
<p><strong>Vision-Language Model——CLIP</strong>: The model is trained only on the base classes in a few-shot setting and evaluated on both base and novel categories.(Table 11)</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-17.jpg"> <img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-18.jpg"> <img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-19.jpg"> <img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-20.jpg"></p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Future direction: </p>
<ul>
<li>wider multi-modal inputs(audio, video, point clouds)<ul>
<li>using pre-trained modal-specific encoders, we can integrate instructional signals of different modalities into the adaption prompts</li>
</ul>
</li>
<li>larger LLaMA(33B, 65B)</li>
<li>other LLMs</li>
<li>diverse benchmarks(VQA v2, OK-VQA, TVQA, DocVQA)<ul>
<li>ScienceQA is only an understanding task</li>
</ul>
</li>
<li>leverage CoT to boost LLaMA-Adapter</li>
</ul>
<h2 id="Code-Implementation"><a href="#Code-Implementation" class="headerlink" title="Code Implementation"></a>Code Implementation</h2><h3 id="Params"><a href="#Params" class="headerlink" title="Params"></a>Params</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">accum_iter: 1. Accumulate gradient iterations (for increasing the effective batch size under memory constraints)</span><br><span class="line">batch_size: 4(per GPU). effective_batch_size = batch_size * accum_iter * gpu_num</span><br><span class="line">epoch: 5</span><br><span class="line">adapter_layer: 30. the num of adapter layer L</span><br><span class="line">adapter_len: 10. the adapter length K</span><br><span class="line">max_seq_len: 512. specifies the maximum number of input tokens. token num &gt;= word num.</span><br><span class="line">max_batch_size: 32.</span><br><span class="line">dim: 4096.</span><br><span class="line">n_heads: 32.</span><br><span class="line">n_layers: 32.</span><br><span class="line">weight_decay: 0.02.</span><br><span class="line">blr: 9e-3. base learning rate.</span><br><span class="line">lr: learning_rate(absolute lr), lr = blr * total_batch_size / 256</span><br><span class="line">min_lr: 0.0. lower lr bound for cyclic schedulers that hit 0</span><br><span class="line">warmup_epochs: 2.</span><br><span class="line">seed: 0.</span><br></pre></td></tr></table></figure>

<p>Why we need max_seq_len? For absolute position embedding(e.g., BERT, Roberta, BART), it uses the index of each token to calculate and its length is limited(max_seq_len). When the input token length exceeds max_seq_len, <strong>“index error”</strong> will be caused. For other position embedding methods(e.g., XLNet, T5), they have no limit of input token length. But longer input token length brings <strong>heavier memory burden</strong>, which may not necessarily lead to better performance.</p>
<p>LLaMA uses Rotary Position Embedding: <a href="https://zhuanlan.zhihu.com/p/627536105">分析 | ROPE的不同实现：llama&amp;palm</a>, <a href="http://t.csdn.cn/U3RXo">blog 2.3 RoPE旋转位置编码</a></p>
<h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">InstructionDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_path, model_path, max_seq_len, partition=<span class="string">&quot;train&quot;</span></span>):</span><br><span class="line">        self.ann = json.load(<span class="built_in">open</span>(data_path))</span><br><span class="line">        <span class="keyword">if</span> partition == <span class="string">&quot;train&quot;</span>:</span><br><span class="line">            self.ann = self.ann</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># It seems that the val set is a sub set of the train set.(data_path is same)</span></span><br><span class="line">            self.ann = self.ann[:<span class="number">200</span>]</span><br><span class="line">        ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        ann = self.ann[index]</span><br><span class="line">        <span class="keyword">if</span> ann.get(<span class="string">&quot;input&quot;</span>, <span class="string">&quot;&quot;</span>) == <span class="string">&quot;&quot;</span>:</span><br><span class="line">            prompt = PROMPT_DICT[<span class="string">&quot;prompt_no_input&quot;</span>].format_map(ann)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            prompt = PROMPT_DICT[<span class="string">&quot;prompt_input&quot;</span>].format_map(ann)</span><br><span class="line">        example = prompt + ann[<span class="string">&quot;output&quot;</span>]</span><br><span class="line">        example = torch.tensor(self.tokenizer1.encode(example, bos=<span class="literal">True</span>, eos=<span class="literal">True</span>), dtype=torch.int64)</span><br><span class="line">        padding = self.max_words - example.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># prompt = Propmt(template, ann[&#x27;instruction&#x27;], ann[&#x27;input&#x27;])</span></span><br><span class="line">        <span class="comment"># max_seq_len refers to tokenizer([prompt, ann[&#x27;output&#x27;]]).length, not tokenizer(prompt).length</span></span><br><span class="line">        <span class="keyword">if</span> padding &gt; <span class="number">0</span>:</span><br><span class="line">            example = torch.cat((example, torch.zeros(padding, dtype=torch.int64) - <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">elif</span> padding &lt; <span class="number">0</span>:</span><br><span class="line">            example = example[: self.max_words]</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>

<h3 id="Learnable-Adaption-Prompts-1"><a href="#Learnable-Adaption-Prompts-1" class="headerlink" title="Learnable Adaption Prompts"></a>Learnable Adaption Prompts</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module): <span class="comment"># Decoder</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, params: ModelArgs</span>):</span><br><span class="line">        ...</span><br><span class="line">        <span class="comment"># randomly initialise the adaption prompts</span></span><br><span class="line">        <span class="comment"># github.com/OpenGVLab/LLaMA-Adapter/issues/9#issuecomment-1501705647</span></span><br><span class="line">        self.adapter_query = nn.Embedding(params.adapter_len * params.adapter_layer, params.dim)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, examples, labels</span>):</span><br><span class="line">        _bsz, seqlen = examples.shape</span><br><span class="line">        ...</span><br><span class="line">        mask = torch.full((<span class="number">1</span>, <span class="number">1</span>, seqlen, seqlen), <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>), device=h.device)</span><br><span class="line">        mask = torch.triu(mask, diagonal=<span class="number">0</span> + <span class="number">1</span>).type_as(h) <span class="comment"># Upper triangular matrix, and diagonal val is 0</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers[: -<span class="number">1</span> * self.adapter_layer]:</span><br><span class="line">            h = layer(h, start_pos, freqs_cis, mask)</span><br><span class="line">        adapter_index = <span class="number">0</span></span><br><span class="line">        <span class="comment"># adapter.shape: (30, 1, 10, 4096)</span></span><br><span class="line">        adapter = self.adapter_query.weight.reshape(-<span class="number">1</span>, self.adapter_len, <span class="number">4096</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers[-<span class="number">1</span> * self.adapter_layer :]:</span><br><span class="line">            h = layer(h, start_pos, freqs_cis, mask, adapter[adapter_index].half())</span><br><span class="line">            adapter_index = adapter_index + <span class="number">1</span></span><br><span class="line">        output = self.output(self.norm(h))</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<ul>
<li>linear projection: queries $Q_l&#x3D;Linear_q(t_l)$, keys $K_l&#x3D;Linear_k([P_l;T_l;t_l])$, values $V_l&#x3D;Linear_v([P_l;T_l;t_l])$</li>
<li>attention scores: $S_l&#x3D;\frac{Q_lK_l^T}{\sqrt{C}}\in\mathbb{R}^{1\times(K+M+1)}$</li>
<li>reformulation: $S_l&#x3D;[S_l^K;S_l^{M+1}]^T, S_l^K\in\mathbb{R}^{K\times 1}, S_l^{M+1}\in\mathbb{R}^{(M+1)\times 1}$</li>
<li>softmax operation: $S_l^g&#x3D;[Softmax(S_l^K)·g_l;Softmax(S_l^{M+1})]^T$</li>
<li>output of the attention layer: $t_l^o&#x3D;Linear_o(S_l^gV_l)\in\mathbb{R}^{1\times C}$</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, args: ModelArgs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        ...</span><br><span class="line">        <span class="comment"># zero-init attention</span></span><br><span class="line">        self.gate = torch.nn.Parameter(torch.zeros(<span class="number">1</span>, self.n_local_heads, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor, start_pos: <span class="built_in">int</span>, freqs_cis: torch.Tensor, mask: <span class="type">Optional</span>[torch.Tensor], adapter=<span class="literal">None</span></span>):</span><br><span class="line">        bsz, seqlen, _ = x.shape</span><br><span class="line">        <span class="comment"># 1. three Linears for x</span></span><br><span class="line">        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)</span><br><span class="line">        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)</span><br><span class="line">        xk = xk.view(bsz, seqlen, self.n_local_heads, self.head_dim)</span><br><span class="line">        xv = xv.view(bsz, seqlen, self.n_local_heads, self.head_dim)</span><br><span class="line">        <span class="comment"># 2. add position info via Rotary Position Embedding</span></span><br><span class="line">        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> adapter <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            adapter_len = adapter.shape[<span class="number">1</span>] <span class="comment"># adapter.shape: (1, 10, 4096)</span></span><br><span class="line">            <span class="comment"># linear projection</span></span><br><span class="line">            <span class="comment"># adapter_k.shape: (bsz, adapter_len, self.n_local_heads, self.head_dim)</span></span><br><span class="line">            adapter_k = self.wk(adapter).view(<span class="number">1</span>, adapter_len, self.n_local_heads, self.head_dim).repeat(bsz, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            adapter_v = self.wv(adapter).view(<span class="number">1</span>, adapter_len, self.n_local_heads, self.head_dim).repeat(bsz, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            xk = torch.cat([adapter_k, xk], dim=<span class="number">1</span>)</span><br><span class="line">            xv = torch.cat([adapter_v, xv], dim=<span class="number">1</span>)</span><br><span class="line">            extra_mask = torch.zeros(<span class="number">1</span>, <span class="number">1</span>, seqlen, adapter_len).to(mask)</span><br><span class="line">            mask = torch.cat([extra_mask, mask], dim=-<span class="number">1</span>) <span class="comment"># (1, 1, seqlen, adapter_len + seqlen)</span></span><br><span class="line">        keys = xk</span><br><span class="line">        values = xv</span><br><span class="line"></span><br><span class="line">        xq = xq.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        keys = keys.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        values = values.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 3. attention scores</span></span><br><span class="line">        scores = torch.matmul(xq, keys.transpose(<span class="number">2</span>, <span class="number">3</span>)) / math.sqrt(self.head_dim)</span><br><span class="line">        <span class="comment"># for decoder type, mask is needed to avoid using the information in the future.</span></span><br><span class="line">        <span class="comment"># the predictions for position i can depend only on the known outputs at positions less than i</span></span><br><span class="line">        <span class="comment"># mask.shape: (1, 1, seqlen, adapter_len + seqlen)</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            scores = scores + mask  <span class="comment"># (bsz, n_local_heads, seqlen, adapter_len + seqlen)</span></span><br><span class="line">        <span class="comment"># 4. softmax</span></span><br><span class="line">        <span class="keyword">if</span> adapter <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            scores = torch.cat(</span><br><span class="line">                [</span><br><span class="line">                    <span class="comment"># zero-init attention</span></span><br><span class="line">                    self.gate.tanh().half() * F.softmax(scores[:, :, :, :adapter_len].<span class="built_in">float</span>(), dim=-<span class="number">1</span>).type_as(xq),</span><br><span class="line">                    F.softmax(scores[:, :, :, adapter_len:].<span class="built_in">float</span>(), dim=-<span class="number">1</span>).type_as(xq),</span><br><span class="line">                ],</span><br><span class="line">                dim=-<span class="number">1</span>,</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            scores = F.softmax(scores.<span class="built_in">float</span>(), dim=-<span class="number">1</span>).type_as(xq)</span><br><span class="line">        output = torch.matmul(scores, values)  <span class="comment"># (bs, n_local_heads, slen, head_dim)</span></span><br><span class="line">        output = output.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(bsz, seqlen, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.wo(output)</span><br></pre></td></tr></table></figure>
<p>(images below are from <a href="http://jalammar.github.io/illustrated-gpt2/">jalammar.github.io&#x2F;illustrated-gpt2&#x2F;</a>)<br><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-9.png"><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-10.png"><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-11.png"></p>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><h3 id="Parameter-Efficient-Fine-Tuning-PEFT"><a href="#Parameter-Efficient-Fine-Tuning-PEFT" class="headerlink" title="Parameter-Efficient Fine-Tuning(PEFT)"></a>Parameter-Efficient Fine-Tuning(PEFT)</h3><p><img src="https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/classic-flowchart-1536x535.png" alt="Three conventional approaches of finetuing"> <a href="https://lightning.ai/pages/community/article/understanding-llama-adapters/">Three conventional approaches of finetuing.(the pre-trained model is not too large)</a> They are all compatible with encoder and decoder style. When finetune generative models, we work with and build on the embeddings they create instead of the generated output texts. But in-context learning only applies to decoder style. <img src="https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/classic-performance-1024x377.png" alt="traininig efficiency and modeling performance"></p>
<p><a href="https://github.com/huggingface/peft">PEFT</a> methods freeze most parameters of pre-trained models, and can still exhibit comparable capabilities on downstream tasks. It is needed when we want to get a similar modeling quality as finetuning II on LLMs. (e.g., Prompt-Tuning, Adapter, LoRA)</p>
<p>Prompt tuning appends a collection of trainable prompt tokens to pre-trained large models, which are inserted either to the input embeddings only, or to all of the intermediate layers. (e.g., Hard&#x2F;Soft Prompt-Tuning, Prefix-Tuning)</p>
<p>Hard Prompt-Tuning: directly change the discrete input tokens, which are not differentiable: <img src="https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/hard-prompting-1024x214.png"></p>
<p>Soft Prompt-Tuning: concatenates the embeddings of the input tokens with a trainable tensor that can be optimized via backpropagation.</p>
<p><a href="https://arxiv.org/pdf/2101.00190.pdf">Prefix Tuning</a>: add a trainable tensor to each transformer block instead of only the input embeddings, as in Soft Prompt-Tuning: <img src="https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/prefix-tuning-1536x907.png"></p>
<p><a href="https://arxiv.org/pdf/1902.00751.pdf">Adapter</a>: adds adapter layers in two places. The hidden dim in each adapter layer is low(e.g., 1024–&gt;24, params 1024×24+24×1024&#x3D;49,512 &lt; 1024×1024&#x3D;1,048,576). <img src="https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/adapter-outline-1024x548.png"></p>
<p><a href="https://lightning.ai/pages/community/tutorial/lora-llm/">LoRA</a>: introduces trainable rank decomposition matrices into each network weights: <img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-13.jpg"></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-12.png"> <a href="https://aclanthology.org/2022.acl-long.433.pdf">(UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning)</a></p>
<p>LLaMA-Adapter is distinct from regular Prefix-Tuning: 1. L topmost layer(not all) 2. zero-init attention(stablity improvement) 3. unified multi-modal tuning(unimodal to multi-modal) <img src="https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/popular-methods-1024x282.png"> <img src="https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/llama-adapter-1447x1536.png"></p>
<h3 id="Instruction-Following-Models"><a href="#Instruction-Following-Models" class="headerlink" title="Instruction-Following Models"></a>Instruction-Following Models</h3><p>Instruction-following capabilities: understand user intentions and follow instructions accurately.</p>
<p>Closed-source restriction and high training costs imped instruction-following models’ development.</p>
<p>Compared to a concurrent work <a href="https://github.com/tloen/alpaca-lora">Alpaca-LoRA</a>, our approach further reduces the computational demands, and can be generalized to follow visual instructions for multi-modal reasoning.</p>
<p><em>Language Modality:</em></p>
<ul>
<li><a href="https://arxiv.org/pdf/2109.01652.pdf">FLAN</a>: introduces an instruction tuning method</li>
<li><a href="https://arxiv.org/pdf/2202.01279.pdf">PromptSource</a>: a web-based GUI for creating and managing natural language prompt</li>
<li><a href="https://aclanthology.org/2022.emnlp-main.340.pdf">SUP-NATINST</a>: an benchmark of instructions on 1,616 NLP tasks</li>
<li><a href="https://arxiv.org/pdf/2203.02155.pdf">InstructGPT</a>: RLHF, significant performance improvements</li>
<li><strong><a href="https://github.com/tatsu-lab/stanford_alpaca">Alpaca</a>: data-efficient(self-instruction), high costs(fine-tuning)</strong></li>
<li><a href="https://lmsys.org/blog/2023-03-30-vicuna/">Vicuna</a> and <a href="https://arxiv.org/pdf/2304.03277.pdf">GPT-4-LLM</a>: reveal that dialog and enhanced instruction-following capabilities can be ignited by fine-tuning on either user-shared ChatGPT conversations or instruction-following data generated by the GPT-4 API</li>
</ul>
<p><em>Multi-modality:</em></p>
<p>Robot</p>
<ul>
<li>2020&#x2F;03&#x2F;31 <a href="https://arxiv.org/pdf/1912.01734.pdf">ALFRED</a>:  a benchmark for robotics instruction following</li>
<li>2022&#x2F;03&#x2F;16 <a href="https://arxiv.org/pdf/2110.07342.pdf">FILM</a>: a modular method for robotics instruction following</li>
</ul>
<p>Video</p>
<ul>
<li>2023&#x2F;05&#x2F;10 <a href="https://arxiv.org/pdf/2305.06355.pdf">VideoChat: Chat-Centric Video Understanding</a>: (1) integrates video foundation models and large language models via a learnable neural interface; (2) propose a video-centric instruction dataset based on WebVid-10M; (3) spatiotemporal reasoning, event localization, and causal relationship inference</li>
<li>2023&#x2F;06&#x2F;08 <a href="https://arxiv.org/pdf/2306.05424.pdf">Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models</a>: (1)  merges a video-adapted visual encoder with a LLM; (2) introduce a new dataset of 100,000 video-instruction pairs; (3) develop a quantitative evaluation framework for video-based dialogue models; (4) understanding and generating detailed conversations about videos</li>
<li>2023&#x2F;06&#x2F;12 <a href="https://arxiv.org/pdf/2306.02858.pdf">Video-LLaMA An Instruction-tuned Audio-Visual Language Model for Video Understanding</a>: (1) propose a Video Q-former for temporal info; (2) propose an Audio Q-former based on ImageBind for audio-visual signals</li>
</ul>
<p>Image(Model)</p>
<ul>
<li>2023&#x2F;04&#x2F;27 <a href="https://arxiv.org/pdf/2302.14045v1.pdf">Kosmos-1</a>, <a href="https://arxiv.org/pdf/2304.10592.pdf">MiniGPT4</a>, <a href="https://arxiv.org/pdf/2304.08485.pdf">LLaVA</a>, <a href="https://arxiv.org/pdf/2304.14178.pdf">mPLUG-Owl</a> <img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v1-14.jpg"></li>
<li>2023&#x2F;05&#x2F;05 <a href="https://arxiv.org/pdf/2305.03726.pdf">Otter: A Multi-Modal Model with In-Context Instruction Tuning</a></li>
<li>2023&#x2F;05&#x2F;11 <a href="https://arxiv.org/pdf/2305.06500.pdf">InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning</a>: (1) gather a wide variety of 26 publicly available datasets, transform them into instruction tuning format; (2)  instruction-aware visual feature extraction method</li>
<li>2023&#x2F;05&#x2F;22 <a href="https://arxiv.org/pdf/2305.04160.pdf">X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages</a>: propose X2L interface to convert other modality(image, speech, video) into foreign language via 3 training stages</li>
<li>2023&#x2F;05&#x2F;24 <a href="https://arxiv.org/pdf/2305.15023.pdf">Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models</a>: (1) Mixture-of-Modality Adaptation(MMA); (2) a routing algorithm for MMA, which enables an automatic shift between single- and multi-modal instructions; (3) MMA+LLaMA&#x3D;LaVIN(efficient), 1.4 training hours with 3.8M trainable params</li>
<li>2023&#x2F;05&#x2F;25 <a href="https://arxiv.org/pdf/2305.16355.pdf">PandaGPT: One Model To Instruction-Follow Them All</a>: (1) visual and auditory instruction-following capabilities(detailed image description generation, writing stories inspired by videos, answering questions about audios); (2) combines the multimodal encoders from ImageBind and the large language models from Vicuna; (3) displays emergent, i.e. zero-shot, cross-modal behaviors for data other than image and text (e.g., video, audio, depth, thermal, and IMU)</li>
<li>2023&#x2F;05&#x2F;25 <a href="https://arxiv.org/pdf/2305.16103.pdf">ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst</a>: (1) show that only language-paired two-modality data is sufficient to connect all modalities; (2) propose a new multi-modal instruction tuning dataset MULTIS, which covers a wide range of 16 multimodal tasks of text, image, video, and audio modalities; (3) a two-stage training, firstly aligns each modality with language, secondly aligns model with user intent</li>
<li>2023&#x2F;05&#x2F;30 <a href="https://arxiv.org/pdf/2305.18752.pdf">GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction</a>: (1) enable open-source LLMs to use multimodal tools; (2) generates an instruction-following dataset by prompting an advanced teacher with various multi-modal contexts; (3) provide a benchmark to evaluate the ability of LLMs to use tools</li>
<li>2023&#x2F;06&#x2F;02 <a href="https://arxiv.org/pdf/2305.05662.pdf">InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language</a>: integrates chatbots with non-verbal instructions(e.g., point movements like gestures and cursors), which requires fine-grained control, editing, and generation of visual content</li>
<li>2023&#x2F;06&#x2F;13 <a href="https://arxiv.org/pdf/2305.04790.pdf">MultiModal-GPT: A Vision and Language Model for Dialogue with Humans</a>: (1) capable of generating detailed captions, counting specific objects, and addressing general inquiries posed by users; (2) OpenFlamingo+LoRA; (3) construct multi-modal instruction templates; (4) also employ language-only instruction-following data for dialogue performance improvement</li>
</ul>
<p>Image(Dataset)</p>
<ul>
<li>2023&#x2F;06&#x2F;08 <a href="https://arxiv.org/pdf/2306.05425.pdf">MIMIC-IT: Multi-Modal In-Context Instruction Tuning</a>: (1) 2.8 million multimodal instruction-response pairs; (2) 8 languages; (3) contains videos</li>
<li>2023&#x2F;06&#x2F;08 <a href="https://arxiv.org/pdf/2306.04387.pdf">M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning</a>: (1) comprises 40 carefully curated datasets, including 2.4 million instances and 400 manually written task instructions, which surpasses previous datasets regarding task coverage; (2) 80 languages; (3) Ying-VLM model</li>
<li>2023&#x2F;06&#x2F;10 <a href="https://arxiv.org/pdf/2212.10773.pdf">MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning</a>: (1) a multimodal instruction tuning benchmark dataset that consists of 62 multimodal tasks; (2) a new evaluation metric, Sensitivity, to evaluate how sensitive the model is to the variety of instructions</li>
<li>2023&#x2F;06&#x2F;11 <a href="https://arxiv.org/pdf/2306.06687.pdf">LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark</a>: (1) extend MLLMs to point clouds; (2) LAMM-Dataset and LAMM-Benchmark for 2D image and 3D point cloud understanding</li>
</ul>
<h3 id="Large-Vision-Language-Models"><a href="#Large-Vision-Language-Models" class="headerlink" title="Large Vision-Language Models"></a>Large Vision-Language Models</h3><p>Recently, some researchers adopt pre-trained unimodal models as initialization and only train the newly introduced parameters. They use mapping networks or cross-attention layers to connect two modalities.</p>
<p>As a new method, LLaMA-Adapter also belongs to this line of work.</p>
<ul>
<li><a href="https://arxiv.org/pdf/2106.13884.pdf">Frozen</a>: fine-tunes an image encoder to transform visual tokens into LLM’s soft prompts</li>
<li><a href="https://arxiv.org/pdf/2111.07991.pdf">LiT</a>: utilizes pretrained image encoder to speed up CLIP training</li>
<li><a href="https://arxiv.org/pdf/2111.09734.pdf">CLIPCap</a>: proposes a mapping network to connect the pre-trained image encoder with LLMs</li>
<li><a href="https://arxiv.org/pdf/2110.04544.pdf">CLIP-Adapter</a>, <a href="https://arxiv.org/pdf/2111.03930.pdf">Tip-Adapter</a> and <a href="https://arxiv.org/pdf/2112.02413.pdf">PointCLIP</a>: introduce customized adapters upon CLIP for 2D and 3D few-shot learning</li>
<li><a href="https://arxiv.org/pdf/2204.14198.pdf">Flamingo</a>: inserts several cross-attention layers to inject visual knowledge into LLMs</li>
<li><a href="https://arxiv.org/pdf/2301.12597.pdf">BLIP2</a>: connects pre-trained image encoders and LLMs with a Q-Former</li>
</ul>
]]></content>
      <categories>
        <category>Paper</category>
        <category>2023</category>
        <category>03</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>PEFT</tag>
        <tag>Prefix-Tuning</tag>
        <tag>Adapter</tag>
        <tag>Instruction-Following</tag>
        <tag>Multi-modal</tag>
      </tags>
  </entry>
  <entry>
    <title>LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model</title>
    <url>/2023/06/19/llama-adapter-v2/</url>
    <content><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-0.png"></p>
<p>Paper: <a href="https://arxiv.org/pdf/2304.15010.pdf">https://arxiv.org/pdf/2304.15010.pdf</a></p>
<p>Code: <a href="https://github.com/OpenGVLab/LLaMA-Adapter">https://github.com/OpenGVLab/LLaMA-Adapter</a></p>
<blockquote>
<p>After trained on language instruction data, LLaMA-Adapter-V1 is fine-tuned on COCO Caption, which introducing new visual cues but damaging instruction-following abilities.</p>
</blockquote>
<aside>
⤴️ Therefore, LLaMA-Adapter-V2 further improves the multi-modal instruction-following abilities of LLaMA by introducing 14M parameters over LLaMA.

</aside>

<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-1.png" alt="Training Pipeline of LLaMA-Adapter V2."></p>
<hr>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><h3 id="Instruction-following-Models"><a href="#Instruction-following-Models" class="headerlink" title="Instruction-following Models"></a>Instruction-following Models</h3><blockquote>
<p>Details can be seen in my previous note of LLaMA-Adapter V1: the ‘Related Work&#x2F;Instruction-Following Models’ section.</p>
</blockquote>
<p>LLaMA-Adapter V2 can function effectively using just language instruction data and image-text pairs, without relying on multi-modal instruction data.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-2.png" alt="Table 1. Training Comparison of Different Methods. CC, VG and L400 represent Conceptual Caption, Visual Genome and LAION 400M, respectively. ∗ denotes the filtered dataset."></p>
<h3 id="Parameter-efficient-Fine-tuning"><a href="#Parameter-efficient-Fine-tuning" class="headerlink" title="Parameter-efficient Fine-tuning"></a>Parameter-efficient Fine-tuning</h3><blockquote>
<p>Details can be seen in my previous note of LLaMA-Adapter V1: the ‘Related Work&#x2F;Parameter-Efficient Fine-Tuning(PEFT)’ section.</p>
</blockquote>
<p>By utilizing an <strong>early fusion strategy</strong> and <strong>bias tuning</strong>, LLaMA-Adapter V2 injects visual features into LLMs, with only <strong>0.04%</strong> parameters of LLaMA.</p>
<h3 id="Integration-of-Expert-Systems"><a href="#Integration-of-Expert-Systems" class="headerlink" title="Integration of Expert Systems"></a>Integration of Expert Systems</h3><p>LLMs act as a core controller for external experts systems to boost its overall performance.</p>
<ul>
<li><strong>Vision Tasks</strong>: visual models as experts. <a href="https://arxiv.org/abs/2303.17580">HuggingGPT</a>, <a href="https://arxiv.org/abs/2303.04671">Visual ChatGPT</a>, <a href="https://arxiv.org/abs/2304.09842">Chameleon</a>, <a href="https://arxiv.org/abs/2303.11381">MMReACT</a> and <a href="https://arxiv.org/abs/2303.08128">ViperGPT</a></li>
<li><strong>Robotics</strong>: real-world sensors as experts. <a href="https://arxiv.org/abs/2303.03378">PaLM-E</a>, <a href="https://arxiv.org/abs/2207.05608">Inner Monologue</a> and <a href="https://arxiv.org/abs/2303.12153">Text2Motion</a></li>
</ul>
<p>For LLaMA-Adapter V2, experts integration compensates for the shortcomings brought by small training data.</p>
<h2 id="LLaMA-Adapter-V2"><a href="#LLaMA-Adapter-V2" class="headerlink" title="LLaMA-Adapter V2"></a>LLaMA-Adapter V2</h2><p>LLaMA-Adapter V2 is based on finetuned LLaMA-Adapter V1.</p>
<h3 id="Bias-Tuning-of-Linear-Layers"><a href="#Bias-Tuning-of-Linear-Layers" class="headerlink" title="Bias Tuning of Linear Layers"></a>Bias Tuning of Linear Layers</h3><aside>
👉 To enhance its language instruction-following ability

</aside>

<p>The added params accounts for <strong>0.04%(~5M) of LLaMA</strong> and its initialization is helpful for stable training at early stages.</p>
<p><strong>We first unfreeze all the normalization layers in LLaMA, and then</strong>:</p>
<ul>
<li>$x$: input</li>
<li>$W$: pre-trained weights(frozen) of a certain linear layer</li>
<li>$b&#x3D;Init(0)$: learnable bias</li>
<li>$s&#x3D;Init(1)$: learnable scale factor</li>
<li>$y&#x3D;W·x\rightarrow y&#x3D;s·(W·x+b)$: <strong>modify each linear layer in the Transformer</strong></li>
</ul>
<p><strong>Our bias tuning method compared with related work:</strong></p>
<ul>
<li><strong>BitFit, SSF</strong>: experiments on 80M parameters scale<ul>
<li><strong>Ours</strong>: from 7B to 65B</li>
</ul>
</li>
<li><strong>LoRA</strong>: input-aware bias<ul>
<li><p><strong>Ours</strong>: input-agnostic, less fine-tuning cost</p>
  <aside>
  ❓ why less cost
  
  </aside></li>
</ul>
</li>
</ul>
<h3 id="Joint-Training-with-Disjoint-Parameters"><a href="#Joint-Training-with-Disjoint-Parameters" class="headerlink" title="Joint Training with Disjoint Parameters"></a>Joint Training with Disjoint Parameters</h3><aside>
👉 For balanced visual instruction tuning

</aside>

<ul>
<li>Trained on <strong>500K image-text pairs</strong>: visual projection layers, early zero-initialized attention</li>
<li>Trained on <strong>50K language-only instruction data</strong>: late zero-initialized attention, unfrozen norm, learnable bias and scale factor (or optional <a href="https://arxiv.org/pdf/2106.09685.pdf">LoRA</a>)</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-3.png" alt="Joint Training Paradigm in LLaMA-Adapter V2."></p>
<h3 id="Early-Fusion-of-Visual-Knowledge"><a href="#Early-Fusion-of-Visual-Knowledge" class="headerlink" title="Early Fusion of Visual Knowledge"></a>Early Fusion of Visual Knowledge</h3><aside>
👉 To balance textual and visual understanding

</aside>

<p>$N$: the total number of Transformer layers</p>
<ul>
<li><p><strong>Visual prompts</strong>: concatenated with the word tokens at <strong>the first $K$ Transformer layers</strong> with zero-initialized attention</p>
<ul>
<li>$K&lt;N-L$</li>
</ul>
</li>
<li><p><strong>Static adaptation prompts</strong>: inserted into the last $L$ layers (e.g., L&#x3D;31)</p>
  <aside>
  ❓ a little hard to understand ”inserted into”: add or concatenate?
  
  </aside></li>
</ul>
<p>Prevent direct interactions between the two via placing them in different layers.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-4.png" alt="Early Fusion of Visual Knowledge."></p>
<aside>
❓ **But in code implementation, textual tokens are not concatenated with visual_query in the first layer**

</aside>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># LLaMA-Adapter/llama_adapter_v2_multimodal/llama/llama_adapter.py</span><br><span class="line"># class LLaMA_adapter:</span><br><span class="line">def forward(self, visual_query, tokens, start_pos: int):</span><br><span class="line">    _bsz, seqlen = tokens.shape</span><br><span class="line">    h = self.llama.tok_embeddings(tokens)</span><br><span class="line">    freqs_cis = self.llama.freqs_cis.to(h.device)</span><br><span class="line">    freqs_cis = freqs_cis[start_pos : start_pos + seqlen]</span><br><span class="line">    mask = None</span><br><span class="line">    mask = torch.full((1, 1, seqlen, seqlen),</span><br><span class="line">                        float(&quot;-inf&quot;), device=h.device)</span><br><span class="line">    mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)</span><br><span class="line"></span><br><span class="line">    for layer in self.llama.layers[:-1 * self.query_layer]:</span><br><span class="line">        h = layer(h, start_pos, freqs_cis, mask)</span><br><span class="line"></span><br><span class="line">    adapter = self.adapter_query.weight.reshape(</span><br><span class="line">        self.query_layer, self.query_len, -1).unsqueeze(1)</span><br><span class="line">    adapter_index = 0</span><br><span class="line">    for layer in self.llama.layers[-1 * self.query_layer:]:</span><br><span class="line">        dynamic_adapter = adapter[adapter_index].repeat(_bsz, 1, 1)</span><br><span class="line">        dynamic_adapter = dynamic_adapter + visual_query</span><br><span class="line">        h = layer(h, start_pos, freqs_cis, mask, dynamic_adapter)</span><br><span class="line">        adapter_index = adapter_index + 1</span><br><span class="line"></span><br><span class="line">    h = self.llama.norm(h)</span><br><span class="line">    output = self.llama.output(h[:, -1, :])</span><br><span class="line">    return output.float()</span><br></pre></td></tr></table></figure>

<h3 id="Integration-with-Experts"><a href="#Integration-with-Experts" class="headerlink" title="Integration with Experts"></a>Integration with Experts</h3><aside>
👉 To boost zero-shot multi-modal reasoning

</aside>

<p><strong>Well-trained img2text experts</strong>:</p>
<ul>
<li><strong>default implementation</strong> adopts LLaMA-Adapter v1 pre-trained on COCO Caption for short and accurate image descriptions generation</li>
<li>can be replaced with a search engine &#x2F; OCR</li>
</ul>
<p>We can easily switch among different experts based on the specific downstream task.</p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-5.png" alt="Generation Pipeline of LLaMA-Adapter V2."></p>
<aside>
❓ The order of 3 input parts(Textual Context, Question, Visual Context)

</aside>

<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Experimental-Setups"><a href="#Experimental-Setups" class="headerlink" title="Experimental Setups"></a>Experimental Setups</h3><p><strong>Training Data</strong>:</p>
<ul>
<li>52K single-turn instruction data from GPT-4-LLM</li>
<li>567K captioning data from COCO Caption</li>
<li>80K conversation data from ShareGPT (train a chatbot, multi-round)</li>
</ul>
<p><strong>Implementation Details</strong>:</p>
<ul>
<li>visual prompt length: 20</li>
</ul>
<h3 id="Stronger-Language-Instruction-Model"><a href="#Stronger-Language-Instruction-Model" class="headerlink" title="Stronger Language Instruction Model"></a>Stronger Language Instruction Model</h3><p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-6.png" alt="Part of Table 2. LLaMA-Adapter V2 provides more comprehensive answers and detailed explanations."></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-7.png" alt="A Chatting Example using 7B LLaMA-Adapter V2. But its understanding of context is not very accurate."></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-8.png" alt="A Chatting Example using 65B LLaMA-Adapter V2."></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-9.png" alt="Response Quality Comparisons(assessed by GPT-4 on 80 questions)."></p>
<ul>
<li>Ours: based on LLaMA-65B, fine-tune 14M parameters</li>
<li>Vicuna: based on LLaMA-13B, fine-tune 13B parameters</li>
</ul>
<h3 id="Visual-Instruction-Model"><a href="#Visual-Instruction-Model" class="headerlink" title="Visual Instruction Model"></a>Visual Instruction Model</h3><aside>
👇 Image Captioning

</aside>

<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-10.png" alt="Comparisons on COCO Caption."></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-11.png" alt="Comparisons of Image Captioning Results(between LLaMA-Adapter and LLaMA-Adapter V2)."></p>
<p>The failure case is intentionally choosen as an out-of-distribution example (cartoon picture) for testing.</p>
<p>It shows that the image-text alignment ability is not strong enough.</p>
<p>This motivates us to employ <strong>additional expert systems</strong> to enhance the image understanding ability.</p>
<aside>
👇 Visual Understanding

</aside>

<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-12.png" alt="The Visual Understanding Examples of LLaMA-Adapter V2."></p>
<ul>
<li>identify and explain the specific object or feature in the image</li>
<li>sophisticated reasoning and decision-making</li>
<li>offer a plausible guess or explanation when image cannot provide sufficient direct info</li>
</ul>
<aside>
👇 Integration with Experts

</aside>

<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-13.png" alt="Visual Understanding with the help of Caption Experts(more precise and detailed responses)."></p>
<p><img src="https://cdn.jsdelivr.net/gh/ifshinelx/blogimage@main/img/llama-adapter-v2-14.png" alt="Visual Understanding with the help of OCR Experts. The example and OCR context are from DocVQA."></p>
<h2 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h2><ul>
<li>integrate more expert visual systems</li>
<li>use multi-modal instruction dataset</li>
<li>other PEFT methods (e.g., LoRA)</li>
</ul>
]]></content>
      <categories>
        <category>Paper</category>
        <category>2023</category>
        <category>04</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>PEFT</tag>
        <tag>Prefix-Tuning</tag>
        <tag>Adapter</tag>
        <tag>Instruction-Following</tag>
        <tag>Multi-modal</tag>
        <tag>Expert Integration</tag>
      </tags>
  </entry>
</search>
